Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/04/17 13:46:18 INFO SparkContext: Running Spark version 1.6.3
17/04/17 13:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/17 13:46:19 WARN SparkConf: 
SPARK_WORKER_INSTANCES was detected (set to '2').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --num-executors to specify the number of executors
 - Or set SPARK_EXECUTOR_INSTANCES
 - spark.executor.instances to configure the number of instances in the spark config.
        
17/04/17 13:46:19 INFO SecurityManager: Changing view acls to: am372811
17/04/17 13:46:19 INFO SecurityManager: Changing modify acls to: am372811
17/04/17 13:46:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(am372811); users with modify permissions: Set(am372811)
17/04/17 13:46:19 INFO Utils: Successfully started service 'sparkDriver' on port 38868.
17/04/17 13:46:20 INFO Slf4jLogger: Slf4jLogger started
17/04/17 13:46:20 INFO Remoting: Starting remoting
17/04/17 13:46:20 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.0.1:43075]
17/04/17 13:46:20 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 43075.
17/04/17 13:46:20 INFO SparkEnv: Registering MapOutputTracker
17/04/17 13:46:20 INFO SparkEnv: Registering BlockManagerMaster
17/04/17 13:46:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a22f650e-ed5d-45fb-8a52-e8b675b5772b
17/04/17 13:46:20 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/04/17 13:46:20 INFO SparkEnv: Registering OutputCommitCoordinator
17/04/17 13:46:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/04/17 13:46:30 INFO SparkUI: Started SparkUI at http://192.168.0.1:4040
17/04/17 13:46:30 INFO HttpFileServer: HTTP File server directory is /tmp/spark-37f41b36-102a-4ee0-b7c6-38e1778a5f48/httpd-00fac26d-5509-49a8-a9c4-ec1827153e09
17/04/17 13:46:30 INFO HttpServer: Starting HTTP Server
17/04/17 13:46:30 INFO Utils: Successfully started service 'HTTP file server' on port 38657.
17/04/17 13:46:31 INFO SparkContext: Added JAR file:/home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar at http://192.168.0.1:38657/jars/chatbot-0.1.0-SNAPSHOT-fat.jar with timestamp 1492416991213
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Connecting to master spark://carl-PC:7077...
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20170417134630-0000
17/04/17 13:46:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35386.
17/04/17 13:46:31 INFO NettyBlockTransferService: Server created on 35386
17/04/17 13:46:31 INFO BlockManagerMaster: Trying to register BlockManager
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor added: app-20170417134630-0000/0 on worker-20170417134351-192.168.0.1-45033 (192.168.0.1:45033) with 4 cores
17/04/17 13:46:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.1:35386 with 511.1 MB RAM, BlockManagerId(driver, 192.168.0.1, 35386)
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: Granted executor ID app-20170417134630-0000/0 on hostPort 192.168.0.1:45033 with 4 cores, 1024.0 MB RAM
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor added: app-20170417134630-0000/1 on worker-20170417134352-192.168.0.2-38357 (192.168.0.2:38357) with 4 cores
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: Granted executor ID app-20170417134630-0000/1 on hostPort 192.168.0.2:38357 with 4 cores, 1024.0 MB RAM
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor added: app-20170417134630-0000/2 on worker-20170417134350-192.168.0.2-46540 (192.168.0.2:46540) with 4 cores
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: Granted executor ID app-20170417134630-0000/2 on hostPort 192.168.0.2:46540 with 4 cores, 1024.0 MB RAM
17/04/17 13:46:31 INFO BlockManagerMaster: Registered BlockManager
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor added: app-20170417134630-0000/3 on worker-20170417134353-192.168.0.1-36101 (192.168.0.1:36101) with 4 cores
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: Granted executor ID app-20170417134630-0000/3 on hostPort 192.168.0.1:36101 with 4 cores, 1024.0 MB RAM
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor updated: app-20170417134630-0000/2 is now RUNNING
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor updated: app-20170417134630-0000/0 is now RUNNING
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor updated: app-20170417134630-0000/1 is now RUNNING
17/04/17 13:46:31 INFO AppClient$ClientEndpoint: Executor updated: app-20170417134630-0000/3 is now RUNNING
17/04/17 13:46:31 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/04/17 13:46:32 INFO SparkContext: Starting job: start at KafkaConsumer.java:71
17/04/17 13:46:32 INFO DAGScheduler: Registering RDD 1 (start at KafkaConsumer.java:71)
17/04/17 13:46:32 INFO DAGScheduler: Got job 0 (start at KafkaConsumer.java:71) with 20 output partitions
17/04/17 13:46:32 INFO DAGScheduler: Final stage: ResultStage 1 (start at KafkaConsumer.java:71)
17/04/17 13:46:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/04/17 13:46:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/04/17 13:46:33 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[1] at start at KafkaConsumer.java:71), which has no missing parents
17/04/17 13:46:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.8 KB, free 511.1 MB)
17/04/17 13:46:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1726.0 B, free 511.1 MB)
17/04/17 13:46:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.1:35386 (size: 1726.0 B, free: 511.1 MB)
17/04/17 13:46:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/04/17 13:46:33 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[1] at start at KafkaConsumer.java:71)
17/04/17 13:46:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 50 tasks
17/04/17 13:46:35 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (carl-PC:47576) with ID 2
17/04/17 13:46:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, carl-PC, partition 0,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, carl-PC, partition 1,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, carl-PC, partition 2,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, carl-PC, partition 3,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (carl-PC:47578) with ID 1
17/04/17 13:46:35 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, carl-PC, partition 4,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, carl-PC, partition 5,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, carl-PC, partition 6,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, carl-PC, partition 7,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:35 INFO BlockManagerMasterEndpoint: Registering block manager carl-PC:40477 with 511.1 MB RAM, BlockManagerId(2, carl-PC, 40477)
17/04/17 13:46:35 INFO BlockManagerMasterEndpoint: Registering block manager carl-PC:42362 with 511.1 MB RAM, BlockManagerId(1, carl-PC, 42362)
17/04/17 13:46:36 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (am372811-PC:53872) with ID 3
17/04/17 13:46:36 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, am372811-PC, partition 8,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, am372811-PC, partition 9,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10, am372811-PC, partition 10,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11, am372811-PC, partition 11,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO BlockManagerMasterEndpoint: Registering block manager am372811-PC:45318 with 511.1 MB RAM, BlockManagerId(3, am372811-PC, 45318)
17/04/17 13:46:36 INFO SparkDeploySchedulerBackend: Registered executor NettyRpcEndpointRef(null) (am372811-PC:53874) with ID 0
17/04/17 13:46:36 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12, am372811-PC, partition 12,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13, am372811-PC, partition 13,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14, am372811-PC, partition 14,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15, am372811-PC, partition 15,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:36 INFO BlockManagerMasterEndpoint: Registering block manager am372811-PC:35315 with 511.1 MB RAM, BlockManagerId(0, am372811-PC, 35315)
17/04/17 13:46:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on carl-PC:40477 (size: 1726.0 B, free: 511.1 MB)
17/04/17 13:46:39 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16, carl-PC, partition 16,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:39 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17, carl-PC, partition 17,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:39 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18, carl-PC, partition 18,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:39 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19, carl-PC, partition 19,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on carl-PC:42362 (size: 1726.0 B, free: 511.1 MB)
17/04/17 13:46:41 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 4875 ms on carl-PC (1/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7304 ms on carl-PC (2/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7249 ms on carl-PC (3/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 7248 ms on carl-PC (4/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20, carl-PC, partition 20,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 2389 ms on carl-PC (5/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21, carl-PC, partition 21,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 2397 ms on carl-PC (6/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22, carl-PC, partition 22,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 2405 ms on carl-PC (7/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23, carl-PC, partition 23,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24, carl-PC, partition 24,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 7206 ms on carl-PC (8/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 2417 ms on carl-PC (9/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 7208 ms on carl-PC (10/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25, carl-PC, partition 25,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26, carl-PC, partition 26,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 7219 ms on carl-PC (11/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27, carl-PC, partition 27,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 7218 ms on carl-PC (12/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 35 ms on carl-PC (13/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28, carl-PC, partition 28,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 36 ms on carl-PC (14/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29, carl-PC, partition 29,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30, carl-PC, partition 30,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 38 ms on carl-PC (15/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31, carl-PC, partition 31,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 34 ms on carl-PC (16/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32, carl-PC, partition 32,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 18 ms on carl-PC (17/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33, carl-PC, partition 33,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 39 ms on carl-PC (18/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34, carl-PC, partition 34,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35, carl-PC, partition 35,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 57 ms on carl-PC (19/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 49 ms on carl-PC (20/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36, carl-PC, partition 36,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37, carl-PC, partition 37,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 49 ms on carl-PC (21/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 41 ms on carl-PC (22/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38, carl-PC, partition 38,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 38 ms on carl-PC (23/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 50 ms on carl-PC (24/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39, carl-PC, partition 39,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40, carl-PC, partition 40,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 22 ms on carl-PC (25/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41, carl-PC, partition 41,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 42 ms on carl-PC (26/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42, carl-PC, partition 42,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43, carl-PC, partition 43,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 45 ms on carl-PC (27/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 35 ms on carl-PC (28/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44, carl-PC, partition 44,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45, carl-PC, partition 45,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 45 ms on carl-PC (29/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 46 ms on carl-PC (30/50)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 63 ms on carl-PC (31/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46, carl-PC, partition 46,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 69 ms on carl-PC (32/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47, carl-PC, partition 47,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 23 ms on carl-PC (33/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48, carl-PC, partition 48,PROCESS_LOCAL, 2137 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 23 ms on carl-PC (34/50)
17/04/17 13:46:42 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49, carl-PC, partition 49,PROCESS_LOCAL, 2194 bytes)
17/04/17 13:46:42 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 29 ms on carl-PC (35/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 944 ms on carl-PC (36/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 950 ms on carl-PC (37/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 928 ms on carl-PC (38/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 922 ms on carl-PC (39/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 935 ms on carl-PC (40/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 946 ms on carl-PC (41/50)
17/04/17 13:46:43 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 945 ms on carl-PC (42/50)
17/04/17 13:46:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on am372811-PC:45318 (size: 1726.0 B, free: 511.1 MB)
17/04/17 13:46:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on am372811-PC:35315 (size: 1726.0 B, free: 511.1 MB)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 23752 ms on am372811-PC (43/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 23751 ms on am372811-PC (44/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 23754 ms on am372811-PC (45/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 23756 ms on am372811-PC (46/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 23578 ms on am372811-PC (47/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 23589 ms on am372811-PC (48/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 23588 ms on am372811-PC (49/50)
17/04/17 13:46:59 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 23594 ms on am372811-PC (50/50)
17/04/17 13:46:59 INFO DAGScheduler: ShuffleMapStage 0 (start at KafkaConsumer.java:71) finished in 26.488 s
17/04/17 13:46:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/17 13:46:59 INFO DAGScheduler: looking for newly runnable stages
17/04/17 13:46:59 INFO DAGScheduler: running: Set()
17/04/17 13:46:59 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/04/17 13:46:59 INFO DAGScheduler: failed: Set()
17/04/17 13:46:59 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[2] at start at KafkaConsumer.java:71), which has no missing parents
17/04/17 13:46:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.6 KB, free 511.1 MB)
17/04/17 13:46:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1588.0 B, free 511.1 MB)
17/04/17 13:46:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.1:35386 (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:46:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/04/17 13:46:59 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 1 (ShuffledRDD[2] at start at KafkaConsumer.java:71)
17/04/17 13:46:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 20 tasks
17/04/17 13:46:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 50, carl-PC, partition 0,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 51, am372811-PC, partition 9,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 52, am372811-PC, partition 10,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 53, carl-PC, partition 1,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 54, carl-PC, partition 2,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 55, am372811-PC, partition 11,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 56, am372811-PC, partition 12,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 57, carl-PC, partition 3,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 58, carl-PC, partition 4,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 59, am372811-PC, partition 13,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 60, am372811-PC, partition 14,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 61, carl-PC, partition 5,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 62, carl-PC, partition 6,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 63, am372811-PC, partition 15,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 64, am372811-PC, partition 16,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 65, carl-PC, partition 7,NODE_LOCAL, 1964 bytes)
17/04/17 13:46:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on carl-PC:40477 (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:46:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on carl-PC:42362 (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to carl-PC:47576
17/04/17 13:47:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to carl-PC:47578
17/04/17 13:47:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 342 bytes
17/04/17 13:47:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on am372811-PC:45318 (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 342 bytes
17/04/17 13:47:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on am372811-PC:35315 (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:00 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 66, carl-PC, partition 8,NODE_LOCAL, 1964 bytes)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 53) in 203 ms on carl-PC (1/20)
17/04/17 13:47:00 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 67, carl-PC, partition 17,NODE_LOCAL, 1964 bytes)
17/04/17 13:47:00 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 68, carl-PC, partition 18,NODE_LOCAL, 1964 bytes)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 57) in 257 ms on carl-PC (2/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 62) in 257 ms on carl-PC (3/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 61) in 263 ms on carl-PC (4/20)
17/04/17 13:47:00 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 69, carl-PC, partition 19,NODE_LOCAL, 1964 bytes)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 65) in 268 ms on carl-PC (5/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 66) in 73 ms on carl-PC (6/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 58) in 277 ms on carl-PC (7/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 54) in 284 ms on carl-PC (8/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 50) in 291 ms on carl-PC (9/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 68) in 40 ms on carl-PC (10/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 67) in 43 ms on carl-PC (11/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 69) in 41 ms on carl-PC (12/20)
17/04/17 13:47:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to am372811-PC:53874
17/04/17 13:47:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to am372811-PC:53872
17/04/17 13:47:00 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 60) in 606 ms on am372811-PC (13/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 64) in 610 ms on am372811-PC (14/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 55) in 640 ms on am372811-PC (15/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 51) in 649 ms on am372811-PC (16/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 56) in 713 ms on am372811-PC (17/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 59) in 714 ms on am372811-PC (18/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 52) in 719 ms on am372811-PC (19/20)
17/04/17 13:47:00 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 63) in 718 ms on am372811-PC (20/20)
17/04/17 13:47:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/04/17 13:47:00 INFO DAGScheduler: ResultStage 1 (start at KafkaConsumer.java:71) finished in 0.727 s
17/04/17 13:47:00 INFO DAGScheduler: Job 0 finished: start at KafkaConsumer.java:71, took 27.764180 s
17/04/17 13:47:00 INFO ReceiverTracker: Starting 1 receivers
17/04/17 13:47:00 INFO ReceiverTracker: ReceiverTracker started
17/04/17 13:47:00 INFO ForEachDStream: metadataCleanupDelay = -1
17/04/17 13:47:00 INFO MappedDStream: metadataCleanupDelay = -1
17/04/17 13:47:00 INFO MappedDStream: metadataCleanupDelay = -1
17/04/17 13:47:00 INFO KafkaInputDStream: metadataCleanupDelay = -1
17/04/17 13:47:00 INFO KafkaInputDStream: Slide time = 10000 ms
17/04/17 13:47:00 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/04/17 13:47:00 INFO KafkaInputDStream: Checkpoint interval = null
17/04/17 13:47:00 INFO KafkaInputDStream: Remember duration = 10000 ms
17/04/17 13:47:00 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@18b08a1a
17/04/17 13:47:00 INFO MappedDStream: Slide time = 10000 ms
17/04/17 13:47:00 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/04/17 13:47:00 INFO MappedDStream: Checkpoint interval = null
17/04/17 13:47:00 INFO MappedDStream: Remember duration = 10000 ms
17/04/17 13:47:00 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@49632eea
17/04/17 13:47:00 INFO MappedDStream: Slide time = 10000 ms
17/04/17 13:47:00 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/04/17 13:47:00 INFO MappedDStream: Checkpoint interval = null
17/04/17 13:47:00 INFO MappedDStream: Remember duration = 10000 ms
17/04/17 13:47:00 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@47912d12
17/04/17 13:47:00 INFO ForEachDStream: Slide time = 10000 ms
17/04/17 13:47:00 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/04/17 13:47:00 INFO ForEachDStream: Checkpoint interval = null
17/04/17 13:47:00 INFO ForEachDStream: Remember duration = 10000 ms
17/04/17 13:47:00 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@47a2960c
17/04/17 13:47:01 INFO DAGScheduler: Got job 1 (start at KafkaConsumer.java:71) with 1 output partitions
17/04/17 13:47:01 INFO DAGScheduler: Final stage: ResultStage 2 (start at KafkaConsumer.java:71)
17/04/17 13:47:01 INFO DAGScheduler: Parents of final stage: List()
17/04/17 13:47:01 INFO DAGScheduler: Missing parents: List()
17/04/17 13:47:01 INFO DAGScheduler: Submitting ResultStage 2 (Receiver 0 ParallelCollectionRDD[3] at makeRDD at ReceiverTracker.scala:588), which has no missing parents
17/04/17 13:47:01 INFO ReceiverTracker: Receiver 0 started
17/04/17 13:47:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 45.5 KB, free 511.1 MB)
17/04/17 13:47:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 14.9 KB, free 511.1 MB)
17/04/17 13:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.1:35386 (size: 14.9 KB, free: 511.1 MB)
17/04/17 13:47:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/04/17 13:47:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (Receiver 0 ParallelCollectionRDD[3] at makeRDD at ReceiverTracker.scala:588)
17/04/17 13:47:01 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/04/17 13:47:01 INFO RecurringTimer: Started timer for JobGenerator at time 1492417030000
17/04/17 13:47:01 INFO JobGenerator: Started JobGenerator at 1492417030000 ms
17/04/17 13:47:01 INFO JobScheduler: Started JobScheduler
17/04/17 13:47:01 INFO StreamingContext: StreamingContext started
17/04/17 13:47:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 70, carl-PC, partition 0,PROCESS_LOCAL, 3080 bytes)
17/04/17 13:47:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on carl-PC:40477 (size: 14.9 KB, free: 511.1 MB)
17/04/17 13:47:01 INFO ReceiverTracker: Registered receiver for stream 0 from carl-PC:47576
17/04/17 13:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on carl-PC:42362 in memory (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on carl-PC:40477 in memory (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.0.1:35386 in memory (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on am372811-PC:35315 in memory (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:05 INFO BlockManagerInfo: Removed broadcast_1_piece0 on am372811-PC:45318 in memory (size: 1588.0 B, free: 511.1 MB)
17/04/17 13:47:05 INFO ContextCleaner: Cleaned accumulator 2
17/04/17 13:47:05 INFO ContextCleaner: Cleaned shuffle 0
17/04/17 13:47:10 INFO JobScheduler: Added jobs for time 1492417030000 ms
17/04/17 13:47:10 INFO JobScheduler: Starting job streaming job 1492417030000 ms.0 from job set of time 1492417030000 ms
-------------------------------------------
Time: 1492417030000 ms
-------------------------------------------

17/04/17 13:47:10 INFO JobScheduler: Finished job streaming job 1492417030000 ms.0 from job set of time 1492417030000 ms
17/04/17 13:47:10 INFO JobScheduler: Total delay: 0.198 s for time 1492417030000 ms (execution: 0.033 s)
17/04/17 13:47:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
17/04/17 13:47:10 INFO InputInfoTracker: remove old batch metadata: 
17/04/17 13:47:20 INFO JobScheduler: Added jobs for time 1492417040000 ms
-------------------------------------------
Time: 1492417040000 ms
-------------------------------------------

17/04/17 13:47:20 INFO JobScheduler: Starting job streaming job 1492417040000 ms.0 from job set of time 1492417040000 ms
17/04/17 13:47:20 INFO JobScheduler: Finished job streaming job 1492417040000 ms.0 from job set of time 1492417040000 ms
17/04/17 13:47:20 INFO JobScheduler: Total delay: 0.010 s for time 1492417040000 ms (execution: 0.000 s)
17/04/17 13:47:20 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
17/04/17 13:47:20 INFO BlockManager: Removing RDD 6
17/04/17 13:47:20 INFO MapPartitionsRDD: Removing RDD 5 from persistence list
17/04/17 13:47:20 INFO BlockManager: Removing RDD 5
17/04/17 13:47:20 INFO BlockRDD: Removing RDD 4 from persistence list
17/04/17 13:47:20 INFO BlockManager: Removing RDD 4
17/04/17 13:47:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[4] at createStream at KafkaConsumer.java:64 of time 1492417040000 ms
17/04/17 13:47:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
17/04/17 13:47:20 INFO InputInfoTracker: remove old batch metadata: 
17/04/17 13:47:30 INFO JobScheduler: Added jobs for time 1492417050000 ms
17/04/17 13:47:30 INFO JobScheduler: Starting job streaming job 1492417050000 ms.0 from job set of time 1492417050000 ms
-------------------------------------------
Time: 1492417050000 ms
-------------------------------------------

17/04/17 13:47:30 INFO JobScheduler: Finished job streaming job 1492417050000 ms.0 from job set of time 1492417050000 ms
17/04/17 13:47:30 INFO JobScheduler: Total delay: 0.012 s for time 1492417050000 ms (execution: 0.001 s)
17/04/17 13:47:30 INFO MapPartitionsRDD: Removing RDD 9 from persistence list
17/04/17 13:47:30 INFO BlockManager: Removing RDD 9
17/04/17 13:47:30 INFO MapPartitionsRDD: Removing RDD 8 from persistence list
17/04/17 13:47:30 INFO BlockManager: Removing RDD 8
17/04/17 13:47:30 INFO BlockRDD: Removing RDD 7 from persistence list
17/04/17 13:47:30 INFO BlockManager: Removing RDD 7
17/04/17 13:47:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[7] at createStream at KafkaConsumer.java:64 of time 1492417050000 ms
17/04/17 13:47:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417030000 ms)
17/04/17 13:47:30 INFO InputInfoTracker: remove old batch metadata: 1492417030000 ms
17/04/17 13:47:40 INFO JobScheduler: Added jobs for time 1492417060000 ms
17/04/17 13:47:40 INFO JobScheduler: Starting job streaming job 1492417060000 ms.0 from job set of time 1492417060000 ms
-------------------------------------------
Time: 1492417060000 ms
-------------------------------------------

17/04/17 13:47:40 INFO JobScheduler: Finished job streaming job 1492417060000 ms.0 from job set of time 1492417060000 ms
17/04/17 13:47:40 INFO MapPartitionsRDD: Removing RDD 12 from persistence list
17/04/17 13:47:40 INFO JobScheduler: Total delay: 0.011 s for time 1492417060000 ms (execution: 0.002 s)
17/04/17 13:47:40 INFO BlockManager: Removing RDD 12
17/04/17 13:47:40 INFO MapPartitionsRDD: Removing RDD 11 from persistence list
17/04/17 13:47:40 INFO BlockManager: Removing RDD 11
17/04/17 13:47:40 INFO BlockRDD: Removing RDD 10 from persistence list
17/04/17 13:47:40 INFO BlockManager: Removing RDD 10
17/04/17 13:47:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[10] at createStream at KafkaConsumer.java:64 of time 1492417060000 ms
17/04/17 13:47:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417040000 ms)
17/04/17 13:47:40 INFO InputInfoTracker: remove old batch metadata: 1492417040000 ms
17/04/17 13:47:50 INFO JobScheduler: Added jobs for time 1492417070000 ms
17/04/17 13:47:50 INFO JobScheduler: Starting job streaming job 1492417070000 ms.0 from job set of time 1492417070000 ms
-------------------------------------------
Time: 1492417070000 ms
-------------------------------------------

17/04/17 13:47:50 INFO JobScheduler: Finished job streaming job 1492417070000 ms.0 from job set of time 1492417070000 ms
17/04/17 13:47:50 INFO MapPartitionsRDD: Removing RDD 15 from persistence list
17/04/17 13:47:50 INFO JobScheduler: Total delay: 0.010 s for time 1492417070000 ms (execution: 0.001 s)
17/04/17 13:47:50 INFO BlockManager: Removing RDD 15
17/04/17 13:47:50 INFO MapPartitionsRDD: Removing RDD 14 from persistence list
17/04/17 13:47:50 INFO BlockManager: Removing RDD 14
17/04/17 13:47:50 INFO BlockRDD: Removing RDD 13 from persistence list
17/04/17 13:47:50 INFO BlockManager: Removing RDD 13
17/04/17 13:47:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[13] at createStream at KafkaConsumer.java:64 of time 1492417070000 ms
17/04/17 13:47:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417050000 ms)
17/04/17 13:47:50 INFO InputInfoTracker: remove old batch metadata: 1492417050000 ms
17/04/17 13:48:00 INFO JobScheduler: Starting job streaming job 1492417080000 ms.0 from job set of time 1492417080000 ms
17/04/17 13:48:00 INFO JobScheduler: Added jobs for time 1492417080000 ms
-------------------------------------------
Time: 1492417080000 ms
-------------------------------------------

17/04/17 13:48:00 INFO JobScheduler: Finished job streaming job 1492417080000 ms.0 from job set of time 1492417080000 ms
17/04/17 13:48:00 INFO MapPartitionsRDD: Removing RDD 18 from persistence list
17/04/17 13:48:00 INFO JobScheduler: Total delay: 0.012 s for time 1492417080000 ms (execution: 0.001 s)
17/04/17 13:48:00 INFO BlockManager: Removing RDD 18
17/04/17 13:48:00 INFO MapPartitionsRDD: Removing RDD 17 from persistence list
17/04/17 13:48:00 INFO BlockManager: Removing RDD 17
17/04/17 13:48:00 INFO BlockRDD: Removing RDD 16 from persistence list
17/04/17 13:48:00 INFO BlockManager: Removing RDD 16
17/04/17 13:48:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[16] at createStream at KafkaConsumer.java:64 of time 1492417080000 ms
17/04/17 13:48:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417060000 ms)
17/04/17 13:48:00 INFO InputInfoTracker: remove old batch metadata: 1492417060000 ms
17/04/17 13:48:10 INFO JobScheduler: Added jobs for time 1492417090000 ms
17/04/17 13:48:10 INFO JobScheduler: Starting job streaming job 1492417090000 ms.0 from job set of time 1492417090000 ms
-------------------------------------------
Time: 1492417090000 ms
-------------------------------------------

17/04/17 13:48:10 INFO JobScheduler: Finished job streaming job 1492417090000 ms.0 from job set of time 1492417090000 ms
17/04/17 13:48:10 INFO MapPartitionsRDD: Removing RDD 21 from persistence list
17/04/17 13:48:10 INFO JobScheduler: Total delay: 0.010 s for time 1492417090000 ms (execution: 0.001 s)
17/04/17 13:48:10 INFO BlockManager: Removing RDD 21
17/04/17 13:48:10 INFO MapPartitionsRDD: Removing RDD 20 from persistence list
17/04/17 13:48:10 INFO BlockManager: Removing RDD 20
17/04/17 13:48:10 INFO BlockRDD: Removing RDD 19 from persistence list
17/04/17 13:48:10 INFO BlockManager: Removing RDD 19
17/04/17 13:48:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[19] at createStream at KafkaConsumer.java:64 of time 1492417090000 ms
17/04/17 13:48:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417070000 ms)
17/04/17 13:48:10 INFO InputInfoTracker: remove old batch metadata: 1492417070000 ms
17/04/17 13:48:20 INFO JobScheduler: Added jobs for time 1492417100000 ms
17/04/17 13:48:20 INFO JobScheduler: Starting job streaming job 1492417100000 ms.0 from job set of time 1492417100000 ms
-------------------------------------------
Time: 1492417100000 ms
-------------------------------------------

17/04/17 13:48:20 INFO JobScheduler: Finished job streaming job 1492417100000 ms.0 from job set of time 1492417100000 ms
17/04/17 13:48:20 INFO MapPartitionsRDD: Removing RDD 24 from persistence list
17/04/17 13:48:20 INFO JobScheduler: Total delay: 0.010 s for time 1492417100000 ms (execution: 0.001 s)
17/04/17 13:48:20 INFO BlockManager: Removing RDD 24
17/04/17 13:48:20 INFO MapPartitionsRDD: Removing RDD 23 from persistence list
17/04/17 13:48:20 INFO BlockManager: Removing RDD 23
17/04/17 13:48:20 INFO BlockRDD: Removing RDD 22 from persistence list
17/04/17 13:48:20 INFO BlockManager: Removing RDD 22
17/04/17 13:48:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[22] at createStream at KafkaConsumer.java:64 of time 1492417100000 ms
17/04/17 13:48:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417080000 ms)
17/04/17 13:48:20 INFO InputInfoTracker: remove old batch metadata: 1492417080000 ms
17/04/17 13:48:30 INFO JobScheduler: Added jobs for time 1492417110000 ms
17/04/17 13:48:30 INFO JobScheduler: Starting job streaming job 1492417110000 ms.0 from job set of time 1492417110000 ms
-------------------------------------------
Time: 1492417110000 ms
-------------------------------------------

17/04/17 13:48:30 INFO JobScheduler: Finished job streaming job 1492417110000 ms.0 from job set of time 1492417110000 ms
17/04/17 13:48:30 INFO MapPartitionsRDD: Removing RDD 27 from persistence list
17/04/17 13:48:30 INFO JobScheduler: Total delay: 0.009 s for time 1492417110000 ms (execution: 0.001 s)
17/04/17 13:48:30 INFO BlockManager: Removing RDD 27
17/04/17 13:48:30 INFO MapPartitionsRDD: Removing RDD 26 from persistence list
17/04/17 13:48:30 INFO BlockManager: Removing RDD 26
17/04/17 13:48:30 INFO BlockRDD: Removing RDD 25 from persistence list
17/04/17 13:48:30 INFO BlockManager: Removing RDD 25
17/04/17 13:48:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[25] at createStream at KafkaConsumer.java:64 of time 1492417110000 ms
17/04/17 13:48:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417090000 ms)
17/04/17 13:48:30 INFO InputInfoTracker: remove old batch metadata: 1492417090000 ms
17/04/17 13:48:40 INFO JobScheduler: Added jobs for time 1492417120000 ms
17/04/17 13:48:40 INFO JobScheduler: Starting job streaming job 1492417120000 ms.0 from job set of time 1492417120000 ms
-------------------------------------------
Time: 1492417120000 ms
-------------------------------------------

17/04/17 13:48:40 INFO JobScheduler: Finished job streaming job 1492417120000 ms.0 from job set of time 1492417120000 ms
17/04/17 13:48:40 INFO MapPartitionsRDD: Removing RDD 30 from persistence list
17/04/17 13:48:40 INFO JobScheduler: Total delay: 0.010 s for time 1492417120000 ms (execution: 0.001 s)
17/04/17 13:48:40 INFO BlockManager: Removing RDD 30
17/04/17 13:48:40 INFO MapPartitionsRDD: Removing RDD 29 from persistence list
17/04/17 13:48:40 INFO BlockManager: Removing RDD 29
17/04/17 13:48:40 INFO BlockRDD: Removing RDD 28 from persistence list
17/04/17 13:48:40 INFO BlockManager: Removing RDD 28
17/04/17 13:48:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[28] at createStream at KafkaConsumer.java:64 of time 1492417120000 ms
17/04/17 13:48:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417100000 ms)
17/04/17 13:48:40 INFO InputInfoTracker: remove old batch metadata: 1492417100000 ms
17/04/17 13:48:50 INFO JobScheduler: Added jobs for time 1492417130000 ms
17/04/17 13:48:50 INFO JobScheduler: Starting job streaming job 1492417130000 ms.0 from job set of time 1492417130000 ms
-------------------------------------------
Time: 1492417130000 ms
-------------------------------------------

17/04/17 13:48:50 INFO JobScheduler: Finished job streaming job 1492417130000 ms.0 from job set of time 1492417130000 ms
17/04/17 13:48:50 INFO MapPartitionsRDD: Removing RDD 33 from persistence list
17/04/17 13:48:50 INFO JobScheduler: Total delay: 0.011 s for time 1492417130000 ms (execution: 0.002 s)
17/04/17 13:48:50 INFO BlockManager: Removing RDD 33
17/04/17 13:48:50 INFO MapPartitionsRDD: Removing RDD 32 from persistence list
17/04/17 13:48:50 INFO BlockManager: Removing RDD 32
17/04/17 13:48:50 INFO BlockRDD: Removing RDD 31 from persistence list
17/04/17 13:48:50 INFO BlockManager: Removing RDD 31
17/04/17 13:48:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[31] at createStream at KafkaConsumer.java:64 of time 1492417130000 ms
17/04/17 13:48:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417110000 ms)
17/04/17 13:48:50 INFO InputInfoTracker: remove old batch metadata: 1492417110000 ms
17/04/17 13:49:00 INFO JobScheduler: Added jobs for time 1492417140000 ms
17/04/17 13:49:00 INFO JobScheduler: Starting job streaming job 1492417140000 ms.0 from job set of time 1492417140000 ms
-------------------------------------------
Time: 1492417140000 ms
-------------------------------------------

17/04/17 13:49:00 INFO JobScheduler: Finished job streaming job 1492417140000 ms.0 from job set of time 1492417140000 ms
17/04/17 13:49:00 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
17/04/17 13:49:00 INFO JobScheduler: Total delay: 0.010 s for time 1492417140000 ms (execution: 0.002 s)
17/04/17 13:49:00 INFO BlockManager: Removing RDD 36
17/04/17 13:49:00 INFO MapPartitionsRDD: Removing RDD 35 from persistence list
17/04/17 13:49:00 INFO BlockManager: Removing RDD 35
17/04/17 13:49:00 INFO BlockRDD: Removing RDD 34 from persistence list
17/04/17 13:49:00 INFO BlockManager: Removing RDD 34
17/04/17 13:49:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[34] at createStream at KafkaConsumer.java:64 of time 1492417140000 ms
17/04/17 13:49:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417120000 ms)
17/04/17 13:49:00 INFO InputInfoTracker: remove old batch metadata: 1492417120000 ms
17/04/17 13:49:10 INFO JobScheduler: Starting job streaming job 1492417150000 ms.0 from job set of time 1492417150000 ms
-------------------------------------------
Time: 1492417150000 ms
-------------------------------------------

17/04/17 13:49:10 INFO JobScheduler: Finished job streaming job 1492417150000 ms.0 from job set of time 1492417150000 ms
17/04/17 13:49:10 INFO JobScheduler: Added jobs for time 1492417150000 ms
17/04/17 13:49:10 INFO JobScheduler: Total delay: 0.009 s for time 1492417150000 ms (execution: 0.001 s)
17/04/17 13:49:10 INFO MapPartitionsRDD: Removing RDD 39 from persistence list
17/04/17 13:49:10 INFO BlockManager: Removing RDD 39
17/04/17 13:49:10 INFO MapPartitionsRDD: Removing RDD 38 from persistence list
17/04/17 13:49:10 INFO BlockManager: Removing RDD 38
17/04/17 13:49:10 INFO BlockRDD: Removing RDD 37 from persistence list
17/04/17 13:49:10 INFO BlockManager: Removing RDD 37
17/04/17 13:49:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[37] at createStream at KafkaConsumer.java:64 of time 1492417150000 ms
17/04/17 13:49:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417130000 ms)
17/04/17 13:49:10 INFO InputInfoTracker: remove old batch metadata: 1492417130000 ms
17/04/17 13:49:20 INFO JobScheduler: Starting job streaming job 1492417160000 ms.0 from job set of time 1492417160000 ms
17/04/17 13:49:20 INFO JobScheduler: Added jobs for time 1492417160000 ms
-------------------------------------------
Time: 1492417160000 ms
-------------------------------------------

17/04/17 13:49:20 INFO JobScheduler: Finished job streaming job 1492417160000 ms.0 from job set of time 1492417160000 ms
17/04/17 13:49:20 INFO MapPartitionsRDD: Removing RDD 42 from persistence list
17/04/17 13:49:20 INFO JobScheduler: Total delay: 0.013 s for time 1492417160000 ms (execution: 0.001 s)
17/04/17 13:49:20 INFO BlockManager: Removing RDD 42
17/04/17 13:49:20 INFO MapPartitionsRDD: Removing RDD 41 from persistence list
17/04/17 13:49:20 INFO BlockManager: Removing RDD 41
17/04/17 13:49:20 INFO BlockRDD: Removing RDD 40 from persistence list
17/04/17 13:49:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[40] at createStream at KafkaConsumer.java:64 of time 1492417160000 ms
17/04/17 13:49:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417140000 ms)
17/04/17 13:49:20 INFO InputInfoTracker: remove old batch metadata: 1492417140000 ms
17/04/17 13:49:20 INFO BlockManager: Removing RDD 40
17/04/17 13:49:30 INFO JobScheduler: Added jobs for time 1492417170000 ms
17/04/17 13:49:30 INFO JobScheduler: Starting job streaming job 1492417170000 ms.0 from job set of time 1492417170000 ms
-------------------------------------------
Time: 1492417170000 ms
-------------------------------------------

17/04/17 13:49:30 INFO JobScheduler: Finished job streaming job 1492417170000 ms.0 from job set of time 1492417170000 ms
17/04/17 13:49:30 INFO JobScheduler: Total delay: 0.007 s for time 1492417170000 ms (execution: 0.001 s)
17/04/17 13:49:30 INFO MapPartitionsRDD: Removing RDD 45 from persistence list
17/04/17 13:49:30 INFO MapPartitionsRDD: Removing RDD 44 from persistence list
17/04/17 13:49:30 INFO BlockManager: Removing RDD 45
17/04/17 13:49:30 INFO BlockManager: Removing RDD 44
17/04/17 13:49:30 INFO BlockRDD: Removing RDD 43 from persistence list
17/04/17 13:49:30 INFO BlockManager: Removing RDD 43
17/04/17 13:49:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[43] at createStream at KafkaConsumer.java:64 of time 1492417170000 ms
17/04/17 13:49:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417150000 ms)
17/04/17 13:49:30 INFO InputInfoTracker: remove old batch metadata: 1492417150000 ms
17/04/17 13:49:40 INFO JobScheduler: Added jobs for time 1492417180000 ms
17/04/17 13:49:40 INFO JobScheduler: Starting job streaming job 1492417180000 ms.0 from job set of time 1492417180000 ms
-------------------------------------------
Time: 1492417180000 ms
-------------------------------------------

17/04/17 13:49:40 INFO JobScheduler: Finished job streaming job 1492417180000 ms.0 from job set of time 1492417180000 ms
17/04/17 13:49:40 INFO JobScheduler: Total delay: 0.006 s for time 1492417180000 ms (execution: 0.000 s)
17/04/17 13:49:40 INFO MapPartitionsRDD: Removing RDD 48 from persistence list
17/04/17 13:49:40 INFO BlockManager: Removing RDD 48
17/04/17 13:49:40 INFO MapPartitionsRDD: Removing RDD 47 from persistence list
17/04/17 13:49:40 INFO BlockManager: Removing RDD 47
17/04/17 13:49:40 INFO BlockRDD: Removing RDD 46 from persistence list
17/04/17 13:49:40 INFO BlockManager: Removing RDD 46
17/04/17 13:49:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[46] at createStream at KafkaConsumer.java:64 of time 1492417180000 ms
17/04/17 13:49:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417160000 ms)
17/04/17 13:49:40 INFO InputInfoTracker: remove old batch metadata: 1492417160000 ms
-------------------------------------------
Time: 1492417190000 ms
-------------------------------------------

17/04/17 13:49:50 INFO JobScheduler: Starting job streaming job 1492417190000 ms.0 from job set of time 1492417190000 ms
17/04/17 13:49:50 INFO JobScheduler: Finished job streaming job 1492417190000 ms.0 from job set of time 1492417190000 ms
17/04/17 13:49:50 INFO JobScheduler: Total delay: 0.009 s for time 1492417190000 ms (execution: 0.000 s)
17/04/17 13:49:50 INFO JobScheduler: Added jobs for time 1492417190000 ms
17/04/17 13:49:50 INFO MapPartitionsRDD: Removing RDD 51 from persistence list
17/04/17 13:49:50 INFO BlockManager: Removing RDD 51
17/04/17 13:49:50 INFO MapPartitionsRDD: Removing RDD 50 from persistence list
17/04/17 13:49:50 INFO BlockManager: Removing RDD 50
17/04/17 13:49:50 INFO BlockRDD: Removing RDD 49 from persistence list
17/04/17 13:49:50 INFO BlockManager: Removing RDD 49
17/04/17 13:49:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[49] at createStream at KafkaConsumer.java:64 of time 1492417190000 ms
17/04/17 13:49:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417170000 ms)
17/04/17 13:49:50 INFO InputInfoTracker: remove old batch metadata: 1492417170000 ms
17/04/17 13:50:00 INFO JobScheduler: Added jobs for time 1492417200000 ms
17/04/17 13:50:00 INFO JobScheduler: Starting job streaming job 1492417200000 ms.0 from job set of time 1492417200000 ms
-------------------------------------------
Time: 1492417200000 ms
-------------------------------------------

17/04/17 13:50:00 INFO JobScheduler: Finished job streaming job 1492417200000 ms.0 from job set of time 1492417200000 ms
17/04/17 13:50:00 INFO MapPartitionsRDD: Removing RDD 54 from persistence list
17/04/17 13:50:00 INFO JobScheduler: Total delay: 0.008 s for time 1492417200000 ms (execution: 0.001 s)
17/04/17 13:50:00 INFO BlockManager: Removing RDD 54
17/04/17 13:50:00 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
17/04/17 13:50:00 INFO BlockManager: Removing RDD 53
17/04/17 13:50:00 INFO BlockRDD: Removing RDD 52 from persistence list
17/04/17 13:50:00 INFO BlockManager: Removing RDD 52
17/04/17 13:50:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[52] at createStream at KafkaConsumer.java:64 of time 1492417200000 ms
17/04/17 13:50:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417180000 ms)
17/04/17 13:50:00 INFO InputInfoTracker: remove old batch metadata: 1492417180000 ms
17/04/17 13:50:10 INFO JobScheduler: Added jobs for time 1492417210000 ms
17/04/17 13:50:10 INFO JobScheduler: Starting job streaming job 1492417210000 ms.0 from job set of time 1492417210000 ms
-------------------------------------------
Time: 1492417210000 ms
-------------------------------------------

17/04/17 13:50:10 INFO JobScheduler: Finished job streaming job 1492417210000 ms.0 from job set of time 1492417210000 ms
17/04/17 13:50:10 INFO MapPartitionsRDD: Removing RDD 57 from persistence list
17/04/17 13:50:10 INFO JobScheduler: Total delay: 0.009 s for time 1492417210000 ms (execution: 0.001 s)
17/04/17 13:50:10 INFO BlockManager: Removing RDD 57
17/04/17 13:50:10 INFO MapPartitionsRDD: Removing RDD 56 from persistence list
17/04/17 13:50:10 INFO BlockManager: Removing RDD 56
17/04/17 13:50:10 INFO BlockRDD: Removing RDD 55 from persistence list
17/04/17 13:50:10 INFO BlockManager: Removing RDD 55
17/04/17 13:50:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[55] at createStream at KafkaConsumer.java:64 of time 1492417210000 ms
17/04/17 13:50:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417190000 ms)
17/04/17 13:50:10 INFO InputInfoTracker: remove old batch metadata: 1492417190000 ms
17/04/17 13:50:20 INFO JobScheduler: Starting job streaming job 1492417220000 ms.0 from job set of time 1492417220000 ms
-------------------------------------------
Time: 1492417220000 ms
-------------------------------------------

17/04/17 13:50:20 INFO JobScheduler: Added jobs for time 1492417220000 ms
17/04/17 13:50:20 INFO JobScheduler: Finished job streaming job 1492417220000 ms.0 from job set of time 1492417220000 ms
17/04/17 13:50:20 INFO JobScheduler: Total delay: 0.010 s for time 1492417220000 ms (execution: 0.001 s)
17/04/17 13:50:20 INFO MapPartitionsRDD: Removing RDD 60 from persistence list
17/04/17 13:50:20 INFO MapPartitionsRDD: Removing RDD 59 from persistence list
17/04/17 13:50:20 INFO BlockManager: Removing RDD 60
17/04/17 13:50:20 INFO BlockManager: Removing RDD 59
17/04/17 13:50:20 INFO BlockRDD: Removing RDD 58 from persistence list
17/04/17 13:50:20 INFO BlockManager: Removing RDD 58
17/04/17 13:50:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[58] at createStream at KafkaConsumer.java:64 of time 1492417220000 ms
17/04/17 13:50:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417200000 ms)
17/04/17 13:50:20 INFO InputInfoTracker: remove old batch metadata: 1492417200000 ms
17/04/17 13:50:30 INFO JobScheduler: Added jobs for time 1492417230000 ms
17/04/17 13:50:30 INFO JobScheduler: Starting job streaming job 1492417230000 ms.0 from job set of time 1492417230000 ms
-------------------------------------------
Time: 1492417230000 ms
-------------------------------------------

17/04/17 13:50:30 INFO JobScheduler: Finished job streaming job 1492417230000 ms.0 from job set of time 1492417230000 ms
17/04/17 13:50:30 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
17/04/17 13:50:30 INFO JobScheduler: Total delay: 0.006 s for time 1492417230000 ms (execution: 0.001 s)
17/04/17 13:50:30 INFO BlockManager: Removing RDD 63
17/04/17 13:50:30 INFO MapPartitionsRDD: Removing RDD 62 from persistence list
17/04/17 13:50:30 INFO BlockManager: Removing RDD 62
17/04/17 13:50:30 INFO BlockRDD: Removing RDD 61 from persistence list
17/04/17 13:50:30 INFO BlockManager: Removing RDD 61
17/04/17 13:50:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[61] at createStream at KafkaConsumer.java:64 of time 1492417230000 ms
17/04/17 13:50:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417210000 ms)
17/04/17 13:50:30 INFO InputInfoTracker: remove old batch metadata: 1492417210000 ms
17/04/17 13:50:40 INFO JobScheduler: Added jobs for time 1492417240000 ms
17/04/17 13:50:40 INFO JobScheduler: Starting job streaming job 1492417240000 ms.0 from job set of time 1492417240000 ms
-------------------------------------------
Time: 1492417240000 ms
-------------------------------------------

17/04/17 13:50:40 INFO JobScheduler: Finished job streaming job 1492417240000 ms.0 from job set of time 1492417240000 ms
17/04/17 13:50:40 INFO JobScheduler: Total delay: 0.009 s for time 1492417240000 ms (execution: 0.001 s)
17/04/17 13:50:40 INFO MapPartitionsRDD: Removing RDD 66 from persistence list
17/04/17 13:50:40 INFO BlockManager: Removing RDD 66
17/04/17 13:50:40 INFO MapPartitionsRDD: Removing RDD 65 from persistence list
17/04/17 13:50:40 INFO BlockManager: Removing RDD 65
17/04/17 13:50:40 INFO BlockRDD: Removing RDD 64 from persistence list
17/04/17 13:50:40 INFO BlockManager: Removing RDD 64
17/04/17 13:50:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[64] at createStream at KafkaConsumer.java:64 of time 1492417240000 ms
17/04/17 13:50:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417220000 ms)
17/04/17 13:50:40 INFO InputInfoTracker: remove old batch metadata: 1492417220000 ms
17/04/17 13:50:50 INFO JobScheduler: Added jobs for time 1492417250000 ms
17/04/17 13:50:50 INFO JobScheduler: Starting job streaming job 1492417250000 ms.0 from job set of time 1492417250000 ms
-------------------------------------------
Time: 1492417250000 ms
-------------------------------------------

17/04/17 13:50:50 INFO JobScheduler: Finished job streaming job 1492417250000 ms.0 from job set of time 1492417250000 ms
17/04/17 13:50:50 INFO MapPartitionsRDD: Removing RDD 69 from persistence list
17/04/17 13:50:50 INFO JobScheduler: Total delay: 0.010 s for time 1492417250000 ms (execution: 0.001 s)
17/04/17 13:50:50 INFO BlockManager: Removing RDD 69
17/04/17 13:50:50 INFO MapPartitionsRDD: Removing RDD 68 from persistence list
17/04/17 13:50:50 INFO BlockManager: Removing RDD 68
17/04/17 13:50:50 INFO BlockRDD: Removing RDD 67 from persistence list
17/04/17 13:50:50 INFO BlockManager: Removing RDD 67
17/04/17 13:50:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[67] at createStream at KafkaConsumer.java:64 of time 1492417250000 ms
17/04/17 13:50:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417230000 ms)
17/04/17 13:50:50 INFO InputInfoTracker: remove old batch metadata: 1492417230000 ms
17/04/17 13:51:00 INFO JobScheduler: Added jobs for time 1492417260000 ms
17/04/17 13:51:00 INFO JobScheduler: Starting job streaming job 1492417260000 ms.0 from job set of time 1492417260000 ms
-------------------------------------------
Time: 1492417260000 ms
-------------------------------------------

17/04/17 13:51:00 INFO JobScheduler: Finished job streaming job 1492417260000 ms.0 from job set of time 1492417260000 ms
17/04/17 13:51:00 INFO MapPartitionsRDD: Removing RDD 72 from persistence list
17/04/17 13:51:00 INFO JobScheduler: Total delay: 0.008 s for time 1492417260000 ms (execution: 0.001 s)
17/04/17 13:51:00 INFO BlockManager: Removing RDD 72
17/04/17 13:51:00 INFO MapPartitionsRDD: Removing RDD 71 from persistence list
17/04/17 13:51:00 INFO BlockManager: Removing RDD 71
17/04/17 13:51:00 INFO BlockRDD: Removing RDD 70 from persistence list
17/04/17 13:51:00 INFO BlockManager: Removing RDD 70
17/04/17 13:51:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[70] at createStream at KafkaConsumer.java:64 of time 1492417260000 ms
17/04/17 13:51:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417240000 ms)
17/04/17 13:51:00 INFO InputInfoTracker: remove old batch metadata: 1492417240000 ms
17/04/17 13:51:10 INFO JobScheduler: Added jobs for time 1492417270000 ms
17/04/17 13:51:10 INFO JobScheduler: Starting job streaming job 1492417270000 ms.0 from job set of time 1492417270000 ms
-------------------------------------------
Time: 1492417270000 ms
-------------------------------------------

17/04/17 13:51:10 INFO JobScheduler: Finished job streaming job 1492417270000 ms.0 from job set of time 1492417270000 ms
17/04/17 13:51:10 INFO MapPartitionsRDD: Removing RDD 75 from persistence list
17/04/17 13:51:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417270000 ms (execution: 0.001 s)
17/04/17 13:51:10 INFO BlockManager: Removing RDD 75
17/04/17 13:51:10 INFO MapPartitionsRDD: Removing RDD 74 from persistence list
17/04/17 13:51:10 INFO BlockRDD: Removing RDD 73 from persistence list
17/04/17 13:51:10 INFO BlockManager: Removing RDD 74
17/04/17 13:51:10 INFO BlockManager: Removing RDD 73
17/04/17 13:51:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[73] at createStream at KafkaConsumer.java:64 of time 1492417270000 ms
17/04/17 13:51:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417250000 ms)
17/04/17 13:51:10 INFO InputInfoTracker: remove old batch metadata: 1492417250000 ms
17/04/17 13:51:20 INFO JobScheduler: Added jobs for time 1492417280000 ms
-------------------------------------------
Time: 1492417280000 ms
-------------------------------------------

17/04/17 13:51:20 INFO JobScheduler: Starting job streaming job 1492417280000 ms.0 from job set of time 1492417280000 ms
17/04/17 13:51:20 INFO JobScheduler: Finished job streaming job 1492417280000 ms.0 from job set of time 1492417280000 ms
17/04/17 13:51:20 INFO MapPartitionsRDD: Removing RDD 78 from persistence list
17/04/17 13:51:20 INFO JobScheduler: Total delay: 0.009 s for time 1492417280000 ms (execution: 0.001 s)
17/04/17 13:51:20 INFO BlockManager: Removing RDD 78
17/04/17 13:51:20 INFO MapPartitionsRDD: Removing RDD 77 from persistence list
17/04/17 13:51:20 INFO BlockManager: Removing RDD 77
17/04/17 13:51:20 INFO BlockRDD: Removing RDD 76 from persistence list
17/04/17 13:51:20 INFO BlockManager: Removing RDD 76
17/04/17 13:51:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[76] at createStream at KafkaConsumer.java:64 of time 1492417280000 ms
17/04/17 13:51:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417260000 ms)
17/04/17 13:51:20 INFO InputInfoTracker: remove old batch metadata: 1492417260000 ms
17/04/17 13:51:30 INFO JobScheduler: Added jobs for time 1492417290000 ms
17/04/17 13:51:30 INFO JobScheduler: Starting job streaming job 1492417290000 ms.0 from job set of time 1492417290000 ms
-------------------------------------------
Time: 1492417290000 ms
-------------------------------------------

17/04/17 13:51:30 INFO JobScheduler: Finished job streaming job 1492417290000 ms.0 from job set of time 1492417290000 ms
17/04/17 13:51:30 INFO JobScheduler: Total delay: 0.009 s for time 1492417290000 ms (execution: 0.001 s)
17/04/17 13:51:30 INFO MapPartitionsRDD: Removing RDD 81 from persistence list
17/04/17 13:51:30 INFO BlockManager: Removing RDD 81
17/04/17 13:51:30 INFO MapPartitionsRDD: Removing RDD 80 from persistence list
17/04/17 13:51:30 INFO BlockRDD: Removing RDD 79 from persistence list
17/04/17 13:51:30 INFO BlockManager: Removing RDD 80
17/04/17 13:51:30 INFO BlockManager: Removing RDD 79
17/04/17 13:51:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[79] at createStream at KafkaConsumer.java:64 of time 1492417290000 ms
17/04/17 13:51:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417270000 ms)
17/04/17 13:51:30 INFO InputInfoTracker: remove old batch metadata: 1492417270000 ms
17/04/17 13:51:40 INFO JobScheduler: Added jobs for time 1492417300000 ms
17/04/17 13:51:40 INFO JobScheduler: Starting job streaming job 1492417300000 ms.0 from job set of time 1492417300000 ms
-------------------------------------------
Time: 1492417300000 ms
-------------------------------------------

17/04/17 13:51:40 INFO JobScheduler: Finished job streaming job 1492417300000 ms.0 from job set of time 1492417300000 ms
17/04/17 13:51:40 INFO MapPartitionsRDD: Removing RDD 84 from persistence list
17/04/17 13:51:40 INFO JobScheduler: Total delay: 0.008 s for time 1492417300000 ms (execution: 0.001 s)
17/04/17 13:51:40 INFO BlockManager: Removing RDD 84
17/04/17 13:51:40 INFO MapPartitionsRDD: Removing RDD 83 from persistence list
17/04/17 13:51:40 INFO BlockManager: Removing RDD 83
17/04/17 13:51:40 INFO BlockRDD: Removing RDD 82 from persistence list
17/04/17 13:51:40 INFO BlockManager: Removing RDD 82
17/04/17 13:51:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[82] at createStream at KafkaConsumer.java:64 of time 1492417300000 ms
17/04/17 13:51:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417280000 ms)
17/04/17 13:51:40 INFO InputInfoTracker: remove old batch metadata: 1492417280000 ms
17/04/17 13:51:50 INFO JobScheduler: Added jobs for time 1492417310000 ms
17/04/17 13:51:50 INFO JobScheduler: Starting job streaming job 1492417310000 ms.0 from job set of time 1492417310000 ms
-------------------------------------------
Time: 1492417310000 ms
-------------------------------------------

17/04/17 13:51:50 INFO JobScheduler: Finished job streaming job 1492417310000 ms.0 from job set of time 1492417310000 ms
17/04/17 13:51:50 INFO JobScheduler: Total delay: 0.008 s for time 1492417310000 ms (execution: 0.001 s)
17/04/17 13:51:50 INFO MapPartitionsRDD: Removing RDD 87 from persistence list
17/04/17 13:51:50 INFO BlockManager: Removing RDD 87
17/04/17 13:51:50 INFO MapPartitionsRDD: Removing RDD 86 from persistence list
17/04/17 13:51:50 INFO BlockManager: Removing RDD 86
17/04/17 13:51:50 INFO BlockRDD: Removing RDD 85 from persistence list
17/04/17 13:51:50 INFO BlockManager: Removing RDD 85
17/04/17 13:51:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[85] at createStream at KafkaConsumer.java:64 of time 1492417310000 ms
17/04/17 13:51:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417290000 ms)
17/04/17 13:51:50 INFO InputInfoTracker: remove old batch metadata: 1492417290000 ms
17/04/17 13:52:00 INFO JobScheduler: Added jobs for time 1492417320000 ms
17/04/17 13:52:00 INFO JobScheduler: Starting job streaming job 1492417320000 ms.0 from job set of time 1492417320000 ms
-------------------------------------------
Time: 1492417320000 ms
-------------------------------------------

17/04/17 13:52:00 INFO JobScheduler: Finished job streaming job 1492417320000 ms.0 from job set of time 1492417320000 ms
17/04/17 13:52:00 INFO MapPartitionsRDD: Removing RDD 90 from persistence list
17/04/17 13:52:00 INFO JobScheduler: Total delay: 0.009 s for time 1492417320000 ms (execution: 0.000 s)
17/04/17 13:52:00 INFO BlockManager: Removing RDD 90
17/04/17 13:52:00 INFO MapPartitionsRDD: Removing RDD 89 from persistence list
17/04/17 13:52:00 INFO BlockManager: Removing RDD 89
17/04/17 13:52:00 INFO BlockRDD: Removing RDD 88 from persistence list
17/04/17 13:52:00 INFO BlockManager: Removing RDD 88
17/04/17 13:52:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[88] at createStream at KafkaConsumer.java:64 of time 1492417320000 ms
17/04/17 13:52:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417300000 ms)
17/04/17 13:52:00 INFO InputInfoTracker: remove old batch metadata: 1492417300000 ms
17/04/17 13:52:10 INFO JobScheduler: Added jobs for time 1492417330000 ms
-------------------------------------------
Time: 1492417330000 ms
-------------------------------------------

17/04/17 13:52:10 INFO JobScheduler: Starting job streaming job 1492417330000 ms.0 from job set of time 1492417330000 ms
17/04/17 13:52:10 INFO JobScheduler: Finished job streaming job 1492417330000 ms.0 from job set of time 1492417330000 ms
17/04/17 13:52:10 INFO MapPartitionsRDD: Removing RDD 93 from persistence list
17/04/17 13:52:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417330000 ms (execution: 0.000 s)
17/04/17 13:52:10 INFO BlockManager: Removing RDD 93
17/04/17 13:52:10 INFO MapPartitionsRDD: Removing RDD 92 from persistence list
17/04/17 13:52:10 INFO BlockManager: Removing RDD 92
17/04/17 13:52:10 INFO BlockRDD: Removing RDD 91 from persistence list
17/04/17 13:52:10 INFO BlockManager: Removing RDD 91
17/04/17 13:52:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[91] at createStream at KafkaConsumer.java:64 of time 1492417330000 ms
17/04/17 13:52:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417310000 ms)
17/04/17 13:52:10 INFO InputInfoTracker: remove old batch metadata: 1492417310000 ms
17/04/17 13:52:20 INFO JobScheduler: Added jobs for time 1492417340000 ms
17/04/17 13:52:20 INFO JobScheduler: Starting job streaming job 1492417340000 ms.0 from job set of time 1492417340000 ms
-------------------------------------------
Time: 1492417340000 ms
-------------------------------------------

17/04/17 13:52:20 INFO JobScheduler: Finished job streaming job 1492417340000 ms.0 from job set of time 1492417340000 ms
17/04/17 13:52:20 INFO JobScheduler: Total delay: 0.008 s for time 1492417340000 ms (execution: 0.001 s)
17/04/17 13:52:20 INFO MapPartitionsRDD: Removing RDD 96 from persistence list
17/04/17 13:52:20 INFO BlockManager: Removing RDD 96
17/04/17 13:52:20 INFO MapPartitionsRDD: Removing RDD 95 from persistence list
17/04/17 13:52:20 INFO BlockManager: Removing RDD 95
17/04/17 13:52:20 INFO BlockRDD: Removing RDD 94 from persistence list
17/04/17 13:52:20 INFO BlockManager: Removing RDD 94
17/04/17 13:52:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[94] at createStream at KafkaConsumer.java:64 of time 1492417340000 ms
17/04/17 13:52:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417320000 ms)
17/04/17 13:52:20 INFO InputInfoTracker: remove old batch metadata: 1492417320000 ms
17/04/17 13:52:30 INFO JobScheduler: Added jobs for time 1492417350000 ms
17/04/17 13:52:30 INFO JobScheduler: Starting job streaming job 1492417350000 ms.0 from job set of time 1492417350000 ms
-------------------------------------------
Time: 1492417350000 ms
-------------------------------------------

17/04/17 13:52:30 INFO JobScheduler: Finished job streaming job 1492417350000 ms.0 from job set of time 1492417350000 ms
17/04/17 13:52:30 INFO MapPartitionsRDD: Removing RDD 99 from persistence list
17/04/17 13:52:30 INFO JobScheduler: Total delay: 0.008 s for time 1492417350000 ms (execution: 0.001 s)
17/04/17 13:52:30 INFO BlockManager: Removing RDD 99
17/04/17 13:52:30 INFO MapPartitionsRDD: Removing RDD 98 from persistence list
17/04/17 13:52:30 INFO BlockManager: Removing RDD 98
17/04/17 13:52:30 INFO BlockRDD: Removing RDD 97 from persistence list
17/04/17 13:52:30 INFO BlockManager: Removing RDD 97
17/04/17 13:52:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[97] at createStream at KafkaConsumer.java:64 of time 1492417350000 ms
17/04/17 13:52:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417330000 ms)
17/04/17 13:52:30 INFO InputInfoTracker: remove old batch metadata: 1492417330000 ms
17/04/17 13:52:40 INFO JobScheduler: Added jobs for time 1492417360000 ms
17/04/17 13:52:40 INFO JobScheduler: Starting job streaming job 1492417360000 ms.0 from job set of time 1492417360000 ms
-------------------------------------------
Time: 1492417360000 ms
-------------------------------------------

17/04/17 13:52:40 INFO JobScheduler: Finished job streaming job 1492417360000 ms.0 from job set of time 1492417360000 ms
17/04/17 13:52:40 INFO MapPartitionsRDD: Removing RDD 102 from persistence list
17/04/17 13:52:40 INFO JobScheduler: Total delay: 0.009 s for time 1492417360000 ms (execution: 0.001 s)
17/04/17 13:52:40 INFO BlockManager: Removing RDD 102
17/04/17 13:52:40 INFO MapPartitionsRDD: Removing RDD 101 from persistence list
17/04/17 13:52:40 INFO BlockManager: Removing RDD 101
17/04/17 13:52:40 INFO BlockRDD: Removing RDD 100 from persistence list
17/04/17 13:52:40 INFO BlockManager: Removing RDD 100
17/04/17 13:52:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[100] at createStream at KafkaConsumer.java:64 of time 1492417360000 ms
17/04/17 13:52:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417340000 ms)
17/04/17 13:52:40 INFO InputInfoTracker: remove old batch metadata: 1492417340000 ms
17/04/17 13:52:50 INFO JobScheduler: Added jobs for time 1492417370000 ms
17/04/17 13:52:50 INFO JobScheduler: Starting job streaming job 1492417370000 ms.0 from job set of time 1492417370000 ms
-------------------------------------------
Time: 1492417370000 ms
-------------------------------------------

17/04/17 13:52:50 INFO JobScheduler: Finished job streaming job 1492417370000 ms.0 from job set of time 1492417370000 ms
17/04/17 13:52:50 INFO MapPartitionsRDD: Removing RDD 105 from persistence list
17/04/17 13:52:50 INFO JobScheduler: Total delay: 0.008 s for time 1492417370000 ms (execution: 0.001 s)
17/04/17 13:52:50 INFO BlockManager: Removing RDD 105
17/04/17 13:52:50 INFO MapPartitionsRDD: Removing RDD 104 from persistence list
17/04/17 13:52:50 INFO BlockManager: Removing RDD 104
17/04/17 13:52:50 INFO BlockRDD: Removing RDD 103 from persistence list
17/04/17 13:52:50 INFO BlockManager: Removing RDD 103
17/04/17 13:52:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[103] at createStream at KafkaConsumer.java:64 of time 1492417370000 ms
17/04/17 13:52:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417350000 ms)
17/04/17 13:52:50 INFO InputInfoTracker: remove old batch metadata: 1492417350000 ms
17/04/17 13:53:00 INFO JobScheduler: Added jobs for time 1492417380000 ms
17/04/17 13:53:00 INFO JobScheduler: Starting job streaming job 1492417380000 ms.0 from job set of time 1492417380000 ms
-------------------------------------------
Time: 1492417380000 ms
-------------------------------------------

17/04/17 13:53:00 INFO JobScheduler: Finished job streaming job 1492417380000 ms.0 from job set of time 1492417380000 ms
17/04/17 13:53:00 INFO MapPartitionsRDD: Removing RDD 108 from persistence list
17/04/17 13:53:00 INFO JobScheduler: Total delay: 0.008 s for time 1492417380000 ms (execution: 0.000 s)
17/04/17 13:53:00 INFO BlockManager: Removing RDD 108
17/04/17 13:53:00 INFO MapPartitionsRDD: Removing RDD 107 from persistence list
17/04/17 13:53:00 INFO BlockManager: Removing RDD 107
17/04/17 13:53:00 INFO BlockRDD: Removing RDD 106 from persistence list
17/04/17 13:53:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[106] at createStream at KafkaConsumer.java:64 of time 1492417380000 ms
17/04/17 13:53:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417360000 ms)
17/04/17 13:53:00 INFO InputInfoTracker: remove old batch metadata: 1492417360000 ms
17/04/17 13:53:00 INFO BlockManager: Removing RDD 106
17/04/17 13:53:10 INFO JobScheduler: Added jobs for time 1492417390000 ms
17/04/17 13:53:10 INFO JobScheduler: Starting job streaming job 1492417390000 ms.0 from job set of time 1492417390000 ms
-------------------------------------------
Time: 1492417390000 ms
-------------------------------------------

17/04/17 13:53:10 INFO JobScheduler: Finished job streaming job 1492417390000 ms.0 from job set of time 1492417390000 ms
17/04/17 13:53:10 INFO MapPartitionsRDD: Removing RDD 111 from persistence list
17/04/17 13:53:10 INFO JobScheduler: Total delay: 0.006 s for time 1492417390000 ms (execution: 0.000 s)
17/04/17 13:53:10 INFO BlockManager: Removing RDD 111
17/04/17 13:53:10 INFO MapPartitionsRDD: Removing RDD 110 from persistence list
17/04/17 13:53:10 INFO BlockManager: Removing RDD 110
17/04/17 13:53:10 INFO BlockRDD: Removing RDD 109 from persistence list
17/04/17 13:53:10 INFO BlockManager: Removing RDD 109
17/04/17 13:53:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[109] at createStream at KafkaConsumer.java:64 of time 1492417390000 ms
17/04/17 13:53:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417370000 ms)
17/04/17 13:53:10 INFO InputInfoTracker: remove old batch metadata: 1492417370000 ms
17/04/17 13:53:20 INFO JobScheduler: Added jobs for time 1492417400000 ms
17/04/17 13:53:20 INFO JobScheduler: Starting job streaming job 1492417400000 ms.0 from job set of time 1492417400000 ms
-------------------------------------------
Time: 1492417400000 ms
-------------------------------------------

17/04/17 13:53:20 INFO JobScheduler: Finished job streaming job 1492417400000 ms.0 from job set of time 1492417400000 ms
17/04/17 13:53:20 INFO MapPartitionsRDD: Removing RDD 114 from persistence list
17/04/17 13:53:20 INFO JobScheduler: Total delay: 0.009 s for time 1492417400000 ms (execution: 0.001 s)
17/04/17 13:53:20 INFO BlockManager: Removing RDD 114
17/04/17 13:53:20 INFO MapPartitionsRDD: Removing RDD 113 from persistence list
17/04/17 13:53:20 INFO BlockManager: Removing RDD 113
17/04/17 13:53:20 INFO BlockRDD: Removing RDD 112 from persistence list
17/04/17 13:53:20 INFO BlockManager: Removing RDD 112
17/04/17 13:53:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[112] at createStream at KafkaConsumer.java:64 of time 1492417400000 ms
17/04/17 13:53:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417380000 ms)
17/04/17 13:53:20 INFO InputInfoTracker: remove old batch metadata: 1492417380000 ms
17/04/17 13:53:30 INFO JobScheduler: Added jobs for time 1492417410000 ms
17/04/17 13:53:30 INFO JobScheduler: Starting job streaming job 1492417410000 ms.0 from job set of time 1492417410000 ms
-------------------------------------------
Time: 1492417410000 ms
-------------------------------------------

17/04/17 13:53:30 INFO JobScheduler: Finished job streaming job 1492417410000 ms.0 from job set of time 1492417410000 ms
17/04/17 13:53:30 INFO JobScheduler: Total delay: 0.009 s for time 1492417410000 ms (execution: 0.001 s)
17/04/17 13:53:30 INFO MapPartitionsRDD: Removing RDD 117 from persistence list
17/04/17 13:53:30 INFO BlockManager: Removing RDD 117
17/04/17 13:53:30 INFO MapPartitionsRDD: Removing RDD 116 from persistence list
17/04/17 13:53:30 INFO BlockManager: Removing RDD 116
17/04/17 13:53:30 INFO BlockRDD: Removing RDD 115 from persistence list
17/04/17 13:53:30 INFO BlockManager: Removing RDD 115
17/04/17 13:53:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[115] at createStream at KafkaConsumer.java:64 of time 1492417410000 ms
17/04/17 13:53:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417390000 ms)
17/04/17 13:53:30 INFO InputInfoTracker: remove old batch metadata: 1492417390000 ms
17/04/17 13:53:40 INFO JobScheduler: Added jobs for time 1492417420000 ms
17/04/17 13:53:40 INFO JobScheduler: Starting job streaming job 1492417420000 ms.0 from job set of time 1492417420000 ms
-------------------------------------------
Time: 1492417420000 ms
-------------------------------------------

17/04/17 13:53:40 INFO JobScheduler: Finished job streaming job 1492417420000 ms.0 from job set of time 1492417420000 ms
17/04/17 13:53:40 INFO MapPartitionsRDD: Removing RDD 120 from persistence list
17/04/17 13:53:40 INFO JobScheduler: Total delay: 0.006 s for time 1492417420000 ms (execution: 0.000 s)
17/04/17 13:53:40 INFO BlockManager: Removing RDD 120
17/04/17 13:53:40 INFO MapPartitionsRDD: Removing RDD 119 from persistence list
17/04/17 13:53:40 INFO BlockManager: Removing RDD 119
17/04/17 13:53:40 INFO BlockRDD: Removing RDD 118 from persistence list
17/04/17 13:53:40 INFO BlockManager: Removing RDD 118
17/04/17 13:53:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[118] at createStream at KafkaConsumer.java:64 of time 1492417420000 ms
17/04/17 13:53:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417400000 ms)
17/04/17 13:53:40 INFO InputInfoTracker: remove old batch metadata: 1492417400000 ms
17/04/17 13:53:50 INFO JobScheduler: Added jobs for time 1492417430000 ms
17/04/17 13:53:50 INFO JobScheduler: Starting job streaming job 1492417430000 ms.0 from job set of time 1492417430000 ms
-------------------------------------------
Time: 1492417430000 ms
-------------------------------------------

17/04/17 13:53:50 INFO JobScheduler: Finished job streaming job 1492417430000 ms.0 from job set of time 1492417430000 ms
17/04/17 13:53:50 INFO MapPartitionsRDD: Removing RDD 123 from persistence list
17/04/17 13:53:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417430000 ms (execution: 0.001 s)
17/04/17 13:53:50 INFO BlockManager: Removing RDD 123
17/04/17 13:53:50 INFO MapPartitionsRDD: Removing RDD 122 from persistence list
17/04/17 13:53:50 INFO BlockManager: Removing RDD 122
17/04/17 13:53:50 INFO BlockRDD: Removing RDD 121 from persistence list
17/04/17 13:53:50 INFO BlockManager: Removing RDD 121
17/04/17 13:53:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[121] at createStream at KafkaConsumer.java:64 of time 1492417430000 ms
17/04/17 13:53:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417410000 ms)
17/04/17 13:53:50 INFO InputInfoTracker: remove old batch metadata: 1492417410000 ms
17/04/17 13:54:00 INFO JobScheduler: Added jobs for time 1492417440000 ms
17/04/17 13:54:00 INFO JobScheduler: Starting job streaming job 1492417440000 ms.0 from job set of time 1492417440000 ms
-------------------------------------------
Time: 1492417440000 ms
-------------------------------------------

17/04/17 13:54:00 INFO JobScheduler: Finished job streaming job 1492417440000 ms.0 from job set of time 1492417440000 ms
17/04/17 13:54:00 INFO MapPartitionsRDD: Removing RDD 126 from persistence list
17/04/17 13:54:00 INFO JobScheduler: Total delay: 0.009 s for time 1492417440000 ms (execution: 0.001 s)
17/04/17 13:54:00 INFO BlockManager: Removing RDD 126
17/04/17 13:54:00 INFO MapPartitionsRDD: Removing RDD 125 from persistence list
17/04/17 13:54:00 INFO BlockManager: Removing RDD 125
17/04/17 13:54:00 INFO BlockRDD: Removing RDD 124 from persistence list
17/04/17 13:54:00 INFO BlockManager: Removing RDD 124
17/04/17 13:54:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[124] at createStream at KafkaConsumer.java:64 of time 1492417440000 ms
17/04/17 13:54:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417420000 ms)
17/04/17 13:54:00 INFO InputInfoTracker: remove old batch metadata: 1492417420000 ms
17/04/17 13:54:10 INFO JobScheduler: Added jobs for time 1492417450000 ms
-------------------------------------------
Time: 1492417450000 ms
-------------------------------------------

17/04/17 13:54:10 INFO JobScheduler: Starting job streaming job 1492417450000 ms.0 from job set of time 1492417450000 ms
17/04/17 13:54:10 INFO JobScheduler: Finished job streaming job 1492417450000 ms.0 from job set of time 1492417450000 ms
17/04/17 13:54:10 INFO JobScheduler: Total delay: 0.009 s for time 1492417450000 ms (execution: 0.000 s)
17/04/17 13:54:10 INFO MapPartitionsRDD: Removing RDD 129 from persistence list
17/04/17 13:54:10 INFO BlockManager: Removing RDD 129
17/04/17 13:54:10 INFO MapPartitionsRDD: Removing RDD 128 from persistence list
17/04/17 13:54:10 INFO BlockRDD: Removing RDD 127 from persistence list
17/04/17 13:54:10 INFO BlockManager: Removing RDD 128
17/04/17 13:54:10 INFO BlockManager: Removing RDD 127
17/04/17 13:54:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[127] at createStream at KafkaConsumer.java:64 of time 1492417450000 ms
17/04/17 13:54:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417430000 ms)
17/04/17 13:54:10 INFO InputInfoTracker: remove old batch metadata: 1492417430000 ms
17/04/17 13:54:20 INFO JobScheduler: Starting job streaming job 1492417460000 ms.0 from job set of time 1492417460000 ms
17/04/17 13:54:20 INFO JobScheduler: Added jobs for time 1492417460000 ms
-------------------------------------------
Time: 1492417460000 ms
-------------------------------------------

17/04/17 13:54:20 INFO JobScheduler: Finished job streaming job 1492417460000 ms.0 from job set of time 1492417460000 ms
17/04/17 13:54:20 INFO MapPartitionsRDD: Removing RDD 132 from persistence list
17/04/17 13:54:20 INFO JobScheduler: Total delay: 0.007 s for time 1492417460000 ms (execution: 0.001 s)
17/04/17 13:54:20 INFO BlockManager: Removing RDD 132
17/04/17 13:54:20 INFO MapPartitionsRDD: Removing RDD 131 from persistence list
17/04/17 13:54:20 INFO BlockManager: Removing RDD 131
17/04/17 13:54:20 INFO BlockRDD: Removing RDD 130 from persistence list
17/04/17 13:54:20 INFO BlockManager: Removing RDD 130
17/04/17 13:54:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[130] at createStream at KafkaConsumer.java:64 of time 1492417460000 ms
17/04/17 13:54:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417440000 ms)
17/04/17 13:54:20 INFO InputInfoTracker: remove old batch metadata: 1492417440000 ms
17/04/17 13:54:30 INFO JobScheduler: Starting job streaming job 1492417470000 ms.0 from job set of time 1492417470000 ms
-------------------------------------------
Time: 1492417470000 ms
-------------------------------------------

17/04/17 13:54:30 INFO JobScheduler: Added jobs for time 1492417470000 ms
17/04/17 13:54:30 INFO JobScheduler: Finished job streaming job 1492417470000 ms.0 from job set of time 1492417470000 ms
17/04/17 13:54:30 INFO MapPartitionsRDD: Removing RDD 135 from persistence list
17/04/17 13:54:30 INFO JobScheduler: Total delay: 0.007 s for time 1492417470000 ms (execution: 0.001 s)
17/04/17 13:54:30 INFO MapPartitionsRDD: Removing RDD 134 from persistence list
17/04/17 13:54:30 INFO BlockManager: Removing RDD 135
17/04/17 13:54:30 INFO BlockManager: Removing RDD 134
17/04/17 13:54:30 INFO BlockRDD: Removing RDD 133 from persistence list
17/04/17 13:54:30 INFO BlockManager: Removing RDD 133
17/04/17 13:54:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[133] at createStream at KafkaConsumer.java:64 of time 1492417470000 ms
17/04/17 13:54:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417450000 ms)
17/04/17 13:54:30 INFO InputInfoTracker: remove old batch metadata: 1492417450000 ms
17/04/17 13:54:40 INFO JobScheduler: Added jobs for time 1492417480000 ms
17/04/17 13:54:40 INFO JobScheduler: Starting job streaming job 1492417480000 ms.0 from job set of time 1492417480000 ms
-------------------------------------------
Time: 1492417480000 ms
-------------------------------------------

17/04/17 13:54:40 INFO JobScheduler: Finished job streaming job 1492417480000 ms.0 from job set of time 1492417480000 ms
17/04/17 13:54:40 INFO MapPartitionsRDD: Removing RDD 138 from persistence list
17/04/17 13:54:40 INFO JobScheduler: Total delay: 0.007 s for time 1492417480000 ms (execution: 0.001 s)
17/04/17 13:54:40 INFO BlockManager: Removing RDD 138
17/04/17 13:54:40 INFO MapPartitionsRDD: Removing RDD 137 from persistence list
17/04/17 13:54:40 INFO BlockRDD: Removing RDD 136 from persistence list
17/04/17 13:54:40 INFO BlockManager: Removing RDD 137
17/04/17 13:54:40 INFO BlockManager: Removing RDD 136
17/04/17 13:54:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[136] at createStream at KafkaConsumer.java:64 of time 1492417480000 ms
17/04/17 13:54:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417460000 ms)
17/04/17 13:54:40 INFO InputInfoTracker: remove old batch metadata: 1492417460000 ms
17/04/17 13:54:50 INFO JobScheduler: Added jobs for time 1492417490000 ms
17/04/17 13:54:50 INFO JobScheduler: Starting job streaming job 1492417490000 ms.0 from job set of time 1492417490000 ms
-------------------------------------------
Time: 1492417490000 ms
-------------------------------------------

17/04/17 13:54:50 INFO JobScheduler: Finished job streaming job 1492417490000 ms.0 from job set of time 1492417490000 ms
17/04/17 13:54:50 INFO MapPartitionsRDD: Removing RDD 141 from persistence list
17/04/17 13:54:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417490000 ms (execution: 0.000 s)
17/04/17 13:54:50 INFO BlockManager: Removing RDD 141
17/04/17 13:54:50 INFO MapPartitionsRDD: Removing RDD 140 from persistence list
17/04/17 13:54:50 INFO BlockManager: Removing RDD 140
17/04/17 13:54:50 INFO BlockRDD: Removing RDD 139 from persistence list
17/04/17 13:54:50 INFO BlockManager: Removing RDD 139
17/04/17 13:54:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[139] at createStream at KafkaConsumer.java:64 of time 1492417490000 ms
17/04/17 13:54:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417470000 ms)
17/04/17 13:54:50 INFO InputInfoTracker: remove old batch metadata: 1492417470000 ms
17/04/17 13:55:00 INFO JobScheduler: Added jobs for time 1492417500000 ms
-------------------------------------------
Time: 1492417500000 ms
-------------------------------------------

17/04/17 13:55:00 INFO JobScheduler: Starting job streaming job 1492417500000 ms.0 from job set of time 1492417500000 ms
17/04/17 13:55:00 INFO JobScheduler: Finished job streaming job 1492417500000 ms.0 from job set of time 1492417500000 ms
17/04/17 13:55:00 INFO MapPartitionsRDD: Removing RDD 144 from persistence list
17/04/17 13:55:00 INFO JobScheduler: Total delay: 0.008 s for time 1492417500000 ms (execution: 0.001 s)
17/04/17 13:55:00 INFO BlockManager: Removing RDD 144
17/04/17 13:55:00 INFO MapPartitionsRDD: Removing RDD 143 from persistence list
17/04/17 13:55:00 INFO BlockManager: Removing RDD 143
17/04/17 13:55:00 INFO BlockRDD: Removing RDD 142 from persistence list
17/04/17 13:55:00 INFO BlockManager: Removing RDD 142
17/04/17 13:55:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[142] at createStream at KafkaConsumer.java:64 of time 1492417500000 ms
17/04/17 13:55:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417480000 ms)
17/04/17 13:55:00 INFO InputInfoTracker: remove old batch metadata: 1492417480000 ms
17/04/17 13:55:10 INFO JobScheduler: Added jobs for time 1492417510000 ms
17/04/17 13:55:10 INFO JobScheduler: Starting job streaming job 1492417510000 ms.0 from job set of time 1492417510000 ms
-------------------------------------------
Time: 1492417510000 ms
-------------------------------------------

17/04/17 13:55:10 INFO JobScheduler: Finished job streaming job 1492417510000 ms.0 from job set of time 1492417510000 ms
17/04/17 13:55:10 INFO MapPartitionsRDD: Removing RDD 147 from persistence list
17/04/17 13:55:10 INFO MapPartitionsRDD: Removing RDD 146 from persistence list
17/04/17 13:55:10 INFO BlockManager: Removing RDD 147
17/04/17 13:55:10 INFO BlockManager: Removing RDD 146
17/04/17 13:55:10 INFO BlockRDD: Removing RDD 145 from persistence list
17/04/17 13:55:10 INFO BlockManager: Removing RDD 145
17/04/17 13:55:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[145] at createStream at KafkaConsumer.java:64 of time 1492417510000 ms
17/04/17 13:55:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417490000 ms)
17/04/17 13:55:10 INFO InputInfoTracker: remove old batch metadata: 1492417490000 ms
17/04/17 13:55:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417510000 ms (execution: 0.001 s)
17/04/17 13:55:20 INFO JobScheduler: Added jobs for time 1492417520000 ms
17/04/17 13:55:20 INFO JobScheduler: Starting job streaming job 1492417520000 ms.0 from job set of time 1492417520000 ms
-------------------------------------------
Time: 1492417520000 ms
-------------------------------------------

17/04/17 13:55:20 INFO JobScheduler: Finished job streaming job 1492417520000 ms.0 from job set of time 1492417520000 ms
17/04/17 13:55:20 INFO MapPartitionsRDD: Removing RDD 150 from persistence list
17/04/17 13:55:20 INFO JobScheduler: Total delay: 0.007 s for time 1492417520000 ms (execution: 0.001 s)
17/04/17 13:55:20 INFO BlockManager: Removing RDD 150
17/04/17 13:55:20 INFO MapPartitionsRDD: Removing RDD 149 from persistence list
17/04/17 13:55:20 INFO BlockManager: Removing RDD 149
17/04/17 13:55:20 INFO BlockRDD: Removing RDD 148 from persistence list
17/04/17 13:55:20 INFO BlockManager: Removing RDD 148
17/04/17 13:55:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[148] at createStream at KafkaConsumer.java:64 of time 1492417520000 ms
17/04/17 13:55:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417500000 ms)
17/04/17 13:55:20 INFO InputInfoTracker: remove old batch metadata: 1492417500000 ms
17/04/17 13:55:30 INFO JobScheduler: Added jobs for time 1492417530000 ms
17/04/17 13:55:30 INFO JobScheduler: Starting job streaming job 1492417530000 ms.0 from job set of time 1492417530000 ms
-------------------------------------------
Time: 1492417530000 ms
-------------------------------------------

17/04/17 13:55:30 INFO JobScheduler: Finished job streaming job 1492417530000 ms.0 from job set of time 1492417530000 ms
17/04/17 13:55:30 INFO MapPartitionsRDD: Removing RDD 153 from persistence list
17/04/17 13:55:30 INFO JobScheduler: Total delay: 0.006 s for time 1492417530000 ms (execution: 0.001 s)
17/04/17 13:55:30 INFO MapPartitionsRDD: Removing RDD 152 from persistence list
17/04/17 13:55:30 INFO BlockManager: Removing RDD 153
17/04/17 13:55:30 INFO BlockManager: Removing RDD 152
17/04/17 13:55:30 INFO BlockRDD: Removing RDD 151 from persistence list
17/04/17 13:55:30 INFO BlockManager: Removing RDD 151
17/04/17 13:55:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[151] at createStream at KafkaConsumer.java:64 of time 1492417530000 ms
17/04/17 13:55:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417510000 ms)
17/04/17 13:55:30 INFO InputInfoTracker: remove old batch metadata: 1492417510000 ms
17/04/17 13:55:40 INFO JobScheduler: Added jobs for time 1492417540000 ms
17/04/17 13:55:40 INFO JobScheduler: Starting job streaming job 1492417540000 ms.0 from job set of time 1492417540000 ms
-------------------------------------------
Time: 1492417540000 ms
-------------------------------------------

17/04/17 13:55:40 INFO JobScheduler: Finished job streaming job 1492417540000 ms.0 from job set of time 1492417540000 ms
17/04/17 13:55:40 INFO JobScheduler: Total delay: 0.007 s for time 1492417540000 ms (execution: 0.000 s)
17/04/17 13:55:40 INFO MapPartitionsRDD: Removing RDD 156 from persistence list
17/04/17 13:55:40 INFO BlockManager: Removing RDD 156
17/04/17 13:55:40 INFO MapPartitionsRDD: Removing RDD 155 from persistence list
17/04/17 13:55:40 INFO BlockManager: Removing RDD 155
17/04/17 13:55:40 INFO BlockRDD: Removing RDD 154 from persistence list
17/04/17 13:55:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[154] at createStream at KafkaConsumer.java:64 of time 1492417540000 ms
17/04/17 13:55:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417520000 ms)
17/04/17 13:55:40 INFO InputInfoTracker: remove old batch metadata: 1492417520000 ms
17/04/17 13:55:40 INFO BlockManager: Removing RDD 154
17/04/17 13:55:50 INFO JobScheduler: Added jobs for time 1492417550000 ms
-------------------------------------------
Time: 1492417550000 ms
-------------------------------------------

17/04/17 13:55:50 INFO JobScheduler: Starting job streaming job 1492417550000 ms.0 from job set of time 1492417550000 ms
17/04/17 13:55:50 INFO JobScheduler: Finished job streaming job 1492417550000 ms.0 from job set of time 1492417550000 ms
17/04/17 13:55:50 INFO MapPartitionsRDD: Removing RDD 159 from persistence list
17/04/17 13:55:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417550000 ms (execution: 0.000 s)
17/04/17 13:55:50 INFO BlockManager: Removing RDD 159
17/04/17 13:55:50 INFO MapPartitionsRDD: Removing RDD 158 from persistence list
17/04/17 13:55:50 INFO BlockManager: Removing RDD 158
17/04/17 13:55:50 INFO BlockRDD: Removing RDD 157 from persistence list
17/04/17 13:55:50 INFO BlockManager: Removing RDD 157
17/04/17 13:55:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[157] at createStream at KafkaConsumer.java:64 of time 1492417550000 ms
17/04/17 13:55:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417530000 ms)
17/04/17 13:55:50 INFO InputInfoTracker: remove old batch metadata: 1492417530000 ms
17/04/17 13:56:00 INFO JobScheduler: Added jobs for time 1492417560000 ms
-------------------------------------------
Time: 1492417560000 ms
-------------------------------------------

17/04/17 13:56:00 INFO JobScheduler: Starting job streaming job 1492417560000 ms.0 from job set of time 1492417560000 ms
17/04/17 13:56:00 INFO JobScheduler: Finished job streaming job 1492417560000 ms.0 from job set of time 1492417560000 ms
17/04/17 13:56:00 INFO MapPartitionsRDD: Removing RDD 162 from persistence list
17/04/17 13:56:00 INFO JobScheduler: Total delay: 0.007 s for time 1492417560000 ms (execution: 0.001 s)
17/04/17 13:56:00 INFO BlockManager: Removing RDD 162
17/04/17 13:56:00 INFO MapPartitionsRDD: Removing RDD 161 from persistence list
17/04/17 13:56:00 INFO BlockRDD: Removing RDD 160 from persistence list
17/04/17 13:56:00 INFO BlockManager: Removing RDD 161
17/04/17 13:56:00 INFO BlockManager: Removing RDD 160
17/04/17 13:56:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[160] at createStream at KafkaConsumer.java:64 of time 1492417560000 ms
17/04/17 13:56:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417540000 ms)
17/04/17 13:56:00 INFO InputInfoTracker: remove old batch metadata: 1492417540000 ms
17/04/17 13:56:10 INFO JobScheduler: Added jobs for time 1492417570000 ms
17/04/17 13:56:10 INFO JobScheduler: Starting job streaming job 1492417570000 ms.0 from job set of time 1492417570000 ms
-------------------------------------------
Time: 1492417570000 ms
-------------------------------------------

17/04/17 13:56:10 INFO JobScheduler: Finished job streaming job 1492417570000 ms.0 from job set of time 1492417570000 ms
17/04/17 13:56:10 INFO MapPartitionsRDD: Removing RDD 165 from persistence list
17/04/17 13:56:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417570000 ms (execution: 0.001 s)
17/04/17 13:56:10 INFO BlockManager: Removing RDD 165
17/04/17 13:56:10 INFO MapPartitionsRDD: Removing RDD 164 from persistence list
17/04/17 13:56:10 INFO BlockRDD: Removing RDD 163 from persistence list
17/04/17 13:56:10 INFO BlockManager: Removing RDD 163
17/04/17 13:56:10 INFO BlockManager: Removing RDD 164
17/04/17 13:56:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[163] at createStream at KafkaConsumer.java:64 of time 1492417570000 ms
17/04/17 13:56:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417550000 ms)
17/04/17 13:56:10 INFO InputInfoTracker: remove old batch metadata: 1492417550000 ms
17/04/17 13:56:20 INFO JobScheduler: Added jobs for time 1492417580000 ms
17/04/17 13:56:20 INFO JobScheduler: Starting job streaming job 1492417580000 ms.0 from job set of time 1492417580000 ms
-------------------------------------------
Time: 1492417580000 ms
-------------------------------------------

17/04/17 13:56:20 INFO JobScheduler: Finished job streaming job 1492417580000 ms.0 from job set of time 1492417580000 ms
17/04/17 13:56:20 INFO MapPartitionsRDD: Removing RDD 168 from persistence list
17/04/17 13:56:20 INFO JobScheduler: Total delay: 0.007 s for time 1492417580000 ms (execution: 0.001 s)
17/04/17 13:56:20 INFO BlockManager: Removing RDD 168
17/04/17 13:56:20 INFO MapPartitionsRDD: Removing RDD 167 from persistence list
17/04/17 13:56:20 INFO BlockRDD: Removing RDD 166 from persistence list
17/04/17 13:56:20 INFO BlockManager: Removing RDD 166
17/04/17 13:56:20 INFO BlockManager: Removing RDD 167
17/04/17 13:56:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[166] at createStream at KafkaConsumer.java:64 of time 1492417580000 ms
17/04/17 13:56:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417560000 ms)
17/04/17 13:56:20 INFO InputInfoTracker: remove old batch metadata: 1492417560000 ms
17/04/17 13:56:30 INFO JobScheduler: Added jobs for time 1492417590000 ms
-------------------------------------------
Time: 1492417590000 ms
-------------------------------------------

17/04/17 13:56:30 INFO JobScheduler: Starting job streaming job 1492417590000 ms.0 from job set of time 1492417590000 ms
17/04/17 13:56:30 INFO JobScheduler: Finished job streaming job 1492417590000 ms.0 from job set of time 1492417590000 ms
17/04/17 13:56:30 INFO MapPartitionsRDD: Removing RDD 171 from persistence list
17/04/17 13:56:30 INFO JobScheduler: Total delay: 0.008 s for time 1492417590000 ms (execution: 0.000 s)
17/04/17 13:56:30 INFO BlockManager: Removing RDD 171
17/04/17 13:56:30 INFO MapPartitionsRDD: Removing RDD 170 from persistence list
17/04/17 13:56:30 INFO BlockManager: Removing RDD 170
17/04/17 13:56:30 INFO BlockRDD: Removing RDD 169 from persistence list
17/04/17 13:56:30 INFO BlockManager: Removing RDD 169
17/04/17 13:56:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[169] at createStream at KafkaConsumer.java:64 of time 1492417590000 ms
17/04/17 13:56:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417570000 ms)
17/04/17 13:56:30 INFO InputInfoTracker: remove old batch metadata: 1492417570000 ms
17/04/17 13:56:40 INFO JobScheduler: Added jobs for time 1492417600000 ms
17/04/17 13:56:40 INFO JobScheduler: Starting job streaming job 1492417600000 ms.0 from job set of time 1492417600000 ms
-------------------------------------------
Time: 1492417600000 ms
-------------------------------------------

17/04/17 13:56:40 INFO JobScheduler: Finished job streaming job 1492417600000 ms.0 from job set of time 1492417600000 ms
17/04/17 13:56:40 INFO MapPartitionsRDD: Removing RDD 174 from persistence list
17/04/17 13:56:40 INFO JobScheduler: Total delay: 0.006 s for time 1492417600000 ms (execution: 0.000 s)
17/04/17 13:56:40 INFO BlockManager: Removing RDD 174
17/04/17 13:56:40 INFO MapPartitionsRDD: Removing RDD 173 from persistence list
17/04/17 13:56:40 INFO BlockManager: Removing RDD 173
17/04/17 13:56:40 INFO BlockRDD: Removing RDD 172 from persistence list
17/04/17 13:56:40 INFO BlockManager: Removing RDD 172
17/04/17 13:56:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[172] at createStream at KafkaConsumer.java:64 of time 1492417600000 ms
17/04/17 13:56:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417580000 ms)
17/04/17 13:56:40 INFO InputInfoTracker: remove old batch metadata: 1492417580000 ms
17/04/17 13:56:50 INFO JobScheduler: Starting job streaming job 1492417610000 ms.0 from job set of time 1492417610000 ms
-------------------------------------------
Time: 1492417610000 ms
-------------------------------------------

17/04/17 13:56:50 INFO JobScheduler: Added jobs for time 1492417610000 ms
17/04/17 13:56:50 INFO JobScheduler: Finished job streaming job 1492417610000 ms.0 from job set of time 1492417610000 ms
17/04/17 13:56:50 INFO MapPartitionsRDD: Removing RDD 177 from persistence list
17/04/17 13:56:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417610000 ms (execution: 0.001 s)
17/04/17 13:56:50 INFO BlockManager: Removing RDD 177
17/04/17 13:56:50 INFO MapPartitionsRDD: Removing RDD 176 from persistence list
17/04/17 13:56:50 INFO BlockManager: Removing RDD 176
17/04/17 13:56:50 INFO BlockRDD: Removing RDD 175 from persistence list
17/04/17 13:56:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[175] at createStream at KafkaConsumer.java:64 of time 1492417610000 ms
17/04/17 13:56:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417590000 ms)
17/04/17 13:56:50 INFO InputInfoTracker: remove old batch metadata: 1492417590000 ms
17/04/17 13:56:50 INFO BlockManager: Removing RDD 175
17/04/17 13:57:00 INFO JobScheduler: Added jobs for time 1492417620000 ms
17/04/17 13:57:00 INFO JobScheduler: Starting job streaming job 1492417620000 ms.0 from job set of time 1492417620000 ms
-------------------------------------------
Time: 1492417620000 ms
-------------------------------------------

17/04/17 13:57:00 INFO JobScheduler: Finished job streaming job 1492417620000 ms.0 from job set of time 1492417620000 ms
17/04/17 13:57:00 INFO JobScheduler: Total delay: 0.009 s for time 1492417620000 ms (execution: 0.001 s)
17/04/17 13:57:00 INFO MapPartitionsRDD: Removing RDD 180 from persistence list
17/04/17 13:57:00 INFO BlockManager: Removing RDD 180
17/04/17 13:57:00 INFO MapPartitionsRDD: Removing RDD 179 from persistence list
17/04/17 13:57:00 INFO BlockManager: Removing RDD 179
17/04/17 13:57:00 INFO BlockRDD: Removing RDD 178 from persistence list
17/04/17 13:57:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[178] at createStream at KafkaConsumer.java:64 of time 1492417620000 ms
17/04/17 13:57:00 INFO BlockManager: Removing RDD 178
17/04/17 13:57:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417600000 ms)
17/04/17 13:57:00 INFO InputInfoTracker: remove old batch metadata: 1492417600000 ms
17/04/17 13:57:10 INFO JobScheduler: Added jobs for time 1492417630000 ms
17/04/17 13:57:10 INFO JobScheduler: Starting job streaming job 1492417630000 ms.0 from job set of time 1492417630000 ms
-------------------------------------------
Time: 1492417630000 ms
-------------------------------------------

17/04/17 13:57:10 INFO JobScheduler: Finished job streaming job 1492417630000 ms.0 from job set of time 1492417630000 ms
17/04/17 13:57:10 INFO MapPartitionsRDD: Removing RDD 183 from persistence list
17/04/17 13:57:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417630000 ms (execution: 0.001 s)
17/04/17 13:57:10 INFO BlockManager: Removing RDD 183
17/04/17 13:57:10 INFO MapPartitionsRDD: Removing RDD 182 from persistence list
17/04/17 13:57:10 INFO BlockManager: Removing RDD 182
17/04/17 13:57:10 INFO BlockRDD: Removing RDD 181 from persistence list
17/04/17 13:57:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[181] at createStream at KafkaConsumer.java:64 of time 1492417630000 ms
17/04/17 13:57:10 INFO BlockManager: Removing RDD 181
17/04/17 13:57:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417610000 ms)
17/04/17 13:57:10 INFO InputInfoTracker: remove old batch metadata: 1492417610000 ms
17/04/17 13:57:20 INFO JobScheduler: Added jobs for time 1492417640000 ms
-------------------------------------------
Time: 1492417640000 ms
-------------------------------------------

17/04/17 13:57:20 INFO JobScheduler: Starting job streaming job 1492417640000 ms.0 from job set of time 1492417640000 ms
17/04/17 13:57:20 INFO JobScheduler: Finished job streaming job 1492417640000 ms.0 from job set of time 1492417640000 ms
17/04/17 13:57:20 INFO MapPartitionsRDD: Removing RDD 186 from persistence list
17/04/17 13:57:20 INFO JobScheduler: Total delay: 0.007 s for time 1492417640000 ms (execution: 0.001 s)
17/04/17 13:57:20 INFO MapPartitionsRDD: Removing RDD 185 from persistence list
17/04/17 13:57:20 INFO BlockManager: Removing RDD 186
17/04/17 13:57:20 INFO BlockManager: Removing RDD 185
17/04/17 13:57:20 INFO BlockRDD: Removing RDD 184 from persistence list
17/04/17 13:57:20 INFO BlockManager: Removing RDD 184
17/04/17 13:57:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[184] at createStream at KafkaConsumer.java:64 of time 1492417640000 ms
17/04/17 13:57:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417620000 ms)
17/04/17 13:57:20 INFO InputInfoTracker: remove old batch metadata: 1492417620000 ms
17/04/17 13:57:30 INFO JobScheduler: Starting job streaming job 1492417650000 ms.0 from job set of time 1492417650000 ms
-------------------------------------------
Time: 1492417650000 ms
-------------------------------------------

17/04/17 13:57:30 INFO JobScheduler: Added jobs for time 1492417650000 ms
17/04/17 13:57:30 INFO JobScheduler: Finished job streaming job 1492417650000 ms.0 from job set of time 1492417650000 ms
17/04/17 13:57:30 INFO JobScheduler: Total delay: 0.009 s for time 1492417650000 ms (execution: 0.001 s)
17/04/17 13:57:30 INFO MapPartitionsRDD: Removing RDD 189 from persistence list
17/04/17 13:57:30 INFO BlockManager: Removing RDD 189
17/04/17 13:57:30 INFO MapPartitionsRDD: Removing RDD 188 from persistence list
17/04/17 13:57:30 INFO BlockManager: Removing RDD 188
17/04/17 13:57:30 INFO BlockRDD: Removing RDD 187 from persistence list
17/04/17 13:57:30 INFO BlockManager: Removing RDD 187
17/04/17 13:57:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[187] at createStream at KafkaConsumer.java:64 of time 1492417650000 ms
17/04/17 13:57:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417630000 ms)
17/04/17 13:57:30 INFO InputInfoTracker: remove old batch metadata: 1492417630000 ms
17/04/17 13:57:40 INFO JobScheduler: Added jobs for time 1492417660000 ms
17/04/17 13:57:40 INFO JobScheduler: Starting job streaming job 1492417660000 ms.0 from job set of time 1492417660000 ms
-------------------------------------------
Time: 1492417660000 ms
-------------------------------------------

17/04/17 13:57:40 INFO JobScheduler: Finished job streaming job 1492417660000 ms.0 from job set of time 1492417660000 ms
17/04/17 13:57:40 INFO MapPartitionsRDD: Removing RDD 192 from persistence list
17/04/17 13:57:40 INFO JobScheduler: Total delay: 0.009 s for time 1492417660000 ms (execution: 0.001 s)
17/04/17 13:57:40 INFO BlockManager: Removing RDD 192
17/04/17 13:57:40 INFO MapPartitionsRDD: Removing RDD 191 from persistence list
17/04/17 13:57:40 INFO BlockManager: Removing RDD 191
17/04/17 13:57:40 INFO BlockRDD: Removing RDD 190 from persistence list
17/04/17 13:57:40 INFO BlockManager: Removing RDD 190
17/04/17 13:57:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[190] at createStream at KafkaConsumer.java:64 of time 1492417660000 ms
17/04/17 13:57:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417640000 ms)
17/04/17 13:57:40 INFO InputInfoTracker: remove old batch metadata: 1492417640000 ms
17/04/17 13:57:50 INFO JobScheduler: Added jobs for time 1492417670000 ms
-------------------------------------------
Time: 1492417670000 ms
-------------------------------------------

17/04/17 13:57:50 INFO JobScheduler: Starting job streaming job 1492417670000 ms.0 from job set of time 1492417670000 ms
17/04/17 13:57:50 INFO JobScheduler: Finished job streaming job 1492417670000 ms.0 from job set of time 1492417670000 ms
17/04/17 13:57:50 INFO MapPartitionsRDD: Removing RDD 195 from persistence list
17/04/17 13:57:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417670000 ms (execution: 0.000 s)
17/04/17 13:57:50 INFO BlockManager: Removing RDD 195
17/04/17 13:57:50 INFO MapPartitionsRDD: Removing RDD 194 from persistence list
17/04/17 13:57:50 INFO BlockManager: Removing RDD 194
17/04/17 13:57:50 INFO BlockRDD: Removing RDD 193 from persistence list
17/04/17 13:57:50 INFO BlockManager: Removing RDD 193
17/04/17 13:57:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[193] at createStream at KafkaConsumer.java:64 of time 1492417670000 ms
17/04/17 13:57:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417650000 ms)
17/04/17 13:57:50 INFO InputInfoTracker: remove old batch metadata: 1492417650000 ms
17/04/17 13:58:00 INFO JobScheduler: Added jobs for time 1492417680000 ms
17/04/17 13:58:00 INFO JobScheduler: Starting job streaming job 1492417680000 ms.0 from job set of time 1492417680000 ms
-------------------------------------------
Time: 1492417680000 ms
-------------------------------------------

17/04/17 13:58:00 INFO JobScheduler: Finished job streaming job 1492417680000 ms.0 from job set of time 1492417680000 ms
17/04/17 13:58:00 INFO JobScheduler: Total delay: 0.006 s for time 1492417680000 ms (execution: 0.000 s)
17/04/17 13:58:00 INFO MapPartitionsRDD: Removing RDD 198 from persistence list
17/04/17 13:58:00 INFO BlockManager: Removing RDD 198
17/04/17 13:58:00 INFO MapPartitionsRDD: Removing RDD 197 from persistence list
17/04/17 13:58:00 INFO BlockManager: Removing RDD 197
17/04/17 13:58:00 INFO BlockRDD: Removing RDD 196 from persistence list
17/04/17 13:58:00 INFO BlockManager: Removing RDD 196
17/04/17 13:58:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[196] at createStream at KafkaConsumer.java:64 of time 1492417680000 ms
17/04/17 13:58:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417660000 ms)
17/04/17 13:58:00 INFO InputInfoTracker: remove old batch metadata: 1492417660000 ms
17/04/17 13:58:10 INFO JobScheduler: Added jobs for time 1492417690000 ms
17/04/17 13:58:10 INFO JobScheduler: Starting job streaming job 1492417690000 ms.0 from job set of time 1492417690000 ms
-------------------------------------------
Time: 1492417690000 ms
-------------------------------------------

17/04/17 13:58:10 INFO JobScheduler: Finished job streaming job 1492417690000 ms.0 from job set of time 1492417690000 ms
17/04/17 13:58:10 INFO MapPartitionsRDD: Removing RDD 201 from persistence list
17/04/17 13:58:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417690000 ms (execution: 0.001 s)
17/04/17 13:58:10 INFO BlockManager: Removing RDD 201
17/04/17 13:58:10 INFO MapPartitionsRDD: Removing RDD 200 from persistence list
17/04/17 13:58:10 INFO BlockManager: Removing RDD 200
17/04/17 13:58:10 INFO BlockRDD: Removing RDD 199 from persistence list
17/04/17 13:58:10 INFO BlockManager: Removing RDD 199
17/04/17 13:58:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[199] at createStream at KafkaConsumer.java:64 of time 1492417690000 ms
17/04/17 13:58:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417670000 ms)
17/04/17 13:58:10 INFO InputInfoTracker: remove old batch metadata: 1492417670000 ms
17/04/17 13:58:20 INFO JobScheduler: Added jobs for time 1492417700000 ms
17/04/17 13:58:20 INFO JobScheduler: Starting job streaming job 1492417700000 ms.0 from job set of time 1492417700000 ms
-------------------------------------------
Time: 1492417700000 ms
-------------------------------------------

17/04/17 13:58:20 INFO JobScheduler: Finished job streaming job 1492417700000 ms.0 from job set of time 1492417700000 ms
17/04/17 13:58:20 INFO MapPartitionsRDD: Removing RDD 204 from persistence list
17/04/17 13:58:20 INFO JobScheduler: Total delay: 0.006 s for time 1492417700000 ms (execution: 0.001 s)
17/04/17 13:58:20 INFO BlockManager: Removing RDD 204
17/04/17 13:58:20 INFO MapPartitionsRDD: Removing RDD 203 from persistence list
17/04/17 13:58:20 INFO BlockManager: Removing RDD 203
17/04/17 13:58:20 INFO BlockRDD: Removing RDD 202 from persistence list
17/04/17 13:58:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[202] at createStream at KafkaConsumer.java:64 of time 1492417700000 ms
17/04/17 13:58:20 INFO BlockManager: Removing RDD 202
17/04/17 13:58:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417680000 ms)
17/04/17 13:58:20 INFO InputInfoTracker: remove old batch metadata: 1492417680000 ms
17/04/17 13:58:30 INFO JobScheduler: Added jobs for time 1492417710000 ms
17/04/17 13:58:30 INFO JobScheduler: Starting job streaming job 1492417710000 ms.0 from job set of time 1492417710000 ms
-------------------------------------------
Time: 1492417710000 ms
-------------------------------------------

17/04/17 13:58:30 INFO JobScheduler: Finished job streaming job 1492417710000 ms.0 from job set of time 1492417710000 ms
17/04/17 13:58:30 INFO MapPartitionsRDD: Removing RDD 207 from persistence list
17/04/17 13:58:30 INFO JobScheduler: Total delay: 0.005 s for time 1492417710000 ms (execution: 0.000 s)
17/04/17 13:58:30 INFO BlockManager: Removing RDD 207
17/04/17 13:58:30 INFO MapPartitionsRDD: Removing RDD 206 from persistence list
17/04/17 13:58:30 INFO BlockManager: Removing RDD 206
17/04/17 13:58:30 INFO BlockRDD: Removing RDD 205 from persistence list
17/04/17 13:58:30 INFO BlockManager: Removing RDD 205
17/04/17 13:58:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[205] at createStream at KafkaConsumer.java:64 of time 1492417710000 ms
17/04/17 13:58:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417690000 ms)
17/04/17 13:58:30 INFO InputInfoTracker: remove old batch metadata: 1492417690000 ms
17/04/17 13:58:40 INFO JobScheduler: Added jobs for time 1492417720000 ms
17/04/17 13:58:40 INFO JobScheduler: Starting job streaming job 1492417720000 ms.0 from job set of time 1492417720000 ms
-------------------------------------------
Time: 1492417720000 ms
-------------------------------------------

17/04/17 13:58:40 INFO JobScheduler: Finished job streaming job 1492417720000 ms.0 from job set of time 1492417720000 ms
17/04/17 13:58:40 INFO MapPartitionsRDD: Removing RDD 210 from persistence list
17/04/17 13:58:40 INFO JobScheduler: Total delay: 0.005 s for time 1492417720000 ms (execution: 0.001 s)
17/04/17 13:58:40 INFO BlockManager: Removing RDD 210
17/04/17 13:58:40 INFO MapPartitionsRDD: Removing RDD 209 from persistence list
17/04/17 13:58:40 INFO BlockManager: Removing RDD 209
17/04/17 13:58:40 INFO BlockRDD: Removing RDD 208 from persistence list
17/04/17 13:58:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[208] at createStream at KafkaConsumer.java:64 of time 1492417720000 ms
17/04/17 13:58:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417700000 ms)
17/04/17 13:58:40 INFO InputInfoTracker: remove old batch metadata: 1492417700000 ms
17/04/17 13:58:40 INFO BlockManager: Removing RDD 208
17/04/17 13:58:50 INFO JobScheduler: Added jobs for time 1492417730000 ms
17/04/17 13:58:50 INFO JobScheduler: Starting job streaming job 1492417730000 ms.0 from job set of time 1492417730000 ms
-------------------------------------------
Time: 1492417730000 ms
-------------------------------------------

17/04/17 13:58:50 INFO JobScheduler: Finished job streaming job 1492417730000 ms.0 from job set of time 1492417730000 ms
17/04/17 13:58:50 INFO MapPartitionsRDD: Removing RDD 213 from persistence list
17/04/17 13:58:50 INFO JobScheduler: Total delay: 0.006 s for time 1492417730000 ms (execution: 0.001 s)
17/04/17 13:58:50 INFO BlockManager: Removing RDD 213
17/04/17 13:58:50 INFO MapPartitionsRDD: Removing RDD 212 from persistence list
17/04/17 13:58:50 INFO BlockManager: Removing RDD 212
17/04/17 13:58:50 INFO BlockRDD: Removing RDD 211 from persistence list
17/04/17 13:58:50 INFO BlockManager: Removing RDD 211
17/04/17 13:58:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[211] at createStream at KafkaConsumer.java:64 of time 1492417730000 ms
17/04/17 13:58:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417710000 ms)
17/04/17 13:58:50 INFO InputInfoTracker: remove old batch metadata: 1492417710000 ms
17/04/17 13:59:00 INFO JobScheduler: Added jobs for time 1492417740000 ms
17/04/17 13:59:00 INFO JobScheduler: Starting job streaming job 1492417740000 ms.0 from job set of time 1492417740000 ms
-------------------------------------------
Time: 1492417740000 ms
-------------------------------------------

17/04/17 13:59:00 INFO JobScheduler: Finished job streaming job 1492417740000 ms.0 from job set of time 1492417740000 ms
17/04/17 13:59:00 INFO MapPartitionsRDD: Removing RDD 216 from persistence list
17/04/17 13:59:00 INFO JobScheduler: Total delay: 0.006 s for time 1492417740000 ms (execution: 0.000 s)
17/04/17 13:59:00 INFO BlockManager: Removing RDD 216
17/04/17 13:59:00 INFO MapPartitionsRDD: Removing RDD 215 from persistence list
17/04/17 13:59:00 INFO BlockManager: Removing RDD 215
17/04/17 13:59:00 INFO BlockRDD: Removing RDD 214 from persistence list
17/04/17 13:59:00 INFO BlockManager: Removing RDD 214
17/04/17 13:59:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[214] at createStream at KafkaConsumer.java:64 of time 1492417740000 ms
17/04/17 13:59:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417720000 ms)
17/04/17 13:59:00 INFO InputInfoTracker: remove old batch metadata: 1492417720000 ms
17/04/17 13:59:10 INFO JobScheduler: Added jobs for time 1492417750000 ms
17/04/17 13:59:10 INFO JobScheduler: Starting job streaming job 1492417750000 ms.0 from job set of time 1492417750000 ms
-------------------------------------------
Time: 1492417750000 ms
-------------------------------------------

17/04/17 13:59:10 INFO JobScheduler: Finished job streaming job 1492417750000 ms.0 from job set of time 1492417750000 ms
17/04/17 13:59:10 INFO MapPartitionsRDD: Removing RDD 219 from persistence list
17/04/17 13:59:10 INFO JobScheduler: Total delay: 0.004 s for time 1492417750000 ms (execution: 0.000 s)
17/04/17 13:59:10 INFO BlockManager: Removing RDD 219
17/04/17 13:59:10 INFO MapPartitionsRDD: Removing RDD 218 from persistence list
17/04/17 13:59:10 INFO BlockManager: Removing RDD 218
17/04/17 13:59:10 INFO BlockRDD: Removing RDD 217 from persistence list
17/04/17 13:59:10 INFO BlockManager: Removing RDD 217
17/04/17 13:59:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[217] at createStream at KafkaConsumer.java:64 of time 1492417750000 ms
17/04/17 13:59:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417730000 ms)
17/04/17 13:59:10 INFO InputInfoTracker: remove old batch metadata: 1492417730000 ms
17/04/17 13:59:20 INFO JobScheduler: Added jobs for time 1492417760000 ms
17/04/17 13:59:20 INFO JobScheduler: Starting job streaming job 1492417760000 ms.0 from job set of time 1492417760000 ms
-------------------------------------------
Time: 1492417760000 ms
-------------------------------------------

17/04/17 13:59:20 INFO JobScheduler: Finished job streaming job 1492417760000 ms.0 from job set of time 1492417760000 ms
17/04/17 13:59:20 INFO MapPartitionsRDD: Removing RDD 222 from persistence list
17/04/17 13:59:20 INFO JobScheduler: Total delay: 0.004 s for time 1492417760000 ms (execution: 0.001 s)
17/04/17 13:59:20 INFO BlockManager: Removing RDD 222
17/04/17 13:59:20 INFO MapPartitionsRDD: Removing RDD 221 from persistence list
17/04/17 13:59:20 INFO BlockManager: Removing RDD 221
17/04/17 13:59:20 INFO BlockRDD: Removing RDD 220 from persistence list
17/04/17 13:59:20 INFO BlockManager: Removing RDD 220
17/04/17 13:59:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[220] at createStream at KafkaConsumer.java:64 of time 1492417760000 ms
17/04/17 13:59:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417740000 ms)
17/04/17 13:59:20 INFO InputInfoTracker: remove old batch metadata: 1492417740000 ms
17/04/17 13:59:30 INFO JobScheduler: Added jobs for time 1492417770000 ms
-------------------------------------------
Time: 1492417770000 ms
-------------------------------------------

17/04/17 13:59:30 INFO JobScheduler: Starting job streaming job 1492417770000 ms.0 from job set of time 1492417770000 ms
17/04/17 13:59:30 INFO JobScheduler: Finished job streaming job 1492417770000 ms.0 from job set of time 1492417770000 ms
17/04/17 13:59:30 INFO MapPartitionsRDD: Removing RDD 225 from persistence list
17/04/17 13:59:30 INFO JobScheduler: Total delay: 0.005 s for time 1492417770000 ms (execution: 0.000 s)
17/04/17 13:59:30 INFO BlockManager: Removing RDD 225
17/04/17 13:59:30 INFO MapPartitionsRDD: Removing RDD 224 from persistence list
17/04/17 13:59:30 INFO BlockManager: Removing RDD 224
17/04/17 13:59:30 INFO BlockRDD: Removing RDD 223 from persistence list
17/04/17 13:59:30 INFO BlockManager: Removing RDD 223
17/04/17 13:59:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[223] at createStream at KafkaConsumer.java:64 of time 1492417770000 ms
17/04/17 13:59:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417750000 ms)
17/04/17 13:59:30 INFO InputInfoTracker: remove old batch metadata: 1492417750000 ms
17/04/17 13:59:40 INFO JobScheduler: Added jobs for time 1492417780000 ms
17/04/17 13:59:40 INFO JobScheduler: Starting job streaming job 1492417780000 ms.0 from job set of time 1492417780000 ms
-------------------------------------------
Time: 1492417780000 ms
-------------------------------------------

17/04/17 13:59:40 INFO JobScheduler: Finished job streaming job 1492417780000 ms.0 from job set of time 1492417780000 ms
17/04/17 13:59:40 INFO MapPartitionsRDD: Removing RDD 228 from persistence list
17/04/17 13:59:40 INFO JobScheduler: Total delay: 0.006 s for time 1492417780000 ms (execution: 0.001 s)
17/04/17 13:59:40 INFO BlockManager: Removing RDD 228
17/04/17 13:59:40 INFO MapPartitionsRDD: Removing RDD 227 from persistence list
17/04/17 13:59:40 INFO BlockManager: Removing RDD 227
17/04/17 13:59:40 INFO BlockRDD: Removing RDD 226 from persistence list
17/04/17 13:59:40 INFO BlockManager: Removing RDD 226
17/04/17 13:59:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[226] at createStream at KafkaConsumer.java:64 of time 1492417780000 ms
17/04/17 13:59:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417760000 ms)
17/04/17 13:59:40 INFO InputInfoTracker: remove old batch metadata: 1492417760000 ms
17/04/17 13:59:50 INFO JobScheduler: Starting job streaming job 1492417790000 ms.0 from job set of time 1492417790000 ms
-------------------------------------------
Time: 1492417790000 ms
-------------------------------------------

17/04/17 13:59:50 INFO JobScheduler: Added jobs for time 1492417790000 ms
17/04/17 13:59:50 INFO JobScheduler: Finished job streaming job 1492417790000 ms.0 from job set of time 1492417790000 ms
17/04/17 13:59:50 INFO MapPartitionsRDD: Removing RDD 231 from persistence list
17/04/17 13:59:50 INFO JobScheduler: Total delay: 0.008 s for time 1492417790000 ms (execution: 0.001 s)
17/04/17 13:59:50 INFO BlockManager: Removing RDD 231
17/04/17 13:59:50 INFO MapPartitionsRDD: Removing RDD 230 from persistence list
17/04/17 13:59:50 INFO BlockManager: Removing RDD 230
17/04/17 13:59:50 INFO BlockRDD: Removing RDD 229 from persistence list
17/04/17 13:59:50 INFO BlockManager: Removing RDD 229
17/04/17 13:59:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[229] at createStream at KafkaConsumer.java:64 of time 1492417790000 ms
17/04/17 13:59:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417770000 ms)
17/04/17 13:59:50 INFO InputInfoTracker: remove old batch metadata: 1492417770000 ms
17/04/17 14:00:00 INFO JobScheduler: Added jobs for time 1492417800000 ms
-------------------------------------------
Time: 1492417800000 ms
-------------------------------------------

17/04/17 14:00:00 INFO JobScheduler: Starting job streaming job 1492417800000 ms.0 from job set of time 1492417800000 ms
17/04/17 14:00:00 INFO JobScheduler: Finished job streaming job 1492417800000 ms.0 from job set of time 1492417800000 ms
17/04/17 14:00:00 INFO MapPartitionsRDD: Removing RDD 234 from persistence list
17/04/17 14:00:00 INFO JobScheduler: Total delay: 0.007 s for time 1492417800000 ms (execution: 0.000 s)
17/04/17 14:00:00 INFO BlockManager: Removing RDD 234
17/04/17 14:00:00 INFO MapPartitionsRDD: Removing RDD 233 from persistence list
17/04/17 14:00:00 INFO BlockManager: Removing RDD 233
17/04/17 14:00:00 INFO BlockRDD: Removing RDD 232 from persistence list
17/04/17 14:00:00 INFO BlockManager: Removing RDD 232
17/04/17 14:00:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[232] at createStream at KafkaConsumer.java:64 of time 1492417800000 ms
17/04/17 14:00:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417780000 ms)
17/04/17 14:00:00 INFO InputInfoTracker: remove old batch metadata: 1492417780000 ms
17/04/17 14:00:10 INFO JobScheduler: Starting job streaming job 1492417810000 ms.0 from job set of time 1492417810000 ms
-------------------------------------------
Time: 1492417810000 ms
-------------------------------------------

17/04/17 14:00:10 INFO JobScheduler: Added jobs for time 1492417810000 ms
17/04/17 14:00:10 INFO JobScheduler: Finished job streaming job 1492417810000 ms.0 from job set of time 1492417810000 ms
17/04/17 14:00:10 INFO MapPartitionsRDD: Removing RDD 237 from persistence list
17/04/17 14:00:10 INFO JobScheduler: Total delay: 0.008 s for time 1492417810000 ms (execution: 0.001 s)
17/04/17 14:00:10 INFO BlockManager: Removing RDD 237
17/04/17 14:00:10 INFO MapPartitionsRDD: Removing RDD 236 from persistence list
17/04/17 14:00:10 INFO BlockManager: Removing RDD 236
17/04/17 14:00:10 INFO BlockRDD: Removing RDD 235 from persistence list
17/04/17 14:00:10 INFO BlockManager: Removing RDD 235
17/04/17 14:00:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[235] at createStream at KafkaConsumer.java:64 of time 1492417810000 ms
17/04/17 14:00:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417790000 ms)
17/04/17 14:00:10 INFO InputInfoTracker: remove old batch metadata: 1492417790000 ms
17/04/17 14:00:20 INFO JobScheduler: Added jobs for time 1492417820000 ms
17/04/17 14:00:20 INFO JobScheduler: Starting job streaming job 1492417820000 ms.0 from job set of time 1492417820000 ms
-------------------------------------------
Time: 1492417820000 ms
-------------------------------------------

17/04/17 14:00:20 INFO JobScheduler: Finished job streaming job 1492417820000 ms.0 from job set of time 1492417820000 ms
17/04/17 14:00:20 INFO MapPartitionsRDD: Removing RDD 240 from persistence list
17/04/17 14:00:20 INFO JobScheduler: Total delay: 0.006 s for time 1492417820000 ms (execution: 0.000 s)
17/04/17 14:00:20 INFO BlockManager: Removing RDD 240
17/04/17 14:00:20 INFO MapPartitionsRDD: Removing RDD 239 from persistence list
17/04/17 14:00:20 WARN BlockManagerMaster: Failed to remove RDD 204 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:20 WARN BlockManagerMaster: Failed to remove RDD 203 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:20 INFO BlockManager: Removing RDD 239
17/04/17 14:00:20 INFO BlockRDD: Removing RDD 238 from persistence list
17/04/17 14:00:20 WARN BlockManagerMaster: Failed to remove RDD 202 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:20 INFO BlockManager: Removing RDD 238
17/04/17 14:00:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[238] at createStream at KafkaConsumer.java:64 of time 1492417820000 ms
17/04/17 14:00:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417800000 ms)
17/04/17 14:00:20 INFO InputInfoTracker: remove old batch metadata: 1492417800000 ms
17/04/17 14:00:30 INFO JobScheduler: Added jobs for time 1492417830000 ms
-------------------------------------------
Time: 1492417830000 ms
-------------------------------------------

17/04/17 14:00:30 INFO JobScheduler: Starting job streaming job 1492417830000 ms.0 from job set of time 1492417830000 ms
17/04/17 14:00:30 WARN BlockManagerMaster: Failed to remove RDD 207 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:30 WARN BlockManagerMaster: Failed to remove RDD 206 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:30 INFO JobScheduler: Finished job streaming job 1492417830000 ms.0 from job set of time 1492417830000 ms
17/04/17 14:00:30 INFO MapPartitionsRDD: Removing RDD 243 from persistence list
17/04/17 14:00:30 INFO JobScheduler: Total delay: 0.010 s for time 1492417830000 ms (execution: 0.003 s)
17/04/17 14:00:30 WARN BlockManagerMaster: Failed to remove RDD 205 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:30 INFO MapPartitionsRDD: Removing RDD 242 from persistence list
17/04/17 14:00:30 INFO BlockManager: Removing RDD 243
17/04/17 14:00:30 INFO BlockManager: Removing RDD 242
17/04/17 14:00:30 INFO BlockRDD: Removing RDD 241 from persistence list
17/04/17 14:00:30 INFO BlockManager: Removing RDD 241
17/04/17 14:00:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[241] at createStream at KafkaConsumer.java:64 of time 1492417830000 ms
17/04/17 14:00:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417810000 ms)
17/04/17 14:00:30 INFO InputInfoTracker: remove old batch metadata: 1492417810000 ms
17/04/17 14:00:31 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 134299 ms exceeds timeout 120000 ms
17/04/17 14:00:31 ERROR TaskSchedulerImpl: Lost executor 2 on carl-PC: Executor heartbeat timed out after 134299 ms
17/04/17 14:00:31 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 70, carl-PC): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 134299 ms
17/04/17 14:00:31 INFO TaskSetManager: Starting task 0.1 in stage 2.0 (TID 71, carl-PC, partition 0,PROCESS_LOCAL, 3080 bytes)
17/04/17 14:00:31 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 139028 ms exceeds timeout 120000 ms
17/04/17 14:00:31 ERROR TaskSchedulerImpl: Lost executor 1 on carl-PC: Executor heartbeat timed out after 139028 ms
17/04/17 14:00:31 WARN TaskSetManager: Lost task 0.1 in stage 2.0 (TID 71, carl-PC): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 139028 ms
17/04/17 14:00:31 INFO SparkDeploySchedulerBackend: Requesting to kill executor(s) 2
17/04/17 14:00:31 INFO DAGScheduler: Executor lost: 2 (epoch 1)
17/04/17 14:00:31 INFO TaskSetManager: Starting task 0.2 in stage 2.0 (TID 72, am372811-PC, partition 0,PROCESS_LOCAL, 3080 bytes)
17/04/17 14:00:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
17/04/17 14:00:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, carl-PC, 40477)
17/04/17 14:00:31 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
17/04/17 14:00:31 INFO DAGScheduler: Executor lost: 1 (epoch 2)
17/04/17 14:00:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/04/17 14:00:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, carl-PC, 42362)
17/04/17 14:00:31 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
17/04/17 14:00:31 INFO DAGScheduler: Host added was in lost list earlier: carl-PC
17/04/17 14:00:40 WARN BlockManagerMaster: Failed to remove RDD 210 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
-------------------------------------------
Time: 1492417840000 ms
-------------------------------------------

17/04/17 14:00:40 WARN BlockManagerMaster: Failed to remove RDD 209 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:40 WARN BlockManagerMaster: Failed to remove RDD 208 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:40 INFO JobScheduler: Added jobs for time 1492417840000 ms
17/04/17 14:00:40 INFO JobScheduler: Starting job streaming job 1492417840000 ms.0 from job set of time 1492417840000 ms
17/04/17 14:00:40 INFO JobScheduler: Finished job streaming job 1492417840000 ms.0 from job set of time 1492417840000 ms
17/04/17 14:00:40 INFO MapPartitionsRDD: Removing RDD 246 from persistence list
17/04/17 14:00:40 INFO JobScheduler: Total delay: 0.013 s for time 1492417840000 ms (execution: 0.003 s)
17/04/17 14:00:40 INFO BlockManager: Removing RDD 246
17/04/17 14:00:40 INFO MapPartitionsRDD: Removing RDD 245 from persistence list
17/04/17 14:00:40 INFO BlockManager: Removing RDD 245
17/04/17 14:00:40 INFO BlockRDD: Removing RDD 244 from persistence list
17/04/17 14:00:40 INFO BlockManager: Removing RDD 244
17/04/17 14:00:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[244] at createStream at KafkaConsumer.java:64 of time 1492417840000 ms
17/04/17 14:00:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417820000 ms)
17/04/17 14:00:40 INFO InputInfoTracker: remove old batch metadata: 1492417820000 ms
17/04/17 14:00:50 INFO JobScheduler: Added jobs for time 1492417850000 ms
-------------------------------------------
Time: 1492417850000 ms
-------------------------------------------

17/04/17 14:00:50 INFO JobScheduler: Starting job streaming job 1492417850000 ms.0 from job set of time 1492417850000 ms
17/04/17 14:00:50 INFO JobScheduler: Finished job streaming job 1492417850000 ms.0 from job set of time 1492417850000 ms
17/04/17 14:00:50 WARN BlockManagerMaster: Failed to remove RDD 213 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:50 WARN BlockManagerMaster: Failed to remove RDD 212 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:50 WARN BlockManagerMaster: Failed to remove RDD 211 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:00:50 INFO MapPartitionsRDD: Removing RDD 249 from persistence list
17/04/17 14:00:50 INFO JobScheduler: Total delay: 0.008 s for time 1492417850000 ms (execution: 0.000 s)
17/04/17 14:00:50 INFO MapPartitionsRDD: Removing RDD 248 from persistence list
17/04/17 14:00:50 INFO BlockManager: Removing RDD 248
17/04/17 14:00:50 INFO BlockRDD: Removing RDD 247 from persistence list
17/04/17 14:00:50 INFO BlockManager: Removing RDD 247
17/04/17 14:00:50 INFO BlockManager: Removing RDD 249
17/04/17 14:00:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[247] at createStream at KafkaConsumer.java:64 of time 1492417850000 ms
17/04/17 14:00:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417830000 ms)
17/04/17 14:00:50 INFO InputInfoTracker: remove old batch metadata: 1492417830000 ms
17/04/17 14:01:00 INFO JobScheduler: Added jobs for time 1492417860000 ms
17/04/17 14:01:00 INFO JobScheduler: Starting job streaming job 1492417860000 ms.0 from job set of time 1492417860000 ms
-------------------------------------------
Time: 1492417860000 ms
-------------------------------------------

17/04/17 14:01:00 INFO JobScheduler: Finished job streaming job 1492417860000 ms.0 from job set of time 1492417860000 ms
17/04/17 14:01:00 INFO MapPartitionsRDD: Removing RDD 252 from persistence list
17/04/17 14:01:00 INFO JobScheduler: Total delay: 0.006 s for time 1492417860000 ms (execution: 0.001 s)
17/04/17 14:01:00 INFO BlockManager: Removing RDD 252
17/04/17 14:01:00 INFO MapPartitionsRDD: Removing RDD 251 from persistence list
17/04/17 14:01:00 INFO BlockManager: Removing RDD 251
17/04/17 14:01:00 INFO BlockRDD: Removing RDD 250 from persistence list
17/04/17 14:01:00 WARN BlockManagerMaster: Failed to remove RDD 215 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:00 INFO BlockManager: Removing RDD 250
17/04/17 14:01:00 WARN BlockManagerMaster: Failed to remove RDD 216 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[250] at createStream at KafkaConsumer.java:64 of time 1492417860000 ms
17/04/17 14:01:00 WARN BlockManagerMaster: Failed to remove RDD 214 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417840000 ms)
17/04/17 14:01:00 INFO InputInfoTracker: remove old batch metadata: 1492417840000 ms
-------------------------------------------
17/04/17 14:01:10 WARN BlockManagerMaster: Failed to remove RDD 219 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
Time: 1492417870000 ms
-------------------------------------------
17/04/17 14:01:10 INFO JobScheduler: Added jobs for time 1492417870000 ms

17/04/17 14:01:10 INFO JobScheduler: Starting job streaming job 1492417870000 ms.0 from job set of time 1492417870000 ms
17/04/17 14:01:10 INFO JobScheduler: Finished job streaming job 1492417870000 ms.0 from job set of time 1492417870000 ms
17/04/17 14:01:10 WARN BlockManagerMaster: Failed to remove RDD 218 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:10 INFO JobScheduler: Total delay: 0.007 s for time 1492417870000 ms (execution: 0.002 s)
17/04/17 14:01:10 INFO MapPartitionsRDD: Removing RDD 255 from persistence list
17/04/17 14:01:10 WARN BlockManagerMaster: Failed to remove RDD 217 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:10 INFO BlockManager: Removing RDD 255
17/04/17 14:01:10 INFO MapPartitionsRDD: Removing RDD 254 from persistence list
17/04/17 14:01:10 INFO BlockManager: Removing RDD 254
17/04/17 14:01:10 INFO BlockRDD: Removing RDD 253 from persistence list
17/04/17 14:01:10 INFO BlockManager: Removing RDD 253
17/04/17 14:01:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[253] at createStream at KafkaConsumer.java:64 of time 1492417870000 ms
17/04/17 14:01:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417850000 ms)
17/04/17 14:01:10 INFO InputInfoTracker: remove old batch metadata: 1492417850000 ms
17/04/17 14:01:20 INFO JobScheduler: Added jobs for time 1492417880000 ms
-------------------------------------------
17/04/17 14:01:20 INFO JobScheduler: Starting job streaming job 1492417880000 ms.0 from job set of time 1492417880000 ms
Time: 1492417880000 ms
-------------------------------------------

17/04/17 14:01:20 WARN BlockManagerMaster: Failed to remove RDD 221 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:20 INFO JobScheduler: Finished job streaming job 1492417880000 ms.0 from job set of time 1492417880000 ms
17/04/17 14:01:20 WARN BlockManagerMaster: Failed to remove RDD 222 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:20 WARN BlockManagerMaster: Failed to remove RDD 220 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:20 INFO MapPartitionsRDD: Removing RDD 258 from persistence list
17/04/17 14:01:20 INFO JobScheduler: Total delay: 0.008 s for time 1492417880000 ms (execution: 0.002 s)
17/04/17 14:01:20 INFO BlockManager: Removing RDD 258
17/04/17 14:01:20 INFO MapPartitionsRDD: Removing RDD 257 from persistence list
17/04/17 14:01:20 INFO BlockManager: Removing RDD 257
17/04/17 14:01:20 INFO BlockRDD: Removing RDD 256 from persistence list
17/04/17 14:01:20 INFO BlockManager: Removing RDD 256
17/04/17 14:01:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[256] at createStream at KafkaConsumer.java:64 of time 1492417880000 ms
17/04/17 14:01:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417860000 ms)
17/04/17 14:01:20 INFO InputInfoTracker: remove old batch metadata: 1492417860000 ms
17/04/17 14:01:30 INFO JobScheduler: Added jobs for time 1492417890000 ms
17/04/17 14:01:30 WARN BlockManagerMaster: Failed to remove RDD 225 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:30 INFO JobScheduler: Starting job streaming job 1492417890000 ms.0 from job set of time 1492417890000 ms
-------------------------------------------
Time: 1492417890000 ms
-------------------------------------------

17/04/17 14:01:30 WARN BlockManagerMaster: Failed to remove RDD 223 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:30 INFO JobScheduler: Finished job streaming job 1492417890000 ms.0 from job set of time 1492417890000 ms
17/04/17 14:01:30 WARN BlockManagerMaster: Failed to remove RDD 224 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:30 INFO JobScheduler: Total delay: 0.011 s for time 1492417890000 ms (execution: 0.002 s)
17/04/17 14:01:30 INFO MapPartitionsRDD: Removing RDD 261 from persistence list
17/04/17 14:01:30 INFO BlockManager: Removing RDD 261
17/04/17 14:01:30 INFO MapPartitionsRDD: Removing RDD 260 from persistence list
17/04/17 14:01:30 INFO BlockManager: Removing RDD 260
17/04/17 14:01:30 INFO BlockRDD: Removing RDD 259 from persistence list
17/04/17 14:01:30 INFO BlockManager: Removing RDD 259
17/04/17 14:01:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[259] at createStream at KafkaConsumer.java:64 of time 1492417890000 ms
17/04/17 14:01:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417870000 ms)
17/04/17 14:01:30 INFO InputInfoTracker: remove old batch metadata: 1492417870000 ms
17/04/17 14:01:31 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 173172 ms exceeds timeout 120000 ms
17/04/17 14:01:31 ERROR TaskSchedulerImpl: Lost executor 0 on am372811-PC: Executor heartbeat timed out after 173172 ms
17/04/17 14:01:31 WARN HeartbeatReceiver: Removing executor 3 with no recent heartbeats: 172766 ms exceeds timeout 120000 ms
17/04/17 14:01:31 INFO DAGScheduler: Executor lost: 0 (epoch 3)
17/04/17 14:01:31 ERROR TaskSchedulerImpl: Lost executor 3 on am372811-PC: Executor heartbeat timed out after 172766 ms
17/04/17 14:01:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/04/17 14:01:31 WARN TaskSetManager: Lost task 0.2 in stage 2.0 (TID 72, am372811-PC): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 172766 ms
17/04/17 14:01:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, am372811-PC, 35315)
17/04/17 14:01:31 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
17/04/17 14:01:31 INFO DAGScheduler: Executor lost: 3 (epoch 4)
17/04/17 14:01:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
17/04/17 14:01:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, am372811-PC, 45318)
17/04/17 14:01:31 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
17/04/17 14:01:31 INFO DAGScheduler: Host added was in lost list earlier: am372811-PC
17/04/17 14:01:31 INFO TaskSetManager: Starting task 0.3 in stage 2.0 (TID 73, carl-PC, partition 0,PROCESS_LOCAL, 3080 bytes)
17/04/17 14:01:40 INFO JobScheduler: Added jobs for time 1492417900000 ms
17/04/17 14:01:40 INFO JobScheduler: Starting job streaming job 1492417900000 ms.0 from job set of time 1492417900000 ms
-------------------------------------------
Time: 1492417900000 ms
-------------------------------------------

17/04/17 14:01:40 INFO JobScheduler: Finished job streaming job 1492417900000 ms.0 from job set of time 1492417900000 ms
17/04/17 14:01:40 INFO MapPartitionsRDD: Removing RDD 264 from persistence list
17/04/17 14:01:40 INFO JobScheduler: Total delay: 0.006 s for time 1492417900000 ms (execution: 0.001 s)
17/04/17 14:01:40 INFO BlockManager: Removing RDD 264
17/04/17 14:01:40 INFO MapPartitionsRDD: Removing RDD 263 from persistence list
17/04/17 14:01:40 INFO BlockManager: Removing RDD 263
17/04/17 14:01:40 INFO BlockRDD: Removing RDD 262 from persistence list
17/04/17 14:01:40 WARN BlockManagerMaster: Failed to remove RDD 227 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:40 INFO BlockManager: Removing RDD 262
17/04/17 14:01:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[262] at createStream at KafkaConsumer.java:64 of time 1492417900000 ms
17/04/17 14:01:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417880000 ms)
17/04/17 14:01:40 WARN BlockManagerMaster: Failed to remove RDD 228 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:40 INFO InputInfoTracker: remove old batch metadata: 1492417880000 ms
17/04/17 14:01:40 WARN BlockManagerMaster: Failed to remove RDD 226 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:50 INFO JobScheduler: Starting job streaming job 1492417910000 ms.0 from job set of time 1492417910000 ms
-------------------------------------------
Time: 1492417910000 ms
-------------------------------------------

17/04/17 14:01:50 INFO JobScheduler: Added jobs for time 1492417910000 ms
17/04/17 14:01:50 INFO JobScheduler: Finished job streaming job 1492417910000 ms.0 from job set of time 1492417910000 ms
17/04/17 14:01:50 INFO MapPartitionsRDD: Removing RDD 267 from persistence list
17/04/17 14:01:50 INFO JobScheduler: Total delay: 0.007 s for time 1492417910000 ms (execution: 0.001 s)
17/04/17 14:01:50 INFO BlockManager: Removing RDD 267
17/04/17 14:01:50 INFO MapPartitionsRDD: Removing RDD 266 from persistence list
17/04/17 14:01:50 INFO BlockManager: Removing RDD 266
17/04/17 14:01:50 INFO BlockRDD: Removing RDD 265 from persistence list
17/04/17 14:01:50 INFO BlockManager: Removing RDD 265
17/04/17 14:01:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[265] at createStream at KafkaConsumer.java:64 of time 1492417910000 ms
17/04/17 14:01:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417890000 ms)
17/04/17 14:01:50 INFO InputInfoTracker: remove old batch metadata: 1492417890000 ms
17/04/17 14:01:50 WARN BlockManagerMaster: Failed to remove RDD 231 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:50 WARN BlockManagerMaster: Failed to remove RDD 230 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:01:50 WARN BlockManagerMaster: Failed to remove RDD 229 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:00 INFO JobScheduler: Added jobs for time 1492417920000 ms
17/04/17 14:02:00 INFO JobScheduler: Starting job streaming job 1492417920000 ms.0 from job set of time 1492417920000 ms
-------------------------------------------
Time: 1492417920000 ms
-------------------------------------------

17/04/17 14:02:00 INFO JobScheduler: Finished job streaming job 1492417920000 ms.0 from job set of time 1492417920000 ms
17/04/17 14:02:00 INFO MapPartitionsRDD: Removing RDD 270 from persistence list
17/04/17 14:02:00 INFO JobScheduler: Total delay: 0.005 s for time 1492417920000 ms (execution: 0.000 s)
17/04/17 14:02:00 INFO BlockManager: Removing RDD 270
17/04/17 14:02:00 INFO MapPartitionsRDD: Removing RDD 269 from persistence list
17/04/17 14:02:00 INFO BlockManager: Removing RDD 269
17/04/17 14:02:00 INFO BlockRDD: Removing RDD 268 from persistence list
17/04/17 14:02:00 WARN BlockManagerMaster: Failed to remove RDD 233 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:00 WARN BlockManagerMaster: Failed to remove RDD 234 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:00 INFO BlockManager: Removing RDD 268
17/04/17 14:02:00 WARN BlockManagerMaster: Failed to remove RDD 232 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[268] at createStream at KafkaConsumer.java:64 of time 1492417920000 ms
17/04/17 14:02:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417900000 ms)
17/04/17 14:02:00 INFO InputInfoTracker: remove old batch metadata: 1492417900000 ms
17/04/17 14:02:10 INFO JobScheduler: Added jobs for time 1492417930000 ms
17/04/17 14:02:10 INFO JobScheduler: Starting job streaming job 1492417930000 ms.0 from job set of time 1492417930000 ms
-------------------------------------------
Time: 1492417930000 ms
-------------------------------------------

17/04/17 14:02:10 INFO JobScheduler: Finished job streaming job 1492417930000 ms.0 from job set of time 1492417930000 ms
17/04/17 14:02:10 INFO MapPartitionsRDD: Removing RDD 273 from persistence list
17/04/17 14:02:10 INFO JobScheduler: Total delay: 0.007 s for time 1492417930000 ms (execution: 0.001 s)
17/04/17 14:02:10 INFO BlockManager: Removing RDD 273
17/04/17 14:02:10 INFO MapPartitionsRDD: Removing RDD 272 from persistence list
17/04/17 14:02:10 INFO BlockManager: Removing RDD 272
17/04/17 14:02:10 INFO BlockRDD: Removing RDD 271 from persistence list
17/04/17 14:02:10 INFO BlockManager: Removing RDD 271
17/04/17 14:02:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[271] at createStream at KafkaConsumer.java:64 of time 1492417930000 ms
17/04/17 14:02:10 WARN BlockManagerMaster: Failed to remove RDD 237 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417910000 ms)
17/04/17 14:02:10 INFO InputInfoTracker: remove old batch metadata: 1492417910000 ms
17/04/17 14:02:10 WARN BlockManagerMaster: Failed to remove RDD 236 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:10 WARN BlockManagerMaster: Failed to remove RDD 235 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:20 INFO JobScheduler: Added jobs for time 1492417940000 ms
17/04/17 14:02:20 INFO JobScheduler: Starting job streaming job 1492417940000 ms.0 from job set of time 1492417940000 ms
-------------------------------------------
Time: 1492417940000 ms
-------------------------------------------

17/04/17 14:02:20 WARN BlockManagerMaster: Failed to remove RDD 240 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:20 INFO JobScheduler: Finished job streaming job 1492417940000 ms.0 from job set of time 1492417940000 ms
17/04/17 14:02:20 INFO MapPartitionsRDD: Removing RDD 276 from persistence list
17/04/17 14:02:20 INFO JobScheduler: Total delay: 0.008 s for time 1492417940000 ms (execution: 0.001 s)
17/04/17 14:02:20 INFO MapPartitionsRDD: Removing RDD 275 from persistence list
17/04/17 14:02:20 INFO BlockManager: Removing RDD 276
17/04/17 14:02:20 INFO BlockManager: Removing RDD 275
17/04/17 14:02:20 WARN BlockManagerMaster: Failed to remove RDD 239 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:20 INFO BlockRDD: Removing RDD 274 from persistence list
17/04/17 14:02:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[274] at createStream at KafkaConsumer.java:64 of time 1492417940000 ms
17/04/17 14:02:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417920000 ms)
17/04/17 14:02:20 INFO InputInfoTracker: remove old batch metadata: 1492417920000 ms
17/04/17 14:02:20 INFO BlockManager: Removing RDD 274
17/04/17 14:02:20 WARN BlockManagerMaster: Failed to remove RDD 238 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:30 INFO JobScheduler: Added jobs for time 1492417950000 ms
17/04/17 14:02:30 INFO JobScheduler: Starting job streaming job 1492417950000 ms.0 from job set of time 1492417950000 ms
-------------------------------------------
Time: 1492417950000 ms
-------------------------------------------

17/04/17 14:02:30 INFO JobScheduler: Finished job streaming job 1492417950000 ms.0 from job set of time 1492417950000 ms
17/04/17 14:02:30 INFO JobScheduler: Total delay: 0.010 s for time 1492417950000 ms (execution: 0.003 s)
17/04/17 14:02:30 INFO MapPartitionsRDD: Removing RDD 279 from persistence list
17/04/17 14:02:30 INFO BlockManager: Removing RDD 279
17/04/17 14:02:30 INFO MapPartitionsRDD: Removing RDD 278 from persistence list
17/04/17 14:02:30 INFO BlockManager: Removing RDD 278
17/04/17 14:02:30 INFO BlockRDD: Removing RDD 277 from persistence list
17/04/17 14:02:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[277] at createStream at KafkaConsumer.java:64 of time 1492417950000 ms
17/04/17 14:02:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417930000 ms)
17/04/17 14:02:30 INFO InputInfoTracker: remove old batch metadata: 1492417930000 ms
17/04/17 14:02:30 INFO BlockManager: Removing RDD 277
17/04/17 14:02:30 WARN BlockManagerMaster: Failed to remove RDD 243 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:30 WARN BlockManagerMaster: Failed to remove RDD 242 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:30 WARN BlockManagerMaster: Failed to remove RDD 241 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:31 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
17/04/17 14:02:31 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 6 more
17/04/17 14:02:40 INFO JobScheduler: Added jobs for time 1492417960000 ms
17/04/17 14:02:40 INFO JobScheduler: Starting job streaming job 1492417960000 ms.0 from job set of time 1492417960000 ms
-------------------------------------------
Time: 1492417960000 ms
-------------------------------------------

17/04/17 14:02:40 INFO JobScheduler: Finished job streaming job 1492417960000 ms.0 from job set of time 1492417960000 ms
17/04/17 14:02:40 INFO MapPartitionsRDD: Removing RDD 282 from persistence list
17/04/17 14:02:40 INFO JobScheduler: Total delay: 0.005 s for time 1492417960000 ms (execution: 0.000 s)
17/04/17 14:02:40 INFO BlockManager: Removing RDD 282
17/04/17 14:02:40 INFO MapPartitionsRDD: Removing RDD 281 from persistence list
17/04/17 14:02:40 INFO BlockManager: Removing RDD 281
17/04/17 14:02:40 INFO BlockRDD: Removing RDD 280 from persistence list
17/04/17 14:02:40 INFO BlockManager: Removing RDD 280
17/04/17 14:02:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[280] at createStream at KafkaConsumer.java:64 of time 1492417960000 ms
17/04/17 14:02:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417940000 ms)
17/04/17 14:02:40 INFO InputInfoTracker: remove old batch metadata: 1492417940000 ms
17/04/17 14:02:40 WARN BlockManagerMaster: Failed to remove RDD 246 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:40 WARN BlockManagerMaster: Failed to remove RDD 245 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:40 WARN BlockManagerMaster: Failed to remove RDD 244 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:50 INFO JobScheduler: Added jobs for time 1492417970000 ms
17/04/17 14:02:50 INFO JobScheduler: Starting job streaming job 1492417970000 ms.0 from job set of time 1492417970000 ms
-------------------------------------------
Time: 1492417970000 ms
-------------------------------------------

17/04/17 14:02:50 INFO JobScheduler: Finished job streaming job 1492417970000 ms.0 from job set of time 1492417970000 ms
17/04/17 14:02:50 INFO MapPartitionsRDD: Removing RDD 285 from persistence list
17/04/17 14:02:50 INFO JobScheduler: Total delay: 0.005 s for time 1492417970000 ms (execution: 0.000 s)
17/04/17 14:02:50 INFO BlockManager: Removing RDD 285
17/04/17 14:02:50 INFO MapPartitionsRDD: Removing RDD 284 from persistence list
17/04/17 14:02:50 INFO BlockManager: Removing RDD 284
17/04/17 14:02:50 INFO BlockRDD: Removing RDD 283 from persistence list
17/04/17 14:02:50 INFO BlockManager: Removing RDD 283
17/04/17 14:02:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[283] at createStream at KafkaConsumer.java:64 of time 1492417970000 ms
17/04/17 14:02:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417950000 ms)
17/04/17 14:02:50 INFO InputInfoTracker: remove old batch metadata: 1492417950000 ms
17/04/17 14:02:50 WARN BlockManagerMaster: Failed to remove RDD 249 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:50 WARN BlockManagerMaster: Failed to remove RDD 248 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:02:50 WARN BlockManagerMaster: Failed to remove RDD 247 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:00 INFO JobScheduler: Added jobs for time 1492417980000 ms
17/04/17 14:03:00 INFO JobScheduler: Starting job streaming job 1492417980000 ms.0 from job set of time 1492417980000 ms
-------------------------------------------
Time: 1492417980000 ms
-------------------------------------------

17/04/17 14:03:00 INFO JobScheduler: Finished job streaming job 1492417980000 ms.0 from job set of time 1492417980000 ms
17/04/17 14:03:00 INFO MapPartitionsRDD: Removing RDD 288 from persistence list
17/04/17 14:03:00 INFO JobScheduler: Total delay: 0.006 s for time 1492417980000 ms (execution: 0.001 s)
17/04/17 14:03:00 INFO BlockManager: Removing RDD 288
17/04/17 14:03:00 INFO MapPartitionsRDD: Removing RDD 287 from persistence list
17/04/17 14:03:00 INFO BlockManager: Removing RDD 287
17/04/17 14:03:00 INFO BlockRDD: Removing RDD 286 from persistence list
17/04/17 14:03:00 WARN BlockManagerMaster: Failed to remove RDD 252 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[286] at createStream at KafkaConsumer.java:64 of time 1492417980000 ms
17/04/17 14:03:00 INFO BlockManager: Removing RDD 286
17/04/17 14:03:00 WARN BlockManagerMaster: Failed to remove RDD 251 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417960000 ms)
17/04/17 14:03:00 INFO InputInfoTracker: remove old batch metadata: 1492417960000 ms
17/04/17 14:03:00 WARN BlockManagerMaster: Failed to remove RDD 250 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:10 INFO JobScheduler: Added jobs for time 1492417990000 ms
17/04/17 14:03:10 INFO JobScheduler: Starting job streaming job 1492417990000 ms.0 from job set of time 1492417990000 ms
-------------------------------------------
Time: 1492417990000 ms
-------------------------------------------

17/04/17 14:03:10 INFO JobScheduler: Finished job streaming job 1492417990000 ms.0 from job set of time 1492417990000 ms
17/04/17 14:03:10 INFO MapPartitionsRDD: Removing RDD 291 from persistence list
17/04/17 14:03:10 INFO JobScheduler: Total delay: 0.007 s for time 1492417990000 ms (execution: 0.001 s)
17/04/17 14:03:10 INFO MapPartitionsRDD: Removing RDD 290 from persistence list
17/04/17 14:03:10 INFO BlockManager: Removing RDD 291
17/04/17 14:03:10 INFO BlockManager: Removing RDD 290
17/04/17 14:03:10 INFO BlockRDD: Removing RDD 289 from persistence list
17/04/17 14:03:10 INFO BlockManager: Removing RDD 289
17/04/17 14:03:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[289] at createStream at KafkaConsumer.java:64 of time 1492417990000 ms
17/04/17 14:03:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417970000 ms)
17/04/17 14:03:10 INFO InputInfoTracker: remove old batch metadata: 1492417970000 ms
17/04/17 14:03:10 WARN BlockManagerMaster: Failed to remove RDD 255 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:10 WARN BlockManagerMaster: Failed to remove RDD 254 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:10 WARN BlockManagerMaster: Failed to remove RDD 253 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:20 INFO JobScheduler: Added jobs for time 1492418000000 ms
-------------------------------------------
Time: 1492418000000 ms
-------------------------------------------

17/04/17 14:03:20 INFO JobScheduler: Starting job streaming job 1492418000000 ms.0 from job set of time 1492418000000 ms
17/04/17 14:03:20 INFO JobScheduler: Finished job streaming job 1492418000000 ms.0 from job set of time 1492418000000 ms
17/04/17 14:03:20 INFO MapPartitionsRDD: Removing RDD 294 from persistence list
17/04/17 14:03:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418000000 ms (execution: 0.001 s)
17/04/17 14:03:20 INFO BlockManager: Removing RDD 294
17/04/17 14:03:20 INFO MapPartitionsRDD: Removing RDD 293 from persistence list
17/04/17 14:03:20 INFO BlockManager: Removing RDD 293
17/04/17 14:03:20 INFO BlockRDD: Removing RDD 292 from persistence list
17/04/17 14:03:20 INFO BlockManager: Removing RDD 292
17/04/17 14:03:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[292] at createStream at KafkaConsumer.java:64 of time 1492418000000 ms
17/04/17 14:03:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417980000 ms)
17/04/17 14:03:20 INFO InputInfoTracker: remove old batch metadata: 1492417980000 ms
17/04/17 14:03:20 WARN BlockManagerMaster: Failed to remove RDD 258 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:20 WARN BlockManagerMaster: Failed to remove RDD 257 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:20 WARN BlockManagerMaster: Failed to remove RDD 256 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:30 INFO JobScheduler: Added jobs for time 1492418010000 ms
17/04/17 14:03:30 INFO JobScheduler: Starting job streaming job 1492418010000 ms.0 from job set of time 1492418010000 ms
-------------------------------------------
Time: 1492418010000 ms
-------------------------------------------

17/04/17 14:03:30 INFO JobScheduler: Finished job streaming job 1492418010000 ms.0 from job set of time 1492418010000 ms
17/04/17 14:03:30 INFO MapPartitionsRDD: Removing RDD 297 from persistence list
17/04/17 14:03:30 INFO JobScheduler: Total delay: 0.006 s for time 1492418010000 ms (execution: 0.001 s)
17/04/17 14:03:30 INFO BlockManager: Removing RDD 297
17/04/17 14:03:30 INFO MapPartitionsRDD: Removing RDD 296 from persistence list
17/04/17 14:03:30 INFO BlockManager: Removing RDD 296
17/04/17 14:03:30 INFO BlockRDD: Removing RDD 295 from persistence list
17/04/17 14:03:30 INFO BlockManager: Removing RDD 295
17/04/17 14:03:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[295] at createStream at KafkaConsumer.java:64 of time 1492418010000 ms
17/04/17 14:03:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492417990000 ms)
17/04/17 14:03:30 INFO InputInfoTracker: remove old batch metadata: 1492417990000 ms
17/04/17 14:03:30 WARN BlockManagerMaster: Failed to remove RDD 261 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:30 WARN BlockManagerMaster: Failed to remove RDD 260 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:30 ERROR TransportChannelHandler: Connection to am372811-PC/192.168.0.1:53874 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
17/04/17 14:03:30 ERROR TransportChannelHandler: Connection to am372811-PC/192.168.0.1:53872 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
17/04/17 14:03:30 WARN BlockManagerMaster: Failed to remove RDD 259 - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
17/04/17 14:03:40 INFO JobScheduler: Added jobs for time 1492418020000 ms
17/04/17 14:03:40 INFO JobScheduler: Starting job streaming job 1492418020000 ms.0 from job set of time 1492418020000 ms
-------------------------------------------
Time: 1492418020000 ms
-------------------------------------------

17/04/17 14:03:40 INFO JobScheduler: Finished job streaming job 1492418020000 ms.0 from job set of time 1492418020000 ms
17/04/17 14:03:40 INFO MapPartitionsRDD: Removing RDD 300 from persistence list
17/04/17 14:03:40 INFO JobScheduler: Total delay: 0.006 s for time 1492418020000 ms (execution: 0.000 s)
17/04/17 14:03:40 INFO BlockManager: Removing RDD 300
17/04/17 14:03:40 INFO MapPartitionsRDD: Removing RDD 299 from persistence list
17/04/17 14:03:40 INFO BlockManager: Removing RDD 299
17/04/17 14:03:40 INFO BlockRDD: Removing RDD 298 from persistence list
17/04/17 14:03:40 INFO BlockManager: Removing RDD 298
17/04/17 14:03:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[298] at createStream at KafkaConsumer.java:64 of time 1492418020000 ms
17/04/17 14:03:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418000000 ms)
17/04/17 14:03:40 INFO InputInfoTracker: remove old batch metadata: 1492418000000 ms
17/04/17 14:03:50 INFO JobScheduler: Added jobs for time 1492418030000 ms
17/04/17 14:03:50 INFO JobScheduler: Starting job streaming job 1492418030000 ms.0 from job set of time 1492418030000 ms
-------------------------------------------
Time: 1492418030000 ms
-------------------------------------------

17/04/17 14:03:50 INFO JobScheduler: Finished job streaming job 1492418030000 ms.0 from job set of time 1492418030000 ms
17/04/17 14:03:50 INFO MapPartitionsRDD: Removing RDD 303 from persistence list
17/04/17 14:03:50 INFO JobScheduler: Total delay: 0.006 s for time 1492418030000 ms (execution: 0.001 s)
17/04/17 14:03:50 INFO BlockManager: Removing RDD 303
17/04/17 14:03:50 INFO MapPartitionsRDD: Removing RDD 302 from persistence list
17/04/17 14:03:50 INFO BlockManager: Removing RDD 302
17/04/17 14:03:50 INFO BlockRDD: Removing RDD 301 from persistence list
17/04/17 14:03:50 INFO BlockManager: Removing RDD 301
17/04/17 14:03:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[301] at createStream at KafkaConsumer.java:64 of time 1492418030000 ms
17/04/17 14:03:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418010000 ms)
17/04/17 14:03:50 INFO InputInfoTracker: remove old batch metadata: 1492418010000 ms
17/04/17 14:04:00 INFO JobScheduler: Added jobs for time 1492418040000 ms
17/04/17 14:04:00 INFO JobScheduler: Starting job streaming job 1492418040000 ms.0 from job set of time 1492418040000 ms
-------------------------------------------
Time: 1492418040000 ms
-------------------------------------------

17/04/17 14:04:00 INFO JobScheduler: Finished job streaming job 1492418040000 ms.0 from job set of time 1492418040000 ms
17/04/17 14:04:00 INFO MapPartitionsRDD: Removing RDD 306 from persistence list
17/04/17 14:04:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418040000 ms (execution: 0.000 s)
17/04/17 14:04:00 INFO BlockManager: Removing RDD 306
17/04/17 14:04:00 INFO MapPartitionsRDD: Removing RDD 305 from persistence list
17/04/17 14:04:00 INFO BlockManager: Removing RDD 305
17/04/17 14:04:00 INFO BlockRDD: Removing RDD 304 from persistence list
17/04/17 14:04:00 INFO BlockManager: Removing RDD 304
17/04/17 14:04:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[304] at createStream at KafkaConsumer.java:64 of time 1492418040000 ms
17/04/17 14:04:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418020000 ms)
17/04/17 14:04:00 INFO InputInfoTracker: remove old batch metadata: 1492418020000 ms
17/04/17 14:04:10 INFO JobScheduler: Added jobs for time 1492418050000 ms
17/04/17 14:04:10 INFO JobScheduler: Starting job streaming job 1492418050000 ms.0 from job set of time 1492418050000 ms
-------------------------------------------
Time: 1492418050000 ms
-------------------------------------------

17/04/17 14:04:10 INFO JobScheduler: Finished job streaming job 1492418050000 ms.0 from job set of time 1492418050000 ms
17/04/17 14:04:10 INFO MapPartitionsRDD: Removing RDD 309 from persistence list
17/04/17 14:04:10 INFO JobScheduler: Total delay: 0.007 s for time 1492418050000 ms (execution: 0.001 s)
17/04/17 14:04:10 INFO BlockManager: Removing RDD 309
17/04/17 14:04:10 INFO MapPartitionsRDD: Removing RDD 308 from persistence list
17/04/17 14:04:10 INFO BlockManager: Removing RDD 308
17/04/17 14:04:10 INFO BlockRDD: Removing RDD 307 from persistence list
17/04/17 14:04:10 INFO BlockManager: Removing RDD 307
17/04/17 14:04:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[307] at createStream at KafkaConsumer.java:64 of time 1492418050000 ms
17/04/17 14:04:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418030000 ms)
17/04/17 14:04:10 INFO InputInfoTracker: remove old batch metadata: 1492418030000 ms
17/04/17 14:04:20 INFO JobScheduler: Added jobs for time 1492418060000 ms
17/04/17 14:04:20 INFO JobScheduler: Starting job streaming job 1492418060000 ms.0 from job set of time 1492418060000 ms
-------------------------------------------
Time: 1492418060000 ms
-------------------------------------------

17/04/17 14:04:20 INFO JobScheduler: Finished job streaming job 1492418060000 ms.0 from job set of time 1492418060000 ms
17/04/17 14:04:20 INFO MapPartitionsRDD: Removing RDD 312 from persistence list
17/04/17 14:04:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418060000 ms (execution: 0.001 s)
17/04/17 14:04:20 INFO BlockManager: Removing RDD 312
17/04/17 14:04:20 INFO MapPartitionsRDD: Removing RDD 311 from persistence list
17/04/17 14:04:20 INFO BlockManager: Removing RDD 311
17/04/17 14:04:20 INFO BlockRDD: Removing RDD 310 from persistence list
17/04/17 14:04:20 INFO BlockManager: Removing RDD 310
17/04/17 14:04:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[310] at createStream at KafkaConsumer.java:64 of time 1492418060000 ms
17/04/17 14:04:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418040000 ms)
17/04/17 14:04:20 INFO InputInfoTracker: remove old batch metadata: 1492418040000 ms
17/04/17 14:04:30 INFO JobScheduler: Added jobs for time 1492418070000 ms
17/04/17 14:04:30 INFO JobScheduler: Starting job streaming job 1492418070000 ms.0 from job set of time 1492418070000 ms
-------------------------------------------
Time: 1492418070000 ms
-------------------------------------------

17/04/17 14:04:30 INFO JobScheduler: Finished job streaming job 1492418070000 ms.0 from job set of time 1492418070000 ms
17/04/17 14:04:30 INFO MapPartitionsRDD: Removing RDD 315 from persistence list
17/04/17 14:04:30 INFO JobScheduler: Total delay: 0.006 s for time 1492418070000 ms (execution: 0.001 s)
17/04/17 14:04:30 INFO BlockManager: Removing RDD 315
17/04/17 14:04:30 INFO MapPartitionsRDD: Removing RDD 314 from persistence list
17/04/17 14:04:30 INFO BlockManager: Removing RDD 314
17/04/17 14:04:30 INFO BlockRDD: Removing RDD 313 from persistence list
17/04/17 14:04:30 INFO BlockManager: Removing RDD 313
17/04/17 14:04:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[313] at createStream at KafkaConsumer.java:64 of time 1492418070000 ms
17/04/17 14:04:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418050000 ms)
17/04/17 14:04:30 INFO InputInfoTracker: remove old batch metadata: 1492418050000 ms
17/04/17 14:04:34 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 2 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 14 more
17/04/17 14:04:34 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 6 more
17/04/17 14:04:34 ERROR TransportChannelHandler: Connection to carl-PC/192.168.0.2:7077 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
17/04/17 14:04:34 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 2 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 6 more
17/04/17 14:04:34 WARN AppClient$ClientEndpoint: Connection to carl-PC:7077 failed; waiting for master to reconnect...
17/04/17 14:04:34 WARN SparkDeploySchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...
17/04/17 14:04:34 WARN AppClient$ClientEndpoint: Connection to carl-PC:7077 failed; waiting for master to reconnect...
17/04/17 14:04:37 ERROR TransportClient: Failed to send RPC 8049128768632333171 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:37 ERROR TransportClient: Failed to send RPC 7267675895984521839 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:37 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 2 attempts
java.io.IOException: Failed to send RPC 8049128768632333171 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:37 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 1 attempts
java.io.IOException: Failed to send RPC 7267675895984521839 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:37 ERROR TransportClient: Failed to send RPC 8493946507099529222 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:37 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 3 attempts
java.io.IOException: Failed to send RPC 8493946507099529222 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:37 WARN NettyRpcEnv: Ignored failure: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))]
17/04/17 14:04:40 INFO JobScheduler: Added jobs for time 1492418080000 ms
17/04/17 14:04:40 INFO JobScheduler: Starting job streaming job 1492418080000 ms.0 from job set of time 1492418080000 ms
-------------------------------------------
Time: 1492418080000 ms
-------------------------------------------

17/04/17 14:04:40 INFO JobScheduler: Finished job streaming job 1492418080000 ms.0 from job set of time 1492418080000 ms
17/04/17 14:04:40 INFO MapPartitionsRDD: Removing RDD 318 from persistence list
17/04/17 14:04:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418080000 ms (execution: 0.000 s)
17/04/17 14:04:40 INFO BlockManager: Removing RDD 318
17/04/17 14:04:40 INFO MapPartitionsRDD: Removing RDD 317 from persistence list
17/04/17 14:04:40 INFO BlockManager: Removing RDD 317
17/04/17 14:04:40 INFO BlockRDD: Removing RDD 316 from persistence list
17/04/17 14:04:40 INFO BlockManager: Removing RDD 316
17/04/17 14:04:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[316] at createStream at KafkaConsumer.java:64 of time 1492418080000 ms
17/04/17 14:04:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418060000 ms)
17/04/17 14:04:40 INFO InputInfoTracker: remove old batch metadata: 1492418060000 ms
17/04/17 14:04:40 ERROR TransportClient: Failed to send RPC 5682553158085813056 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:40 ERROR TransportClient: Failed to send RPC 7715239564059773010 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:40 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 3 attempts
java.io.IOException: Failed to send RPC 5682553158085813056 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:40 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 2 attempts
java.io.IOException: Failed to send RPC 7715239564059773010 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:40 WARN NettyRpcEnv: Ignored failure: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))]
17/04/17 14:04:43 ERROR TransportClient: Failed to send RPC 8954736065181916095 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:43 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 3 attempts
java.io.IOException: Failed to send RPC 8954736065181916095 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:43 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 8954736065181916095 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:43 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(2))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	... 3 more
Caused by: java.io.IOException: Failed to send RPC 8954736065181916095 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:43 ERROR TaskSchedulerImpl: Lost executor 0 on am372811-PC: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/17 14:04:43 INFO SparkDeploySchedulerBackend: Requesting to kill executor(s) 1
17/04/17 14:04:43 INFO DAGScheduler: Executor lost: 0 (epoch 5)
17/04/17 14:04:43 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/04/17 14:04:43 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
17/04/17 14:04:43 ERROR TransportClient: Failed to send RPC 6730585941045885688 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:43 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 1 attempts
java.io.IOException: Failed to send RPC 6730585941045885688 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:46 ERROR TransportClient: Failed to send RPC 9156362092279719594 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:46 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 2 attempts
java.io.IOException: Failed to send RPC 9156362092279719594 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:49 ERROR TransportClient: Failed to send RPC 6509387753017879923 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:49 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 3 attempts
java.io.IOException: Failed to send RPC 6509387753017879923 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:49 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 1 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 6509387753017879923 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:50 INFO JobScheduler: Added jobs for time 1492418090000 ms
17/04/17 14:04:50 INFO JobScheduler: Starting job streaming job 1492418090000 ms.0 from job set of time 1492418090000 ms
-------------------------------------------
Time: 1492418090000 ms
-------------------------------------------

17/04/17 14:04:50 INFO JobScheduler: Finished job streaming job 1492418090000 ms.0 from job set of time 1492418090000 ms
17/04/17 14:04:50 INFO MapPartitionsRDD: Removing RDD 321 from persistence list
17/04/17 14:04:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418090000 ms (execution: 0.000 s)
17/04/17 14:04:50 INFO BlockManager: Removing RDD 321
17/04/17 14:04:50 INFO MapPartitionsRDD: Removing RDD 320 from persistence list
17/04/17 14:04:50 INFO BlockRDD: Removing RDD 319 from persistence list
17/04/17 14:04:50 INFO BlockManager: Removing RDD 319
17/04/17 14:04:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[319] at createStream at KafkaConsumer.java:64 of time 1492418090000 ms
17/04/17 14:04:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418070000 ms)
17/04/17 14:04:50 INFO InputInfoTracker: remove old batch metadata: 1492418070000 ms
17/04/17 14:04:50 INFO BlockManager: Removing RDD 320
17/04/17 14:04:52 ERROR TransportClient: Failed to send RPC 8148311062899464082 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:52 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 1 attempts
java.io.IOException: Failed to send RPC 8148311062899464082 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:55 ERROR TransportClient: Failed to send RPC 7082146348093568944 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:55 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 2 attempts
java.io.IOException: Failed to send RPC 7082146348093568944 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:58 ERROR TransportClient: Failed to send RPC 9074702262216588418 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:04:58 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 3 attempts
java.io.IOException: Failed to send RPC 9074702262216588418 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:04:58 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 2 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 9074702262216588418 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:00 INFO JobScheduler: Added jobs for time 1492418100000 ms
17/04/17 14:05:00 INFO JobScheduler: Starting job streaming job 1492418100000 ms.0 from job set of time 1492418100000 ms
-------------------------------------------
Time: 1492418100000 ms
-------------------------------------------

17/04/17 14:05:00 INFO JobScheduler: Finished job streaming job 1492418100000 ms.0 from job set of time 1492418100000 ms
17/04/17 14:05:00 INFO MapPartitionsRDD: Removing RDD 324 from persistence list
17/04/17 14:05:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418100000 ms (execution: 0.000 s)
17/04/17 14:05:00 INFO BlockManager: Removing RDD 324
17/04/17 14:05:00 INFO MapPartitionsRDD: Removing RDD 323 from persistence list
17/04/17 14:05:00 INFO BlockManager: Removing RDD 323
17/04/17 14:05:00 INFO BlockRDD: Removing RDD 322 from persistence list
17/04/17 14:05:00 INFO BlockManager: Removing RDD 322
17/04/17 14:05:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[322] at createStream at KafkaConsumer.java:64 of time 1492418100000 ms
17/04/17 14:05:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418080000 ms)
17/04/17 14:05:00 INFO InputInfoTracker: remove old batch metadata: 1492418080000 ms
17/04/17 14:05:01 ERROR TransportClient: Failed to send RPC 5995698900574034177 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:01 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 1 attempts
java.io.IOException: Failed to send RPC 5995698900574034177 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:04 ERROR TransportClient: Failed to send RPC 8747711083281826231 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:04 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 2 attempts
java.io.IOException: Failed to send RPC 8747711083281826231 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:07 ERROR TransportClient: Failed to send RPC 6840896115980705461 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:07 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 3 attempts
java.io.IOException: Failed to send RPC 6840896115980705461 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:07 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 6840896115980705461 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:07 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List(1))]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	... 3 more
Caused by: java.io.IOException: Failed to send RPC 6840896115980705461 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:07 ERROR TaskSchedulerImpl: Lost executor 3 on am372811-PC: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/17 14:05:07 INFO SparkDeploySchedulerBackend: Requesting to kill executor(s) 0
17/04/17 14:05:07 INFO DAGScheduler: Executor lost: 3 (epoch 6)
17/04/17 14:05:07 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
17/04/17 14:05:07 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
17/04/17 14:05:07 WARN SparkDeploySchedulerBackend: Executor to kill 0 does not exist!
17/04/17 14:05:07 ERROR TransportClient: Failed to send RPC 6178631014988526657 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:07 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 6178631014988526657 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:10 INFO JobScheduler: Starting job streaming job 1492418110000 ms.0 from job set of time 1492418110000 ms
-------------------------------------------
Time: 1492418110000 ms
-------------------------------------------

17/04/17 14:05:10 INFO JobScheduler: Added jobs for time 1492418110000 ms
17/04/17 14:05:10 INFO JobScheduler: Finished job streaming job 1492418110000 ms.0 from job set of time 1492418110000 ms
17/04/17 14:05:10 INFO MapPartitionsRDD: Removing RDD 327 from persistence list
17/04/17 14:05:10 INFO JobScheduler: Total delay: 0.006 s for time 1492418110000 ms (execution: 0.001 s)
17/04/17 14:05:10 INFO BlockManager: Removing RDD 327
17/04/17 14:05:10 INFO MapPartitionsRDD: Removing RDD 326 from persistence list
17/04/17 14:05:10 INFO BlockRDD: Removing RDD 325 from persistence list
17/04/17 14:05:10 INFO BlockManager: Removing RDD 326
17/04/17 14:05:10 INFO BlockManager: Removing RDD 325
17/04/17 14:05:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[325] at createStream at KafkaConsumer.java:64 of time 1492418110000 ms
17/04/17 14:05:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418090000 ms)
17/04/17 14:05:10 INFO InputInfoTracker: remove old batch metadata: 1492418090000 ms
17/04/17 14:05:10 ERROR TransportClient: Failed to send RPC 5052842317583201720 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:10 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 5052842317583201720 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:13 ERROR TransportClient: Failed to send RPC 5691356173505157829 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:13 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 5691356173505157829 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:13 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 5691356173505157829 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:16 ERROR TransportClient: Failed to send RPC 6693937281469725632 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:16 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 6693937281469725632 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:19 ERROR TransportClient: Failed to send RPC 5526869361631786116 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:19 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 5526869361631786116 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:20 INFO JobScheduler: Added jobs for time 1492418120000 ms
17/04/17 14:05:20 INFO JobScheduler: Starting job streaming job 1492418120000 ms.0 from job set of time 1492418120000 ms
-------------------------------------------
Time: 1492418120000 ms
-------------------------------------------

17/04/17 14:05:20 INFO JobScheduler: Finished job streaming job 1492418120000 ms.0 from job set of time 1492418120000 ms
17/04/17 14:05:20 INFO MapPartitionsRDD: Removing RDD 330 from persistence list
17/04/17 14:05:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418120000 ms (execution: 0.001 s)
17/04/17 14:05:20 INFO BlockManager: Removing RDD 330
17/04/17 14:05:20 INFO MapPartitionsRDD: Removing RDD 329 from persistence list
17/04/17 14:05:20 INFO BlockManager: Removing RDD 329
17/04/17 14:05:20 INFO BlockRDD: Removing RDD 328 from persistence list
17/04/17 14:05:20 INFO BlockManager: Removing RDD 328
17/04/17 14:05:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[328] at createStream at KafkaConsumer.java:64 of time 1492418120000 ms
17/04/17 14:05:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418100000 ms)
17/04/17 14:05:20 INFO InputInfoTracker: remove old batch metadata: 1492418100000 ms
17/04/17 14:05:22 ERROR TransportClient: Failed to send RPC 8440010375262477926 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:22 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 8440010375262477926 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:22 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 8440010375262477926 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:25 ERROR TransportClient: Failed to send RPC 8056462159616923365 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:25 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 8056462159616923365 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:28 ERROR TransportClient: Failed to send RPC 7087270816378201775 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:28 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 7087270816378201775 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:30 INFO JobScheduler: Added jobs for time 1492418130000 ms
17/04/17 14:05:30 INFO JobScheduler: Starting job streaming job 1492418130000 ms.0 from job set of time 1492418130000 ms
-------------------------------------------
Time: 1492418130000 ms
-------------------------------------------

17/04/17 14:05:30 INFO JobScheduler: Finished job streaming job 1492418130000 ms.0 from job set of time 1492418130000 ms
17/04/17 14:05:30 INFO MapPartitionsRDD: Removing RDD 333 from persistence list
17/04/17 14:05:30 INFO JobScheduler: Total delay: 0.007 s for time 1492418130000 ms (execution: 0.001 s)
17/04/17 14:05:30 INFO BlockManager: Removing RDD 333
17/04/17 14:05:30 INFO MapPartitionsRDD: Removing RDD 332 from persistence list
17/04/17 14:05:30 INFO BlockManager: Removing RDD 332
17/04/17 14:05:30 INFO BlockRDD: Removing RDD 331 from persistence list
17/04/17 14:05:30 INFO BlockManager: Removing RDD 331
17/04/17 14:05:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[331] at createStream at KafkaConsumer.java:64 of time 1492418130000 ms
17/04/17 14:05:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418110000 ms)
17/04/17 14:05:30 INFO InputInfoTracker: remove old batch metadata: 1492418110000 ms
17/04/17 14:05:31 ERROR TransportClient: Failed to send RPC 5246078614747282834 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:31 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 5246078614747282834 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:31 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 5246078614747282834 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:31 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	... 3 more
Caused by: java.io.IOException: Failed to send RPC 5246078614747282834 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:31 INFO SparkDeploySchedulerBackend: Requesting to kill executor(s) 3
17/04/17 14:05:31 WARN SparkDeploySchedulerBackend: Executor to kill 3 does not exist!
17/04/17 14:05:31 ERROR TransportClient: Failed to send RPC 7765431333711977468 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:31 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 7765431333711977468 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:34 ERROR TransportClient: Failed to send RPC 7073014904322072350 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:34 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 7073014904322072350 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:37 ERROR TransportClient: Failed to send RPC 5710111789976493434 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:37 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 5710111789976493434 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:37 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 5710111789976493434 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:40 INFO JobScheduler: Added jobs for time 1492418140000 ms
17/04/17 14:05:40 INFO JobScheduler: Starting job streaming job 1492418140000 ms.0 from job set of time 1492418140000 ms
-------------------------------------------
Time: 1492418140000 ms
-------------------------------------------

17/04/17 14:05:40 INFO JobScheduler: Finished job streaming job 1492418140000 ms.0 from job set of time 1492418140000 ms
17/04/17 14:05:40 INFO MapPartitionsRDD: Removing RDD 336 from persistence list
17/04/17 14:05:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418140000 ms (execution: 0.000 s)
17/04/17 14:05:40 INFO BlockManager: Removing RDD 336
17/04/17 14:05:40 INFO MapPartitionsRDD: Removing RDD 335 from persistence list
17/04/17 14:05:40 INFO BlockManager: Removing RDD 335
17/04/17 14:05:40 INFO BlockRDD: Removing RDD 334 from persistence list
17/04/17 14:05:40 INFO BlockManager: Removing RDD 334
17/04/17 14:05:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[334] at createStream at KafkaConsumer.java:64 of time 1492418140000 ms
17/04/17 14:05:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418120000 ms)
17/04/17 14:05:40 INFO InputInfoTracker: remove old batch metadata: 1492418120000 ms
17/04/17 14:05:40 ERROR TransportClient: Failed to send RPC 5481133758088942714 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:40 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 5481133758088942714 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:43 ERROR TransportClient: Failed to send RPC 6302182156829069670 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:43 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 6302182156829069670 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:46 ERROR TransportClient: Failed to send RPC 6075095821178159562 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:46 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 6075095821178159562 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:46 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 6075095821178159562 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:49 ERROR TransportClient: Failed to send RPC 7857002753732546065 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:49 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 1 attempts
java.io.IOException: Failed to send RPC 7857002753732546065 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:50 INFO JobScheduler: Added jobs for time 1492418150000 ms
17/04/17 14:05:50 INFO JobScheduler: Starting job streaming job 1492418150000 ms.0 from job set of time 1492418150000 ms
-------------------------------------------
Time: 1492418150000 ms
-------------------------------------------

17/04/17 14:05:50 INFO JobScheduler: Finished job streaming job 1492418150000 ms.0 from job set of time 1492418150000 ms
17/04/17 14:05:50 INFO MapPartitionsRDD: Removing RDD 339 from persistence list
17/04/17 14:05:50 INFO JobScheduler: Total delay: 0.007 s for time 1492418150000 ms (execution: 0.001 s)
17/04/17 14:05:50 INFO BlockManager: Removing RDD 339
17/04/17 14:05:50 INFO MapPartitionsRDD: Removing RDD 338 from persistence list
17/04/17 14:05:50 INFO BlockManager: Removing RDD 338
17/04/17 14:05:50 INFO BlockRDD: Removing RDD 337 from persistence list
17/04/17 14:05:50 INFO BlockManager: Removing RDD 337
17/04/17 14:05:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[337] at createStream at KafkaConsumer.java:64 of time 1492418150000 ms
17/04/17 14:05:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418130000 ms)
17/04/17 14:05:50 INFO InputInfoTracker: remove old batch metadata: 1492418130000 ms
17/04/17 14:05:52 ERROR TransportClient: Failed to send RPC 6509626895142135391 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:52 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 2 attempts
java.io.IOException: Failed to send RPC 6509626895142135391 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:55 ERROR TransportClient: Failed to send RPC 6416477919604106667 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:05:55 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
java.io.IOException: Failed to send RPC 6416477919604106667 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:55 WARN NettyRpcEndpointRef: Error sending message [message = KillExecutors(app-20170417134630-0000,List())] in 3 attempts
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send RPC 6416477919604106667 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:05:55 ERROR Utils: Uncaught exception in thread kill-executor-thread
org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient.killExecutors(AppClient.scala:318)
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.doKillExecutors(SparkDeploySchedulerBackend.scala:178)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:512)
	at org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1499)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3$$anonfun$run$3.apply$mcV$sp(HeartbeatReceiver.scala:206)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.HeartbeatReceiver$$anonfun$org$apache$spark$HeartbeatReceiver$$expireDeadHosts$3$$anon$3.run(HeartbeatReceiver.scala:203)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Error sending message [message = KillExecutors(app-20170417134630-0000,List())]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:118)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$3.run(AppClient.scala:227)
	... 3 more
Caused by: java.io.IOException: Failed to send RPC 6416477919604106667 to carl-PC/192.168.0.2:7077: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:239)
	at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:226)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:680)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:567)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:424)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:801)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:699)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1122)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:633)
	at io.netty.channel.AbstractChannelHandlerContext.access$1900(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:908)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:960)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
Caused by: java.nio.channels.ClosedChannelException
17/04/17 14:06:00 INFO JobScheduler: Added jobs for time 1492418160000 ms
17/04/17 14:06:00 INFO JobScheduler: Starting job streaming job 1492418160000 ms.0 from job set of time 1492418160000 ms
-------------------------------------------
Time: 1492418160000 ms
-------------------------------------------

17/04/17 14:06:00 INFO JobScheduler: Finished job streaming job 1492418160000 ms.0 from job set of time 1492418160000 ms
17/04/17 14:06:00 INFO MapPartitionsRDD: Removing RDD 342 from persistence list
17/04/17 14:06:00 INFO JobScheduler: Total delay: 0.006 s for time 1492418160000 ms (execution: 0.001 s)
17/04/17 14:06:00 INFO MapPartitionsRDD: Removing RDD 341 from persistence list
17/04/17 14:06:00 INFO BlockManager: Removing RDD 342
17/04/17 14:06:00 INFO BlockManager: Removing RDD 341
17/04/17 14:06:00 INFO BlockRDD: Removing RDD 340 from persistence list
17/04/17 14:06:00 INFO BlockManager: Removing RDD 340
17/04/17 14:06:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[340] at createStream at KafkaConsumer.java:64 of time 1492418160000 ms
17/04/17 14:06:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418140000 ms)
17/04/17 14:06:00 INFO InputInfoTracker: remove old batch metadata: 1492418140000 ms
17/04/17 14:06:10 INFO JobScheduler: Added jobs for time 1492418170000 ms
17/04/17 14:06:10 INFO JobScheduler: Starting job streaming job 1492418170000 ms.0 from job set of time 1492418170000 ms
-------------------------------------------
Time: 1492418170000 ms
-------------------------------------------

17/04/17 14:06:10 INFO JobScheduler: Finished job streaming job 1492418170000 ms.0 from job set of time 1492418170000 ms
17/04/17 14:06:10 INFO MapPartitionsRDD: Removing RDD 345 from persistence list
17/04/17 14:06:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418170000 ms (execution: 0.001 s)
17/04/17 14:06:10 INFO BlockManager: Removing RDD 345
17/04/17 14:06:10 INFO MapPartitionsRDD: Removing RDD 344 from persistence list
17/04/17 14:06:10 INFO BlockManager: Removing RDD 344
17/04/17 14:06:10 INFO BlockRDD: Removing RDD 343 from persistence list
17/04/17 14:06:10 INFO BlockManager: Removing RDD 343
17/04/17 14:06:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[343] at createStream at KafkaConsumer.java:64 of time 1492418170000 ms
17/04/17 14:06:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418150000 ms)
17/04/17 14:06:10 INFO InputInfoTracker: remove old batch metadata: 1492418150000 ms
17/04/17 14:06:20 INFO JobScheduler: Added jobs for time 1492418180000 ms
-------------------------------------------
Time: 1492418180000 ms
-------------------------------------------

17/04/17 14:06:20 INFO JobScheduler: Starting job streaming job 1492418180000 ms.0 from job set of time 1492418180000 ms
17/04/17 14:06:20 INFO JobScheduler: Finished job streaming job 1492418180000 ms.0 from job set of time 1492418180000 ms
17/04/17 14:06:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418180000 ms (execution: 0.000 s)
17/04/17 14:06:20 INFO MapPartitionsRDD: Removing RDD 348 from persistence list
17/04/17 14:06:20 INFO BlockManager: Removing RDD 348
17/04/17 14:06:20 INFO MapPartitionsRDD: Removing RDD 347 from persistence list
17/04/17 14:06:20 INFO BlockManager: Removing RDD 347
17/04/17 14:06:20 INFO BlockRDD: Removing RDD 346 from persistence list
17/04/17 14:06:20 INFO BlockManager: Removing RDD 346
17/04/17 14:06:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[346] at createStream at KafkaConsumer.java:64 of time 1492418180000 ms
17/04/17 14:06:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418160000 ms)
17/04/17 14:06:20 INFO InputInfoTracker: remove old batch metadata: 1492418160000 ms
17/04/17 14:06:30 INFO JobScheduler: Added jobs for time 1492418190000 ms
17/04/17 14:06:30 INFO JobScheduler: Starting job streaming job 1492418190000 ms.0 from job set of time 1492418190000 ms
-------------------------------------------
Time: 1492418190000 ms
-------------------------------------------

17/04/17 14:06:30 INFO JobScheduler: Finished job streaming job 1492418190000 ms.0 from job set of time 1492418190000 ms
17/04/17 14:06:30 INFO MapPartitionsRDD: Removing RDD 351 from persistence list
17/04/17 14:06:30 INFO JobScheduler: Total delay: 0.006 s for time 1492418190000 ms (execution: 0.001 s)
17/04/17 14:06:30 INFO BlockManager: Removing RDD 351
17/04/17 14:06:30 INFO MapPartitionsRDD: Removing RDD 350 from persistence list
17/04/17 14:06:30 INFO BlockManager: Removing RDD 350
17/04/17 14:06:30 INFO BlockRDD: Removing RDD 349 from persistence list
17/04/17 14:06:30 INFO BlockManager: Removing RDD 349
17/04/17 14:06:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[349] at createStream at KafkaConsumer.java:64 of time 1492418190000 ms
17/04/17 14:06:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418170000 ms)
17/04/17 14:06:30 INFO InputInfoTracker: remove old batch metadata: 1492418170000 ms
17/04/17 14:06:40 INFO JobScheduler: Added jobs for time 1492418200000 ms
17/04/17 14:06:40 INFO JobScheduler: Starting job streaming job 1492418200000 ms.0 from job set of time 1492418200000 ms
-------------------------------------------
Time: 1492418200000 ms
-------------------------------------------

17/04/17 14:06:40 INFO JobScheduler: Finished job streaming job 1492418200000 ms.0 from job set of time 1492418200000 ms
17/04/17 14:06:40 INFO MapPartitionsRDD: Removing RDD 354 from persistence list
17/04/17 14:06:40 INFO JobScheduler: Total delay: 0.006 s for time 1492418200000 ms (execution: 0.001 s)
17/04/17 14:06:40 INFO BlockManager: Removing RDD 354
17/04/17 14:06:40 INFO MapPartitionsRDD: Removing RDD 353 from persistence list
17/04/17 14:06:40 INFO BlockManager: Removing RDD 353
17/04/17 14:06:40 INFO BlockRDD: Removing RDD 352 from persistence list
17/04/17 14:06:40 INFO BlockManager: Removing RDD 352
17/04/17 14:06:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[352] at createStream at KafkaConsumer.java:64 of time 1492418200000 ms
17/04/17 14:06:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418180000 ms)
17/04/17 14:06:40 INFO InputInfoTracker: remove old batch metadata: 1492418180000 ms
17/04/17 14:06:50 INFO JobScheduler: Added jobs for time 1492418210000 ms
17/04/17 14:06:50 INFO JobScheduler: Starting job streaming job 1492418210000 ms.0 from job set of time 1492418210000 ms
-------------------------------------------
Time: 1492418210000 ms
-------------------------------------------

17/04/17 14:06:50 INFO JobScheduler: Finished job streaming job 1492418210000 ms.0 from job set of time 1492418210000 ms
17/04/17 14:06:50 INFO MapPartitionsRDD: Removing RDD 357 from persistence list
17/04/17 14:06:50 INFO JobScheduler: Total delay: 0.006 s for time 1492418210000 ms (execution: 0.001 s)
17/04/17 14:06:50 INFO BlockManager: Removing RDD 357
17/04/17 14:06:50 INFO MapPartitionsRDD: Removing RDD 356 from persistence list
17/04/17 14:06:50 INFO BlockManager: Removing RDD 356
17/04/17 14:06:50 INFO BlockRDD: Removing RDD 355 from persistence list
17/04/17 14:06:50 INFO BlockManager: Removing RDD 355
17/04/17 14:06:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[355] at createStream at KafkaConsumer.java:64 of time 1492418210000 ms
17/04/17 14:06:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418190000 ms)
17/04/17 14:06:50 INFO InputInfoTracker: remove old batch metadata: 1492418190000 ms
17/04/17 14:07:00 INFO JobScheduler: Added jobs for time 1492418220000 ms
17/04/17 14:07:00 INFO JobScheduler: Starting job streaming job 1492418220000 ms.0 from job set of time 1492418220000 ms
-------------------------------------------
Time: 1492418220000 ms
-------------------------------------------

17/04/17 14:07:00 INFO JobScheduler: Finished job streaming job 1492418220000 ms.0 from job set of time 1492418220000 ms
17/04/17 14:07:00 INFO MapPartitionsRDD: Removing RDD 360 from persistence list
17/04/17 14:07:00 INFO JobScheduler: Total delay: 0.007 s for time 1492418220000 ms (execution: 0.001 s)
17/04/17 14:07:00 INFO BlockManager: Removing RDD 360
17/04/17 14:07:00 INFO MapPartitionsRDD: Removing RDD 359 from persistence list
17/04/17 14:07:00 INFO BlockManager: Removing RDD 359
17/04/17 14:07:00 INFO BlockRDD: Removing RDD 358 from persistence list
17/04/17 14:07:00 INFO BlockManager: Removing RDD 358
17/04/17 14:07:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[358] at createStream at KafkaConsumer.java:64 of time 1492418220000 ms
17/04/17 14:07:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418200000 ms)
17/04/17 14:07:00 INFO InputInfoTracker: remove old batch metadata: 1492418200000 ms
17/04/17 14:07:10 INFO JobScheduler: Starting job streaming job 1492418230000 ms.0 from job set of time 1492418230000 ms
-------------------------------------------
Time: 1492418230000 ms
-------------------------------------------

17/04/17 14:07:10 INFO JobScheduler: Added jobs for time 1492418230000 ms
17/04/17 14:07:10 INFO JobScheduler: Finished job streaming job 1492418230000 ms.0 from job set of time 1492418230000 ms
17/04/17 14:07:10 INFO MapPartitionsRDD: Removing RDD 363 from persistence list
17/04/17 14:07:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418230000 ms (execution: 0.000 s)
17/04/17 14:07:10 INFO BlockManager: Removing RDD 363
17/04/17 14:07:10 INFO MapPartitionsRDD: Removing RDD 362 from persistence list
17/04/17 14:07:10 INFO BlockManager: Removing RDD 362
17/04/17 14:07:10 INFO BlockRDD: Removing RDD 361 from persistence list
17/04/17 14:07:10 INFO BlockManager: Removing RDD 361
17/04/17 14:07:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[361] at createStream at KafkaConsumer.java:64 of time 1492418230000 ms
17/04/17 14:07:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418210000 ms)
17/04/17 14:07:10 INFO InputInfoTracker: remove old batch metadata: 1492418210000 ms
17/04/17 14:07:20 INFO JobScheduler: Added jobs for time 1492418240000 ms
17/04/17 14:07:20 INFO JobScheduler: Starting job streaming job 1492418240000 ms.0 from job set of time 1492418240000 ms
-------------------------------------------
Time: 1492418240000 ms
-------------------------------------------

17/04/17 14:07:20 INFO JobScheduler: Finished job streaming job 1492418240000 ms.0 from job set of time 1492418240000 ms
17/04/17 14:07:20 INFO MapPartitionsRDD: Removing RDD 366 from persistence list
17/04/17 14:07:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418240000 ms (execution: 0.001 s)
17/04/17 14:07:20 INFO BlockManager: Removing RDD 366
17/04/17 14:07:20 INFO MapPartitionsRDD: Removing RDD 365 from persistence list
17/04/17 14:07:20 INFO BlockManager: Removing RDD 365
17/04/17 14:07:20 INFO BlockRDD: Removing RDD 364 from persistence list
17/04/17 14:07:20 INFO BlockManager: Removing RDD 364
17/04/17 14:07:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[364] at createStream at KafkaConsumer.java:64 of time 1492418240000 ms
17/04/17 14:07:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418220000 ms)
17/04/17 14:07:20 INFO InputInfoTracker: remove old batch metadata: 1492418220000 ms
17/04/17 14:07:30 INFO JobScheduler: Added jobs for time 1492418250000 ms
17/04/17 14:07:30 INFO JobScheduler: Starting job streaming job 1492418250000 ms.0 from job set of time 1492418250000 ms
-------------------------------------------
Time: 1492418250000 ms
-------------------------------------------

17/04/17 14:07:30 INFO JobScheduler: Finished job streaming job 1492418250000 ms.0 from job set of time 1492418250000 ms
17/04/17 14:07:30 INFO MapPartitionsRDD: Removing RDD 369 from persistence list
17/04/17 14:07:30 INFO JobScheduler: Total delay: 0.007 s for time 1492418250000 ms (execution: 0.000 s)
17/04/17 14:07:30 INFO BlockManager: Removing RDD 369
17/04/17 14:07:30 INFO MapPartitionsRDD: Removing RDD 368 from persistence list
17/04/17 14:07:30 INFO BlockManager: Removing RDD 368
17/04/17 14:07:30 INFO BlockRDD: Removing RDD 367 from persistence list
17/04/17 14:07:30 INFO BlockManager: Removing RDD 367
17/04/17 14:07:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[367] at createStream at KafkaConsumer.java:64 of time 1492418250000 ms
17/04/17 14:07:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418230000 ms)
17/04/17 14:07:30 INFO InputInfoTracker: remove old batch metadata: 1492418230000 ms
17/04/17 14:07:40 INFO JobScheduler: Added jobs for time 1492418260000 ms
17/04/17 14:07:40 INFO JobScheduler: Starting job streaming job 1492418260000 ms.0 from job set of time 1492418260000 ms
-------------------------------------------
Time: 1492418260000 ms
-------------------------------------------

17/04/17 14:07:40 INFO JobScheduler: Finished job streaming job 1492418260000 ms.0 from job set of time 1492418260000 ms
17/04/17 14:07:40 INFO MapPartitionsRDD: Removing RDD 372 from persistence list
17/04/17 14:07:40 INFO BlockManager: Removing RDD 372
17/04/17 14:07:40 INFO JobScheduler: Total delay: 0.006 s for time 1492418260000 ms (execution: 0.001 s)
17/04/17 14:07:40 INFO MapPartitionsRDD: Removing RDD 371 from persistence list
17/04/17 14:07:40 INFO BlockManager: Removing RDD 371
17/04/17 14:07:40 INFO BlockRDD: Removing RDD 370 from persistence list
17/04/17 14:07:40 INFO BlockManager: Removing RDD 370
17/04/17 14:07:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[370] at createStream at KafkaConsumer.java:64 of time 1492418260000 ms
17/04/17 14:07:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418240000 ms)
17/04/17 14:07:40 INFO InputInfoTracker: remove old batch metadata: 1492418240000 ms
17/04/17 14:07:50 INFO JobScheduler: Starting job streaming job 1492418270000 ms.0 from job set of time 1492418270000 ms
-------------------------------------------
Time: 1492418270000 ms
-------------------------------------------

17/04/17 14:07:50 INFO JobScheduler: Added jobs for time 1492418270000 ms
17/04/17 14:07:50 INFO JobScheduler: Finished job streaming job 1492418270000 ms.0 from job set of time 1492418270000 ms
17/04/17 14:07:50 INFO MapPartitionsRDD: Removing RDD 375 from persistence list
17/04/17 14:07:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418270000 ms (execution: 0.000 s)
17/04/17 14:07:50 INFO BlockManager: Removing RDD 375
17/04/17 14:07:50 INFO MapPartitionsRDD: Removing RDD 374 from persistence list
17/04/17 14:07:50 INFO BlockManager: Removing RDD 374
17/04/17 14:07:50 INFO BlockRDD: Removing RDD 373 from persistence list
17/04/17 14:07:50 INFO BlockManager: Removing RDD 373
17/04/17 14:07:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[373] at createStream at KafkaConsumer.java:64 of time 1492418270000 ms
17/04/17 14:07:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418250000 ms)
17/04/17 14:07:50 INFO InputInfoTracker: remove old batch metadata: 1492418250000 ms
17/04/17 14:08:00 INFO JobScheduler: Added jobs for time 1492418280000 ms
-------------------------------------------
Time: 1492418280000 ms
-------------------------------------------

17/04/17 14:08:00 INFO JobScheduler: Starting job streaming job 1492418280000 ms.0 from job set of time 1492418280000 ms
17/04/17 14:08:00 INFO JobScheduler: Finished job streaming job 1492418280000 ms.0 from job set of time 1492418280000 ms
17/04/17 14:08:00 INFO MapPartitionsRDD: Removing RDD 378 from persistence list
17/04/17 14:08:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418280000 ms (execution: 0.000 s)
17/04/17 14:08:00 INFO BlockManager: Removing RDD 378
17/04/17 14:08:00 INFO MapPartitionsRDD: Removing RDD 377 from persistence list
17/04/17 14:08:00 INFO BlockManager: Removing RDD 377
17/04/17 14:08:00 INFO BlockRDD: Removing RDD 376 from persistence list
17/04/17 14:08:00 INFO BlockManager: Removing RDD 376
17/04/17 14:08:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[376] at createStream at KafkaConsumer.java:64 of time 1492418280000 ms
17/04/17 14:08:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418260000 ms)
17/04/17 14:08:00 INFO InputInfoTracker: remove old batch metadata: 1492418260000 ms
17/04/17 14:08:10 INFO JobScheduler: Added jobs for time 1492418290000 ms
17/04/17 14:08:10 INFO JobScheduler: Starting job streaming job 1492418290000 ms.0 from job set of time 1492418290000 ms
-------------------------------------------
Time: 1492418290000 ms
-------------------------------------------

17/04/17 14:08:10 INFO JobScheduler: Finished job streaming job 1492418290000 ms.0 from job set of time 1492418290000 ms
17/04/17 14:08:10 INFO MapPartitionsRDD: Removing RDD 381 from persistence list
17/04/17 14:08:10 INFO JobScheduler: Total delay: 0.006 s for time 1492418290000 ms (execution: 0.001 s)
17/04/17 14:08:10 INFO BlockManager: Removing RDD 381
17/04/17 14:08:10 INFO MapPartitionsRDD: Removing RDD 380 from persistence list
17/04/17 14:08:10 INFO BlockManager: Removing RDD 380
17/04/17 14:08:10 INFO BlockRDD: Removing RDD 379 from persistence list
17/04/17 14:08:10 INFO BlockManager: Removing RDD 379
17/04/17 14:08:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[379] at createStream at KafkaConsumer.java:64 of time 1492418290000 ms
17/04/17 14:08:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418270000 ms)
17/04/17 14:08:10 INFO InputInfoTracker: remove old batch metadata: 1492418270000 ms
17/04/17 14:08:20 INFO JobScheduler: Added jobs for time 1492418300000 ms
17/04/17 14:08:20 INFO JobScheduler: Starting job streaming job 1492418300000 ms.0 from job set of time 1492418300000 ms
-------------------------------------------
Time: 1492418300000 ms
-------------------------------------------

17/04/17 14:08:20 INFO JobScheduler: Finished job streaming job 1492418300000 ms.0 from job set of time 1492418300000 ms
17/04/17 14:08:20 INFO MapPartitionsRDD: Removing RDD 384 from persistence list
17/04/17 14:08:20 INFO JobScheduler: Total delay: 0.008 s for time 1492418300000 ms (execution: 0.001 s)
17/04/17 14:08:20 INFO BlockManager: Removing RDD 384
17/04/17 14:08:20 INFO MapPartitionsRDD: Removing RDD 383 from persistence list
17/04/17 14:08:20 INFO BlockManager: Removing RDD 383
17/04/17 14:08:20 INFO BlockRDD: Removing RDD 382 from persistence list
17/04/17 14:08:20 INFO BlockManager: Removing RDD 382
17/04/17 14:08:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[382] at createStream at KafkaConsumer.java:64 of time 1492418300000 ms
17/04/17 14:08:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418280000 ms)
17/04/17 14:08:20 INFO InputInfoTracker: remove old batch metadata: 1492418280000 ms
17/04/17 14:08:30 INFO JobScheduler: Added jobs for time 1492418310000 ms
17/04/17 14:08:30 INFO JobScheduler: Starting job streaming job 1492418310000 ms.0 from job set of time 1492418310000 ms
-------------------------------------------
Time: 1492418310000 ms
-------------------------------------------

17/04/17 14:08:30 INFO JobScheduler: Finished job streaming job 1492418310000 ms.0 from job set of time 1492418310000 ms
17/04/17 14:08:30 INFO MapPartitionsRDD: Removing RDD 387 from persistence list
17/04/17 14:08:30 INFO JobScheduler: Total delay: 0.006 s for time 1492418310000 ms (execution: 0.000 s)
17/04/17 14:08:30 INFO BlockManager: Removing RDD 387
17/04/17 14:08:30 INFO MapPartitionsRDD: Removing RDD 386 from persistence list
17/04/17 14:08:30 INFO BlockManager: Removing RDD 386
17/04/17 14:08:30 INFO BlockRDD: Removing RDD 385 from persistence list
17/04/17 14:08:30 INFO BlockManager: Removing RDD 385
17/04/17 14:08:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[385] at createStream at KafkaConsumer.java:64 of time 1492418310000 ms
17/04/17 14:08:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418290000 ms)
17/04/17 14:08:30 INFO InputInfoTracker: remove old batch metadata: 1492418290000 ms
17/04/17 14:08:40 INFO JobScheduler: Added jobs for time 1492418320000 ms
17/04/17 14:08:40 INFO JobScheduler: Starting job streaming job 1492418320000 ms.0 from job set of time 1492418320000 ms
-------------------------------------------
Time: 1492418320000 ms
-------------------------------------------

17/04/17 14:08:40 INFO JobScheduler: Finished job streaming job 1492418320000 ms.0 from job set of time 1492418320000 ms
17/04/17 14:08:40 INFO JobScheduler: Total delay: 0.004 s for time 1492418320000 ms (execution: 0.000 s)
17/04/17 14:08:40 INFO MapPartitionsRDD: Removing RDD 390 from persistence list
17/04/17 14:08:40 INFO BlockManager: Removing RDD 390
17/04/17 14:08:40 INFO MapPartitionsRDD: Removing RDD 389 from persistence list
17/04/17 14:08:40 INFO BlockManager: Removing RDD 389
17/04/17 14:08:40 INFO BlockRDD: Removing RDD 388 from persistence list
17/04/17 14:08:40 INFO BlockManager: Removing RDD 388
17/04/17 14:08:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[388] at createStream at KafkaConsumer.java:64 of time 1492418320000 ms
17/04/17 14:08:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418300000 ms)
17/04/17 14:08:40 INFO InputInfoTracker: remove old batch metadata: 1492418300000 ms
17/04/17 14:08:50 INFO JobScheduler: Added jobs for time 1492418330000 ms
17/04/17 14:08:50 INFO JobScheduler: Starting job streaming job 1492418330000 ms.0 from job set of time 1492418330000 ms
-------------------------------------------
Time: 1492418330000 ms
-------------------------------------------

17/04/17 14:08:50 INFO JobScheduler: Finished job streaming job 1492418330000 ms.0 from job set of time 1492418330000 ms
17/04/17 14:08:50 INFO MapPartitionsRDD: Removing RDD 393 from persistence list
17/04/17 14:08:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418330000 ms (execution: 0.000 s)
17/04/17 14:08:50 INFO BlockManager: Removing RDD 393
17/04/17 14:08:50 INFO MapPartitionsRDD: Removing RDD 392 from persistence list
17/04/17 14:08:50 INFO BlockManager: Removing RDD 392
17/04/17 14:08:50 INFO BlockRDD: Removing RDD 391 from persistence list
17/04/17 14:08:50 INFO BlockManager: Removing RDD 391
17/04/17 14:08:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[391] at createStream at KafkaConsumer.java:64 of time 1492418330000 ms
17/04/17 14:08:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418310000 ms)
17/04/17 14:08:50 INFO InputInfoTracker: remove old batch metadata: 1492418310000 ms
17/04/17 14:09:00 INFO JobScheduler: Added jobs for time 1492418340000 ms
17/04/17 14:09:00 INFO JobScheduler: Starting job streaming job 1492418340000 ms.0 from job set of time 1492418340000 ms
-------------------------------------------
Time: 1492418340000 ms
-------------------------------------------

17/04/17 14:09:00 INFO JobScheduler: Finished job streaming job 1492418340000 ms.0 from job set of time 1492418340000 ms
17/04/17 14:09:00 INFO MapPartitionsRDD: Removing RDD 396 from persistence list
17/04/17 14:09:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418340000 ms (execution: 0.000 s)
17/04/17 14:09:00 INFO BlockManager: Removing RDD 396
17/04/17 14:09:00 INFO MapPartitionsRDD: Removing RDD 395 from persistence list
17/04/17 14:09:00 INFO BlockManager: Removing RDD 395
17/04/17 14:09:00 INFO BlockRDD: Removing RDD 394 from persistence list
17/04/17 14:09:00 INFO BlockManager: Removing RDD 394
17/04/17 14:09:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[394] at createStream at KafkaConsumer.java:64 of time 1492418340000 ms
17/04/17 14:09:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418320000 ms)
17/04/17 14:09:00 INFO InputInfoTracker: remove old batch metadata: 1492418320000 ms
17/04/17 14:09:10 INFO JobScheduler: Added jobs for time 1492418350000 ms
17/04/17 14:09:10 INFO JobScheduler: Starting job streaming job 1492418350000 ms.0 from job set of time 1492418350000 ms
-------------------------------------------
Time: 1492418350000 ms
-------------------------------------------

17/04/17 14:09:10 INFO JobScheduler: Finished job streaming job 1492418350000 ms.0 from job set of time 1492418350000 ms
17/04/17 14:09:10 INFO MapPartitionsRDD: Removing RDD 399 from persistence list
17/04/17 14:09:10 INFO JobScheduler: Total delay: 0.006 s for time 1492418350000 ms (execution: 0.001 s)
17/04/17 14:09:10 INFO BlockManager: Removing RDD 399
17/04/17 14:09:10 INFO MapPartitionsRDD: Removing RDD 398 from persistence list
17/04/17 14:09:10 INFO BlockManager: Removing RDD 398
17/04/17 14:09:10 INFO BlockRDD: Removing RDD 397 from persistence list
17/04/17 14:09:10 INFO BlockManager: Removing RDD 397
17/04/17 14:09:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[397] at createStream at KafkaConsumer.java:64 of time 1492418350000 ms
17/04/17 14:09:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418330000 ms)
17/04/17 14:09:10 INFO InputInfoTracker: remove old batch metadata: 1492418330000 ms
17/04/17 14:09:20 INFO JobScheduler: Added jobs for time 1492418360000 ms
-------------------------------------------
Time: 1492418360000 ms
-------------------------------------------

17/04/17 14:09:20 INFO JobScheduler: Starting job streaming job 1492418360000 ms.0 from job set of time 1492418360000 ms
17/04/17 14:09:20 INFO JobScheduler: Finished job streaming job 1492418360000 ms.0 from job set of time 1492418360000 ms
17/04/17 14:09:20 INFO MapPartitionsRDD: Removing RDD 402 from persistence list
17/04/17 14:09:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418360000 ms (execution: 0.000 s)
17/04/17 14:09:20 INFO BlockManager: Removing RDD 402
17/04/17 14:09:20 INFO MapPartitionsRDD: Removing RDD 401 from persistence list
17/04/17 14:09:20 INFO BlockManager: Removing RDD 401
17/04/17 14:09:20 INFO BlockRDD: Removing RDD 400 from persistence list
17/04/17 14:09:20 INFO BlockManager: Removing RDD 400
17/04/17 14:09:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[400] at createStream at KafkaConsumer.java:64 of time 1492418360000 ms
17/04/17 14:09:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418340000 ms)
17/04/17 14:09:20 INFO InputInfoTracker: remove old batch metadata: 1492418340000 ms
17/04/17 14:09:30 INFO JobScheduler: Added jobs for time 1492418370000 ms
17/04/17 14:09:30 INFO JobScheduler: Starting job streaming job 1492418370000 ms.0 from job set of time 1492418370000 ms
-------------------------------------------
Time: 1492418370000 ms
-------------------------------------------

17/04/17 14:09:30 INFO JobScheduler: Finished job streaming job 1492418370000 ms.0 from job set of time 1492418370000 ms
17/04/17 14:09:30 INFO JobScheduler: Total delay: 0.003 s for time 1492418370000 ms (execution: 0.001 s)
17/04/17 14:09:30 INFO MapPartitionsRDD: Removing RDD 405 from persistence list
17/04/17 14:09:30 INFO BlockManager: Removing RDD 405
17/04/17 14:09:30 INFO MapPartitionsRDD: Removing RDD 404 from persistence list
17/04/17 14:09:30 INFO BlockManager: Removing RDD 404
17/04/17 14:09:30 INFO BlockRDD: Removing RDD 403 from persistence list
17/04/17 14:09:30 INFO BlockManager: Removing RDD 403
17/04/17 14:09:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[403] at createStream at KafkaConsumer.java:64 of time 1492418370000 ms
17/04/17 14:09:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418350000 ms)
17/04/17 14:09:30 INFO InputInfoTracker: remove old batch metadata: 1492418350000 ms
17/04/17 14:09:40 INFO JobScheduler: Added jobs for time 1492418380000 ms
17/04/17 14:09:40 INFO JobScheduler: Starting job streaming job 1492418380000 ms.0 from job set of time 1492418380000 ms
-------------------------------------------
Time: 1492418380000 ms
-------------------------------------------

17/04/17 14:09:40 INFO JobScheduler: Finished job streaming job 1492418380000 ms.0 from job set of time 1492418380000 ms
17/04/17 14:09:40 INFO MapPartitionsRDD: Removing RDD 408 from persistence list
17/04/17 14:09:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418380000 ms (execution: 0.000 s)
17/04/17 14:09:40 INFO BlockManager: Removing RDD 408
17/04/17 14:09:40 INFO MapPartitionsRDD: Removing RDD 407 from persistence list
17/04/17 14:09:40 INFO BlockManager: Removing RDD 407
17/04/17 14:09:40 INFO BlockRDD: Removing RDD 406 from persistence list
17/04/17 14:09:40 INFO BlockManager: Removing RDD 406
17/04/17 14:09:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[406] at createStream at KafkaConsumer.java:64 of time 1492418380000 ms
17/04/17 14:09:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418360000 ms)
17/04/17 14:09:40 INFO InputInfoTracker: remove old batch metadata: 1492418360000 ms
17/04/17 14:09:50 INFO JobScheduler: Added jobs for time 1492418390000 ms
17/04/17 14:09:50 INFO JobScheduler: Starting job streaming job 1492418390000 ms.0 from job set of time 1492418390000 ms
-------------------------------------------
Time: 1492418390000 ms
-------------------------------------------

17/04/17 14:09:50 INFO JobScheduler: Finished job streaming job 1492418390000 ms.0 from job set of time 1492418390000 ms
17/04/17 14:09:50 INFO MapPartitionsRDD: Removing RDD 411 from persistence list
17/04/17 14:09:50 INFO JobScheduler: Total delay: 0.006 s for time 1492418390000 ms (execution: 0.001 s)
17/04/17 14:09:50 INFO BlockManager: Removing RDD 411
17/04/17 14:09:50 INFO MapPartitionsRDD: Removing RDD 410 from persistence list
17/04/17 14:09:50 INFO BlockManager: Removing RDD 410
17/04/17 14:09:50 INFO BlockRDD: Removing RDD 409 from persistence list
17/04/17 14:09:50 INFO BlockManager: Removing RDD 409
17/04/17 14:09:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[409] at createStream at KafkaConsumer.java:64 of time 1492418390000 ms
17/04/17 14:09:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418370000 ms)
17/04/17 14:09:50 INFO InputInfoTracker: remove old batch metadata: 1492418370000 ms
17/04/17 14:10:00 INFO JobScheduler: Added jobs for time 1492418400000 ms
17/04/17 14:10:00 INFO JobScheduler: Starting job streaming job 1492418400000 ms.0 from job set of time 1492418400000 ms
-------------------------------------------
Time: 1492418400000 ms
-------------------------------------------

17/04/17 14:10:00 INFO JobScheduler: Finished job streaming job 1492418400000 ms.0 from job set of time 1492418400000 ms
17/04/17 14:10:00 INFO MapPartitionsRDD: Removing RDD 414 from persistence list
17/04/17 14:10:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418400000 ms (execution: 0.001 s)
17/04/17 14:10:00 INFO BlockManager: Removing RDD 414
17/04/17 14:10:00 INFO MapPartitionsRDD: Removing RDD 413 from persistence list
17/04/17 14:10:00 INFO BlockManager: Removing RDD 413
17/04/17 14:10:00 INFO BlockRDD: Removing RDD 412 from persistence list
17/04/17 14:10:00 INFO BlockManager: Removing RDD 412
17/04/17 14:10:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[412] at createStream at KafkaConsumer.java:64 of time 1492418400000 ms
17/04/17 14:10:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418380000 ms)
17/04/17 14:10:00 INFO InputInfoTracker: remove old batch metadata: 1492418380000 ms
17/04/17 14:10:10 INFO JobScheduler: Added jobs for time 1492418410000 ms
17/04/17 14:10:10 INFO JobScheduler: Starting job streaming job 1492418410000 ms.0 from job set of time 1492418410000 ms
-------------------------------------------
Time: 1492418410000 ms
-------------------------------------------

17/04/17 14:10:10 INFO JobScheduler: Finished job streaming job 1492418410000 ms.0 from job set of time 1492418410000 ms
17/04/17 14:10:10 INFO MapPartitionsRDD: Removing RDD 417 from persistence list
17/04/17 14:10:10 INFO JobScheduler: Total delay: 0.006 s for time 1492418410000 ms (execution: 0.000 s)
17/04/17 14:10:10 INFO BlockManager: Removing RDD 417
17/04/17 14:10:10 INFO MapPartitionsRDD: Removing RDD 416 from persistence list
17/04/17 14:10:10 INFO BlockManager: Removing RDD 416
17/04/17 14:10:10 INFO BlockRDD: Removing RDD 415 from persistence list
17/04/17 14:10:10 INFO BlockManager: Removing RDD 415
17/04/17 14:10:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[415] at createStream at KafkaConsumer.java:64 of time 1492418410000 ms
17/04/17 14:10:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418390000 ms)
17/04/17 14:10:10 INFO InputInfoTracker: remove old batch metadata: 1492418390000 ms
17/04/17 14:10:20 INFO JobScheduler: Added jobs for time 1492418420000 ms
17/04/17 14:10:20 INFO JobScheduler: Starting job streaming job 1492418420000 ms.0 from job set of time 1492418420000 ms
-------------------------------------------
Time: 1492418420000 ms
-------------------------------------------

17/04/17 14:10:20 INFO JobScheduler: Finished job streaming job 1492418420000 ms.0 from job set of time 1492418420000 ms
17/04/17 14:10:20 INFO MapPartitionsRDD: Removing RDD 420 from persistence list
17/04/17 14:10:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418420000 ms (execution: 0.000 s)
17/04/17 14:10:20 INFO BlockManager: Removing RDD 420
17/04/17 14:10:20 INFO MapPartitionsRDD: Removing RDD 419 from persistence list
17/04/17 14:10:20 INFO BlockManager: Removing RDD 419
17/04/17 14:10:20 INFO BlockRDD: Removing RDD 418 from persistence list
17/04/17 14:10:20 INFO BlockManager: Removing RDD 418
17/04/17 14:10:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[418] at createStream at KafkaConsumer.java:64 of time 1492418420000 ms
17/04/17 14:10:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418400000 ms)
17/04/17 14:10:20 INFO InputInfoTracker: remove old batch metadata: 1492418400000 ms
17/04/17 14:10:30 INFO JobScheduler: Added jobs for time 1492418430000 ms
17/04/17 14:10:30 INFO JobScheduler: Starting job streaming job 1492418430000 ms.0 from job set of time 1492418430000 ms
-------------------------------------------
Time: 1492418430000 ms
-------------------------------------------

17/04/17 14:10:30 INFO JobScheduler: Finished job streaming job 1492418430000 ms.0 from job set of time 1492418430000 ms
17/04/17 14:10:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418430000 ms (execution: 0.001 s)
17/04/17 14:10:30 INFO MapPartitionsRDD: Removing RDD 423 from persistence list
17/04/17 14:10:30 INFO BlockManager: Removing RDD 423
17/04/17 14:10:30 INFO MapPartitionsRDD: Removing RDD 422 from persistence list
17/04/17 14:10:30 INFO BlockManager: Removing RDD 422
17/04/17 14:10:30 INFO BlockRDD: Removing RDD 421 from persistence list
17/04/17 14:10:30 INFO BlockManager: Removing RDD 421
17/04/17 14:10:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[421] at createStream at KafkaConsumer.java:64 of time 1492418430000 ms
17/04/17 14:10:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418410000 ms)
17/04/17 14:10:30 INFO InputInfoTracker: remove old batch metadata: 1492418410000 ms
17/04/17 14:10:40 INFO JobScheduler: Added jobs for time 1492418440000 ms
17/04/17 14:10:40 INFO JobScheduler: Starting job streaming job 1492418440000 ms.0 from job set of time 1492418440000 ms
-------------------------------------------
Time: 1492418440000 ms
-------------------------------------------

17/04/17 14:10:40 INFO JobScheduler: Finished job streaming job 1492418440000 ms.0 from job set of time 1492418440000 ms
17/04/17 14:10:40 INFO MapPartitionsRDD: Removing RDD 426 from persistence list
17/04/17 14:10:40 INFO JobScheduler: Total delay: 0.006 s for time 1492418440000 ms (execution: 0.001 s)
17/04/17 14:10:40 INFO BlockManager: Removing RDD 426
17/04/17 14:10:40 INFO MapPartitionsRDD: Removing RDD 425 from persistence list
17/04/17 14:10:40 INFO BlockManager: Removing RDD 425
17/04/17 14:10:40 INFO BlockRDD: Removing RDD 424 from persistence list
17/04/17 14:10:40 INFO BlockManager: Removing RDD 424
17/04/17 14:10:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[424] at createStream at KafkaConsumer.java:64 of time 1492418440000 ms
17/04/17 14:10:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418420000 ms)
17/04/17 14:10:40 INFO InputInfoTracker: remove old batch metadata: 1492418420000 ms
17/04/17 14:10:50 INFO JobScheduler: Added jobs for time 1492418450000 ms
17/04/17 14:10:50 INFO JobScheduler: Starting job streaming job 1492418450000 ms.0 from job set of time 1492418450000 ms
-------------------------------------------
Time: 1492418450000 ms
-------------------------------------------

17/04/17 14:10:50 INFO JobScheduler: Finished job streaming job 1492418450000 ms.0 from job set of time 1492418450000 ms
17/04/17 14:10:50 INFO MapPartitionsRDD: Removing RDD 429 from persistence list
17/04/17 14:10:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418450000 ms (execution: 0.000 s)
17/04/17 14:10:50 INFO BlockManager: Removing RDD 429
17/04/17 14:10:50 INFO MapPartitionsRDD: Removing RDD 428 from persistence list
17/04/17 14:10:50 INFO BlockManager: Removing RDD 428
17/04/17 14:10:50 INFO BlockRDD: Removing RDD 427 from persistence list
17/04/17 14:10:50 INFO BlockManager: Removing RDD 427
17/04/17 14:10:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[427] at createStream at KafkaConsumer.java:64 of time 1492418450000 ms
17/04/17 14:10:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418430000 ms)
17/04/17 14:10:50 INFO InputInfoTracker: remove old batch metadata: 1492418430000 ms
17/04/17 14:11:00 INFO JobScheduler: Added jobs for time 1492418460000 ms
17/04/17 14:11:00 INFO JobScheduler: Starting job streaming job 1492418460000 ms.0 from job set of time 1492418460000 ms
-------------------------------------------
Time: 1492418460000 ms
-------------------------------------------

17/04/17 14:11:00 INFO JobScheduler: Finished job streaming job 1492418460000 ms.0 from job set of time 1492418460000 ms
17/04/17 14:11:00 INFO MapPartitionsRDD: Removing RDD 432 from persistence list
17/04/17 14:11:00 INFO JobScheduler: Total delay: 0.006 s for time 1492418460000 ms (execution: 0.000 s)
17/04/17 14:11:00 INFO BlockManager: Removing RDD 432
17/04/17 14:11:00 INFO MapPartitionsRDD: Removing RDD 431 from persistence list
17/04/17 14:11:00 INFO BlockRDD: Removing RDD 430 from persistence list
17/04/17 14:11:00 INFO BlockManager: Removing RDD 431
17/04/17 14:11:00 INFO BlockManager: Removing RDD 430
17/04/17 14:11:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[430] at createStream at KafkaConsumer.java:64 of time 1492418460000 ms
17/04/17 14:11:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418440000 ms)
17/04/17 14:11:00 INFO InputInfoTracker: remove old batch metadata: 1492418440000 ms
17/04/17 14:11:10 INFO JobScheduler: Added jobs for time 1492418470000 ms
17/04/17 14:11:10 INFO JobScheduler: Starting job streaming job 1492418470000 ms.0 from job set of time 1492418470000 ms
-------------------------------------------
Time: 1492418470000 ms
-------------------------------------------

17/04/17 14:11:10 INFO JobScheduler: Finished job streaming job 1492418470000 ms.0 from job set of time 1492418470000 ms
17/04/17 14:11:10 INFO MapPartitionsRDD: Removing RDD 435 from persistence list
17/04/17 14:11:10 INFO JobScheduler: Total delay: 0.006 s for time 1492418470000 ms (execution: 0.001 s)
17/04/17 14:11:10 INFO BlockManager: Removing RDD 435
17/04/17 14:11:10 INFO MapPartitionsRDD: Removing RDD 434 from persistence list
17/04/17 14:11:10 INFO BlockManager: Removing RDD 434
17/04/17 14:11:10 INFO BlockRDD: Removing RDD 433 from persistence list
17/04/17 14:11:10 INFO BlockManager: Removing RDD 433
17/04/17 14:11:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[433] at createStream at KafkaConsumer.java:64 of time 1492418470000 ms
17/04/17 14:11:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418450000 ms)
17/04/17 14:11:10 INFO InputInfoTracker: remove old batch metadata: 1492418450000 ms
17/04/17 14:11:20 INFO JobScheduler: Added jobs for time 1492418480000 ms
-------------------------------------------
Time: 1492418480000 ms
-------------------------------------------

17/04/17 14:11:20 INFO JobScheduler: Starting job streaming job 1492418480000 ms.0 from job set of time 1492418480000 ms
17/04/17 14:11:20 INFO JobScheduler: Finished job streaming job 1492418480000 ms.0 from job set of time 1492418480000 ms
17/04/17 14:11:20 INFO MapPartitionsRDD: Removing RDD 438 from persistence list
17/04/17 14:11:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418480000 ms (execution: 0.000 s)
17/04/17 14:11:20 INFO MapPartitionsRDD: Removing RDD 437 from persistence list
17/04/17 14:11:20 INFO BlockManager: Removing RDD 438
17/04/17 14:11:20 INFO BlockRDD: Removing RDD 436 from persistence list
17/04/17 14:11:20 INFO BlockManager: Removing RDD 437
17/04/17 14:11:20 INFO BlockManager: Removing RDD 436
17/04/17 14:11:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[436] at createStream at KafkaConsumer.java:64 of time 1492418480000 ms
17/04/17 14:11:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418460000 ms)
17/04/17 14:11:20 INFO InputInfoTracker: remove old batch metadata: 1492418460000 ms
17/04/17 14:11:30 INFO JobScheduler: Added jobs for time 1492418490000 ms
17/04/17 14:11:30 INFO JobScheduler: Starting job streaming job 1492418490000 ms.0 from job set of time 1492418490000 ms
-------------------------------------------
Time: 1492418490000 ms
-------------------------------------------

17/04/17 14:11:30 INFO JobScheduler: Finished job streaming job 1492418490000 ms.0 from job set of time 1492418490000 ms
17/04/17 14:11:30 INFO MapPartitionsRDD: Removing RDD 441 from persistence list
17/04/17 14:11:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418490000 ms (execution: 0.000 s)
17/04/17 14:11:30 INFO BlockManager: Removing RDD 441
17/04/17 14:11:30 INFO MapPartitionsRDD: Removing RDD 440 from persistence list
17/04/17 14:11:30 INFO BlockManager: Removing RDD 440
17/04/17 14:11:30 INFO BlockRDD: Removing RDD 439 from persistence list
17/04/17 14:11:30 INFO BlockManager: Removing RDD 439
17/04/17 14:11:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[439] at createStream at KafkaConsumer.java:64 of time 1492418490000 ms
17/04/17 14:11:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418470000 ms)
17/04/17 14:11:30 INFO InputInfoTracker: remove old batch metadata: 1492418470000 ms
17/04/17 14:11:40 INFO JobScheduler: Added jobs for time 1492418500000 ms
17/04/17 14:11:40 INFO JobScheduler: Starting job streaming job 1492418500000 ms.0 from job set of time 1492418500000 ms
-------------------------------------------
Time: 1492418500000 ms
-------------------------------------------

17/04/17 14:11:40 INFO JobScheduler: Finished job streaming job 1492418500000 ms.0 from job set of time 1492418500000 ms
17/04/17 14:11:40 INFO MapPartitionsRDD: Removing RDD 444 from persistence list
17/04/17 14:11:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418500000 ms (execution: 0.000 s)
17/04/17 14:11:40 INFO BlockManager: Removing RDD 444
17/04/17 14:11:40 INFO MapPartitionsRDD: Removing RDD 443 from persistence list
17/04/17 14:11:40 INFO BlockManager: Removing RDD 443
17/04/17 14:11:40 INFO BlockRDD: Removing RDD 442 from persistence list
17/04/17 14:11:40 INFO BlockManager: Removing RDD 442
17/04/17 14:11:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[442] at createStream at KafkaConsumer.java:64 of time 1492418500000 ms
17/04/17 14:11:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418480000 ms)
17/04/17 14:11:40 INFO InputInfoTracker: remove old batch metadata: 1492418480000 ms
17/04/17 14:11:50 INFO JobScheduler: Added jobs for time 1492418510000 ms
17/04/17 14:11:50 INFO JobScheduler: Starting job streaming job 1492418510000 ms.0 from job set of time 1492418510000 ms
-------------------------------------------
Time: 1492418510000 ms
-------------------------------------------

17/04/17 14:11:50 INFO JobScheduler: Finished job streaming job 1492418510000 ms.0 from job set of time 1492418510000 ms
17/04/17 14:11:50 INFO MapPartitionsRDD: Removing RDD 447 from persistence list
17/04/17 14:11:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418510000 ms (execution: 0.000 s)
17/04/17 14:11:50 INFO BlockManager: Removing RDD 447
17/04/17 14:11:50 INFO MapPartitionsRDD: Removing RDD 446 from persistence list
17/04/17 14:11:50 INFO BlockManager: Removing RDD 446
17/04/17 14:11:50 INFO BlockRDD: Removing RDD 445 from persistence list
17/04/17 14:11:50 INFO BlockManager: Removing RDD 445
17/04/17 14:11:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[445] at createStream at KafkaConsumer.java:64 of time 1492418510000 ms
17/04/17 14:11:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418490000 ms)
17/04/17 14:11:50 INFO InputInfoTracker: remove old batch metadata: 1492418490000 ms
17/04/17 14:12:00 INFO JobScheduler: Added jobs for time 1492418520000 ms
17/04/17 14:12:00 INFO JobScheduler: Starting job streaming job 1492418520000 ms.0 from job set of time 1492418520000 ms
-------------------------------------------
Time: 1492418520000 ms
-------------------------------------------

17/04/17 14:12:00 INFO JobScheduler: Finished job streaming job 1492418520000 ms.0 from job set of time 1492418520000 ms
17/04/17 14:12:00 INFO MapPartitionsRDD: Removing RDD 450 from persistence list
17/04/17 14:12:00 INFO JobScheduler: Total delay: 0.006 s for time 1492418520000 ms (execution: 0.001 s)
17/04/17 14:12:00 INFO BlockManager: Removing RDD 450
17/04/17 14:12:00 INFO MapPartitionsRDD: Removing RDD 449 from persistence list
17/04/17 14:12:00 INFO BlockManager: Removing RDD 449
17/04/17 14:12:00 INFO BlockRDD: Removing RDD 448 from persistence list
17/04/17 14:12:00 INFO BlockManager: Removing RDD 448
17/04/17 14:12:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[448] at createStream at KafkaConsumer.java:64 of time 1492418520000 ms
17/04/17 14:12:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418500000 ms)
17/04/17 14:12:00 INFO InputInfoTracker: remove old batch metadata: 1492418500000 ms
17/04/17 14:12:10 INFO JobScheduler: Added jobs for time 1492418530000 ms
17/04/17 14:12:10 INFO JobScheduler: Starting job streaming job 1492418530000 ms.0 from job set of time 1492418530000 ms
-------------------------------------------
Time: 1492418530000 ms
-------------------------------------------

17/04/17 14:12:10 INFO JobScheduler: Finished job streaming job 1492418530000 ms.0 from job set of time 1492418530000 ms
17/04/17 14:12:10 INFO MapPartitionsRDD: Removing RDD 453 from persistence list
17/04/17 14:12:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418530000 ms (execution: 0.000 s)
17/04/17 14:12:10 INFO BlockManager: Removing RDD 453
17/04/17 14:12:10 INFO MapPartitionsRDD: Removing RDD 452 from persistence list
17/04/17 14:12:10 INFO BlockManager: Removing RDD 452
17/04/17 14:12:10 INFO BlockRDD: Removing RDD 451 from persistence list
17/04/17 14:12:10 INFO BlockManager: Removing RDD 451
17/04/17 14:12:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[451] at createStream at KafkaConsumer.java:64 of time 1492418530000 ms
17/04/17 14:12:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418510000 ms)
17/04/17 14:12:10 INFO InputInfoTracker: remove old batch metadata: 1492418510000 ms
17/04/17 14:12:20 INFO JobScheduler: Added jobs for time 1492418540000 ms
17/04/17 14:12:20 INFO JobScheduler: Starting job streaming job 1492418540000 ms.0 from job set of time 1492418540000 ms
-------------------------------------------
Time: 1492418540000 ms
-------------------------------------------

17/04/17 14:12:20 INFO JobScheduler: Finished job streaming job 1492418540000 ms.0 from job set of time 1492418540000 ms
17/04/17 14:12:20 INFO MapPartitionsRDD: Removing RDD 456 from persistence list
17/04/17 14:12:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418540000 ms (execution: 0.000 s)
17/04/17 14:12:20 INFO BlockManager: Removing RDD 456
17/04/17 14:12:20 INFO MapPartitionsRDD: Removing RDD 455 from persistence list
17/04/17 14:12:20 INFO BlockManager: Removing RDD 455
17/04/17 14:12:20 INFO BlockRDD: Removing RDD 454 from persistence list
17/04/17 14:12:20 INFO BlockManager: Removing RDD 454
17/04/17 14:12:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[454] at createStream at KafkaConsumer.java:64 of time 1492418540000 ms
17/04/17 14:12:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418520000 ms)
17/04/17 14:12:20 INFO InputInfoTracker: remove old batch metadata: 1492418520000 ms
17/04/17 14:12:30 INFO JobScheduler: Added jobs for time 1492418550000 ms
17/04/17 14:12:30 INFO JobScheduler: Starting job streaming job 1492418550000 ms.0 from job set of time 1492418550000 ms
-------------------------------------------
Time: 1492418550000 ms
-------------------------------------------

17/04/17 14:12:30 INFO JobScheduler: Finished job streaming job 1492418550000 ms.0 from job set of time 1492418550000 ms
17/04/17 14:12:30 INFO MapPartitionsRDD: Removing RDD 459 from persistence list
17/04/17 14:12:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418550000 ms (execution: 0.001 s)
17/04/17 14:12:30 INFO BlockManager: Removing RDD 459
17/04/17 14:12:30 INFO MapPartitionsRDD: Removing RDD 458 from persistence list
17/04/17 14:12:30 INFO BlockManager: Removing RDD 458
17/04/17 14:12:30 INFO BlockRDD: Removing RDD 457 from persistence list
17/04/17 14:12:30 INFO BlockManager: Removing RDD 457
17/04/17 14:12:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[457] at createStream at KafkaConsumer.java:64 of time 1492418550000 ms
17/04/17 14:12:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418530000 ms)
17/04/17 14:12:30 INFO InputInfoTracker: remove old batch metadata: 1492418530000 ms
17/04/17 14:12:40 INFO JobScheduler: Added jobs for time 1492418560000 ms
17/04/17 14:12:40 INFO JobScheduler: Starting job streaming job 1492418560000 ms.0 from job set of time 1492418560000 ms
-------------------------------------------
Time: 1492418560000 ms
-------------------------------------------

17/04/17 14:12:40 INFO JobScheduler: Finished job streaming job 1492418560000 ms.0 from job set of time 1492418560000 ms
17/04/17 14:12:40 INFO JobScheduler: Total delay: 0.007 s for time 1492418560000 ms (execution: 0.001 s)
17/04/17 14:12:40 INFO MapPartitionsRDD: Removing RDD 462 from persistence list
17/04/17 14:12:40 INFO BlockManager: Removing RDD 462
17/04/17 14:12:40 INFO MapPartitionsRDD: Removing RDD 461 from persistence list
17/04/17 14:12:40 INFO BlockManager: Removing RDD 461
17/04/17 14:12:40 INFO BlockRDD: Removing RDD 460 from persistence list
17/04/17 14:12:40 INFO BlockManager: Removing RDD 460
17/04/17 14:12:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[460] at createStream at KafkaConsumer.java:64 of time 1492418560000 ms
17/04/17 14:12:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418540000 ms)
17/04/17 14:12:40 INFO InputInfoTracker: remove old batch metadata: 1492418540000 ms
17/04/17 14:12:50 INFO JobScheduler: Added jobs for time 1492418570000 ms
17/04/17 14:12:50 INFO JobScheduler: Starting job streaming job 1492418570000 ms.0 from job set of time 1492418570000 ms
-------------------------------------------
Time: 1492418570000 ms
-------------------------------------------

17/04/17 14:12:50 INFO JobScheduler: Finished job streaming job 1492418570000 ms.0 from job set of time 1492418570000 ms
17/04/17 14:12:50 INFO MapPartitionsRDD: Removing RDD 465 from persistence list
17/04/17 14:12:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418570000 ms (execution: 0.000 s)
17/04/17 14:12:50 INFO BlockManager: Removing RDD 465
17/04/17 14:12:50 INFO MapPartitionsRDD: Removing RDD 464 from persistence list
17/04/17 14:12:50 INFO BlockManager: Removing RDD 464
17/04/17 14:12:50 INFO BlockRDD: Removing RDD 463 from persistence list
17/04/17 14:12:50 INFO BlockManager: Removing RDD 463
17/04/17 14:12:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[463] at createStream at KafkaConsumer.java:64 of time 1492418570000 ms
17/04/17 14:12:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418550000 ms)
17/04/17 14:12:50 INFO InputInfoTracker: remove old batch metadata: 1492418550000 ms
17/04/17 14:13:00 INFO JobScheduler: Added jobs for time 1492418580000 ms
17/04/17 14:13:00 INFO JobScheduler: Starting job streaming job 1492418580000 ms.0 from job set of time 1492418580000 ms
-------------------------------------------
Time: 1492418580000 ms
-------------------------------------------

17/04/17 14:13:00 INFO JobScheduler: Finished job streaming job 1492418580000 ms.0 from job set of time 1492418580000 ms
17/04/17 14:13:00 INFO MapPartitionsRDD: Removing RDD 468 from persistence list
17/04/17 14:13:00 INFO JobScheduler: Total delay: 0.006 s for time 1492418580000 ms (execution: 0.001 s)
17/04/17 14:13:00 INFO BlockManager: Removing RDD 468
17/04/17 14:13:00 INFO MapPartitionsRDD: Removing RDD 467 from persistence list
17/04/17 14:13:00 INFO BlockManager: Removing RDD 467
17/04/17 14:13:00 INFO BlockRDD: Removing RDD 466 from persistence list
17/04/17 14:13:00 INFO BlockManager: Removing RDD 466
17/04/17 14:13:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[466] at createStream at KafkaConsumer.java:64 of time 1492418580000 ms
17/04/17 14:13:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418560000 ms)
17/04/17 14:13:00 INFO InputInfoTracker: remove old batch metadata: 1492418560000 ms
17/04/17 14:13:10 INFO JobScheduler: Added jobs for time 1492418590000 ms
17/04/17 14:13:10 INFO JobScheduler: Starting job streaming job 1492418590000 ms.0 from job set of time 1492418590000 ms
-------------------------------------------
Time: 1492418590000 ms
-------------------------------------------

17/04/17 14:13:10 INFO JobScheduler: Finished job streaming job 1492418590000 ms.0 from job set of time 1492418590000 ms
17/04/17 14:13:10 INFO MapPartitionsRDD: Removing RDD 471 from persistence list
17/04/17 14:13:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418590000 ms (execution: 0.000 s)
17/04/17 14:13:10 INFO BlockManager: Removing RDD 471
17/04/17 14:13:10 INFO MapPartitionsRDD: Removing RDD 470 from persistence list
17/04/17 14:13:10 INFO BlockManager: Removing RDD 470
17/04/17 14:13:10 INFO BlockRDD: Removing RDD 469 from persistence list
17/04/17 14:13:10 INFO BlockManager: Removing RDD 469
17/04/17 14:13:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[469] at createStream at KafkaConsumer.java:64 of time 1492418590000 ms
17/04/17 14:13:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418570000 ms)
17/04/17 14:13:10 INFO InputInfoTracker: remove old batch metadata: 1492418570000 ms
17/04/17 14:13:20 INFO JobScheduler: Starting job streaming job 1492418600000 ms.0 from job set of time 1492418600000 ms
-------------------------------------------
Time: 1492418600000 ms
-------------------------------------------

17/04/17 14:13:20 INFO JobScheduler: Added jobs for time 1492418600000 ms
17/04/17 14:13:20 INFO JobScheduler: Finished job streaming job 1492418600000 ms.0 from job set of time 1492418600000 ms
17/04/17 14:13:20 INFO MapPartitionsRDD: Removing RDD 474 from persistence list
17/04/17 14:13:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418600000 ms (execution: 0.001 s)
17/04/17 14:13:20 INFO BlockManager: Removing RDD 474
17/04/17 14:13:20 INFO MapPartitionsRDD: Removing RDD 473 from persistence list
17/04/17 14:13:20 INFO BlockManager: Removing RDD 473
17/04/17 14:13:20 INFO BlockRDD: Removing RDD 472 from persistence list
17/04/17 14:13:20 INFO BlockManager: Removing RDD 472
17/04/17 14:13:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[472] at createStream at KafkaConsumer.java:64 of time 1492418600000 ms
17/04/17 14:13:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418580000 ms)
17/04/17 14:13:20 INFO InputInfoTracker: remove old batch metadata: 1492418580000 ms
17/04/17 14:13:30 INFO JobScheduler: Added jobs for time 1492418610000 ms
17/04/17 14:13:30 INFO JobScheduler: Starting job streaming job 1492418610000 ms.0 from job set of time 1492418610000 ms
-------------------------------------------
Time: 1492418610000 ms
-------------------------------------------

17/04/17 14:13:30 INFO JobScheduler: Finished job streaming job 1492418610000 ms.0 from job set of time 1492418610000 ms
17/04/17 14:13:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418610000 ms (execution: 0.000 s)
17/04/17 14:13:30 INFO MapPartitionsRDD: Removing RDD 477 from persistence list
17/04/17 14:13:30 INFO BlockManager: Removing RDD 477
17/04/17 14:13:30 INFO MapPartitionsRDD: Removing RDD 476 from persistence list
17/04/17 14:13:30 INFO BlockManager: Removing RDD 476
17/04/17 14:13:30 INFO BlockRDD: Removing RDD 475 from persistence list
17/04/17 14:13:30 INFO BlockManager: Removing RDD 475
17/04/17 14:13:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[475] at createStream at KafkaConsumer.java:64 of time 1492418610000 ms
17/04/17 14:13:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418590000 ms)
17/04/17 14:13:30 INFO InputInfoTracker: remove old batch metadata: 1492418590000 ms
17/04/17 14:13:40 INFO JobScheduler: Added jobs for time 1492418620000 ms
17/04/17 14:13:40 INFO JobScheduler: Starting job streaming job 1492418620000 ms.0 from job set of time 1492418620000 ms
-------------------------------------------
Time: 1492418620000 ms
-------------------------------------------

17/04/17 14:13:40 INFO JobScheduler: Finished job streaming job 1492418620000 ms.0 from job set of time 1492418620000 ms
17/04/17 14:13:40 INFO MapPartitionsRDD: Removing RDD 480 from persistence list
17/04/17 14:13:40 INFO JobScheduler: Total delay: 0.006 s for time 1492418620000 ms (execution: 0.001 s)
17/04/17 14:13:40 INFO MapPartitionsRDD: Removing RDD 479 from persistence list
17/04/17 14:13:40 INFO BlockManager: Removing RDD 480
17/04/17 14:13:40 INFO BlockManager: Removing RDD 479
17/04/17 14:13:40 INFO BlockRDD: Removing RDD 478 from persistence list
17/04/17 14:13:40 INFO BlockManager: Removing RDD 478
17/04/17 14:13:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[478] at createStream at KafkaConsumer.java:64 of time 1492418620000 ms
17/04/17 14:13:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418600000 ms)
17/04/17 14:13:40 INFO InputInfoTracker: remove old batch metadata: 1492418600000 ms
17/04/17 14:13:50 INFO JobScheduler: Added jobs for time 1492418630000 ms
-------------------------------------------
Time: 1492418630000 ms
-------------------------------------------

17/04/17 14:13:50 INFO JobScheduler: Starting job streaming job 1492418630000 ms.0 from job set of time 1492418630000 ms
17/04/17 14:13:50 INFO JobScheduler: Finished job streaming job 1492418630000 ms.0 from job set of time 1492418630000 ms
17/04/17 14:13:50 INFO JobScheduler: Total delay: 0.006 s for time 1492418630000 ms (execution: 0.000 s)
17/04/17 14:13:50 INFO MapPartitionsRDD: Removing RDD 483 from persistence list
17/04/17 14:13:50 INFO BlockManager: Removing RDD 483
17/04/17 14:13:50 INFO MapPartitionsRDD: Removing RDD 482 from persistence list
17/04/17 14:13:50 INFO BlockManager: Removing RDD 482
17/04/17 14:13:50 INFO BlockRDD: Removing RDD 481 from persistence list
17/04/17 14:13:50 INFO BlockManager: Removing RDD 481
17/04/17 14:13:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[481] at createStream at KafkaConsumer.java:64 of time 1492418630000 ms
17/04/17 14:13:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418610000 ms)
17/04/17 14:13:50 INFO InputInfoTracker: remove old batch metadata: 1492418610000 ms
17/04/17 14:13:51 WARN TransportChannelHandler: Exception in connection from carl-PC/192.168.0.2:47576
java.io.IOException: Invalid argument
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
17/04/17 14:13:51 WARN TransportChannelHandler: Exception in connection from carl-PC/192.168.0.2:47578
java.io.IOException: Invalid argument
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8008981892646521441 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 6156231185590796967 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8008981892646521441 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 6156231185590796967 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8501542978503480606 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 6112532340005605802 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8501542978503480606 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 6112532340005605802 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 9084142263114226062 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 5719974121632698724 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 9084142263114226062 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 5719974121632698724 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8917030282561707135 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 7291982363312392392 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8917030282561707135 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 7291982363312392392 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 7010819089094764026 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 7024947965972567865 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 7010819089094764026 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 7024947965972567865 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 5920295670542318455 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 5883650378730350404 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 5920295670542318455 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 5883650378730350404 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 9026205724005079914 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 5024189237418391196 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 9026205724005079914 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8646523058974063153 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 5024189237418391196 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8646523058974063153 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8085587419668485870 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8961563618102777806 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8085587419668485870 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8961563618102777806 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8160941082527725521 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8821475943089977019 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8160941082527725521 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8821475943089977019 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 9153438536072215512 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 7532346854017808000 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 9153438536072215512 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 7532346854017808000 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8950001086804935324 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 6780870870000763681 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8950001086804935324 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 6780870870000763681 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 8903763645626998574 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 7826748275473806391 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 8903763645626998574 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 7826748275473806391 to carl-PC/192.168.0.2:47576: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TransportClient: Failed to send RPC 5548686720595858765 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
17/04/17 14:13:51 ERROR TaskSchedulerImpl: Lost executor 2 on carl-PC: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/17 14:13:51 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC 5548686720595858765 to carl-PC/192.168.0.2:47578: java.nio.channels.ClosedChannelException
17/04/17 14:13:51 INFO DAGScheduler: Executor lost: 2 (epoch 7)
17/04/17 14:13:51 ERROR TaskSchedulerImpl: Lost executor 1 on carl-PC: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/17 14:13:51 WARN TaskSetManager: Lost task 0.3 in stage 2.0 (TID 73, carl-PC): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/04/17 14:13:51 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
17/04/17 14:13:51 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor
17/04/17 14:13:51 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
17/04/17 14:13:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/04/17 14:13:51 INFO TaskSchedulerImpl: Cancelling stage 2
17/04/17 14:13:51 INFO DAGScheduler: ResultStage 2 (start at KafkaConsumer.java:71) failed in 1610.169 s
17/04/17 14:13:51 INFO DAGScheduler: Executor lost: 1 (epoch 8)
17/04/17 14:13:51 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/04/17 14:13:51 ERROR ReceiverTracker: Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 73, carl-PC): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
17/04/17 14:13:51 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
17/04/17 14:13:51 INFO ReceiverTracker: Restarting Receiver 0
17/04/17 14:13:51 INFO ReceiverTracker: Receiver 0 started
17/04/17 14:13:51 INFO DAGScheduler: Got job 2 (start at KafkaConsumer.java:71) with 1 output partitions
17/04/17 14:13:51 INFO DAGScheduler: Final stage: ResultStage 3 (start at KafkaConsumer.java:71)
17/04/17 14:13:51 INFO DAGScheduler: Parents of final stage: List()
17/04/17 14:13:51 INFO DAGScheduler: Missing parents: List()
17/04/17 14:13:51 INFO DAGScheduler: Submitting ResultStage 3 (Receiver 0 ParallelCollectionRDD[487] at makeRDD at ReceiverTracker.scala:585), which has no missing parents
17/04/17 14:13:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 45.1 KB, free 511.0 MB)
17/04/17 14:13:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.6 KB, free 511.0 MB)
17/04/17 14:13:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.1:35386 (size: 14.6 KB, free: 511.1 MB)
17/04/17 14:13:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/04/17 14:13:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (Receiver 0 ParallelCollectionRDD[487] at makeRDD at ReceiverTracker.scala:585)
17/04/17 14:13:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/04/17 14:14:00 INFO JobScheduler: Added jobs for time 1492418640000 ms
17/04/17 14:14:00 INFO JobScheduler: Starting job streaming job 1492418640000 ms.0 from job set of time 1492418640000 ms
-------------------------------------------
Time: 1492418640000 ms
-------------------------------------------

17/04/17 14:14:00 INFO JobScheduler: Finished job streaming job 1492418640000 ms.0 from job set of time 1492418640000 ms
17/04/17 14:14:00 INFO MapPartitionsRDD: Removing RDD 486 from persistence list
17/04/17 14:14:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418640000 ms (execution: 0.001 s)
17/04/17 14:14:00 INFO BlockManager: Removing RDD 486
17/04/17 14:14:00 INFO MapPartitionsRDD: Removing RDD 485 from persistence list
17/04/17 14:14:00 INFO BlockManager: Removing RDD 485
17/04/17 14:14:00 INFO BlockRDD: Removing RDD 484 from persistence list
17/04/17 14:14:00 INFO BlockManager: Removing RDD 484
17/04/17 14:14:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[484] at createStream at KafkaConsumer.java:64 of time 1492418640000 ms
17/04/17 14:14:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418620000 ms)
17/04/17 14:14:00 INFO InputInfoTracker: remove old batch metadata: 1492418620000 ms
17/04/17 14:14:10 INFO JobScheduler: Added jobs for time 1492418650000 ms
17/04/17 14:14:10 INFO JobScheduler: Starting job streaming job 1492418650000 ms.0 from job set of time 1492418650000 ms
-------------------------------------------
Time: 1492418650000 ms
-------------------------------------------

17/04/17 14:14:10 INFO JobScheduler: Finished job streaming job 1492418650000 ms.0 from job set of time 1492418650000 ms
17/04/17 14:14:10 INFO MapPartitionsRDD: Removing RDD 490 from persistence list
17/04/17 14:14:10 INFO JobScheduler: Total delay: 0.007 s for time 1492418650000 ms (execution: 0.001 s)
17/04/17 14:14:10 INFO BlockManager: Removing RDD 490
17/04/17 14:14:10 INFO MapPartitionsRDD: Removing RDD 489 from persistence list
17/04/17 14:14:10 INFO BlockManager: Removing RDD 489
17/04/17 14:14:10 INFO BlockRDD: Removing RDD 488 from persistence list
17/04/17 14:14:10 INFO BlockManager: Removing RDD 488
17/04/17 14:14:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[488] at createStream at KafkaConsumer.java:64 of time 1492418650000 ms
17/04/17 14:14:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418630000 ms)
17/04/17 14:14:10 INFO InputInfoTracker: remove old batch metadata: 1492418630000 ms
17/04/17 14:14:20 INFO JobScheduler: Added jobs for time 1492418660000 ms
17/04/17 14:14:20 INFO JobScheduler: Starting job streaming job 1492418660000 ms.0 from job set of time 1492418660000 ms
-------------------------------------------
Time: 1492418660000 ms
-------------------------------------------

17/04/17 14:14:20 INFO JobScheduler: Finished job streaming job 1492418660000 ms.0 from job set of time 1492418660000 ms
17/04/17 14:14:20 INFO MapPartitionsRDD: Removing RDD 493 from persistence list
17/04/17 14:14:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418660000 ms (execution: 0.001 s)
17/04/17 14:14:20 INFO BlockManager: Removing RDD 493
17/04/17 14:14:20 INFO MapPartitionsRDD: Removing RDD 492 from persistence list
17/04/17 14:14:20 INFO BlockManager: Removing RDD 492
17/04/17 14:14:20 INFO BlockRDD: Removing RDD 491 from persistence list
17/04/17 14:14:20 INFO BlockManager: Removing RDD 491
17/04/17 14:14:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[491] at createStream at KafkaConsumer.java:64 of time 1492418660000 ms
17/04/17 14:14:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418640000 ms)
17/04/17 14:14:20 INFO InputInfoTracker: remove old batch metadata: 1492418640000 ms
17/04/17 14:14:30 INFO JobScheduler: Added jobs for time 1492418670000 ms
17/04/17 14:14:30 INFO JobScheduler: Starting job streaming job 1492418670000 ms.0 from job set of time 1492418670000 ms
-------------------------------------------
Time: 1492418670000 ms
-------------------------------------------

17/04/17 14:14:30 INFO JobScheduler: Finished job streaming job 1492418670000 ms.0 from job set of time 1492418670000 ms
17/04/17 14:14:30 INFO MapPartitionsRDD: Removing RDD 496 from persistence list
17/04/17 14:14:30 INFO JobScheduler: Total delay: 0.006 s for time 1492418670000 ms (execution: 0.001 s)
17/04/17 14:14:30 INFO MapPartitionsRDD: Removing RDD 495 from persistence list
17/04/17 14:14:30 INFO BlockManager: Removing RDD 496
17/04/17 14:14:30 INFO BlockManager: Removing RDD 495
17/04/17 14:14:30 INFO BlockRDD: Removing RDD 494 from persistence list
17/04/17 14:14:30 INFO BlockManager: Removing RDD 494
17/04/17 14:14:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[494] at createStream at KafkaConsumer.java:64 of time 1492418670000 ms
17/04/17 14:14:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418650000 ms)
17/04/17 14:14:30 INFO InputInfoTracker: remove old batch metadata: 1492418650000 ms
17/04/17 14:14:40 INFO JobScheduler: Added jobs for time 1492418680000 ms
17/04/17 14:14:40 INFO JobScheduler: Starting job streaming job 1492418680000 ms.0 from job set of time 1492418680000 ms
-------------------------------------------
Time: 1492418680000 ms
-------------------------------------------

17/04/17 14:14:40 INFO JobScheduler: Finished job streaming job 1492418680000 ms.0 from job set of time 1492418680000 ms
17/04/17 14:14:40 INFO MapPartitionsRDD: Removing RDD 499 from persistence list
17/04/17 14:14:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418680000 ms (execution: 0.000 s)
17/04/17 14:14:40 INFO BlockManager: Removing RDD 499
17/04/17 14:14:40 INFO MapPartitionsRDD: Removing RDD 498 from persistence list
17/04/17 14:14:40 INFO BlockManager: Removing RDD 498
17/04/17 14:14:40 INFO BlockRDD: Removing RDD 497 from persistence list
17/04/17 14:14:40 INFO BlockManager: Removing RDD 497
17/04/17 14:14:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[497] at createStream at KafkaConsumer.java:64 of time 1492418680000 ms
17/04/17 14:14:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418660000 ms)
17/04/17 14:14:40 INFO InputInfoTracker: remove old batch metadata: 1492418660000 ms
17/04/17 14:14:50 INFO JobScheduler: Added jobs for time 1492418690000 ms
17/04/17 14:14:50 INFO JobScheduler: Starting job streaming job 1492418690000 ms.0 from job set of time 1492418690000 ms
-------------------------------------------
Time: 1492418690000 ms
-------------------------------------------

17/04/17 14:14:50 INFO JobScheduler: Finished job streaming job 1492418690000 ms.0 from job set of time 1492418690000 ms
17/04/17 14:14:50 INFO MapPartitionsRDD: Removing RDD 502 from persistence list
17/04/17 14:14:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418690000 ms (execution: 0.001 s)
17/04/17 14:14:50 INFO BlockManager: Removing RDD 502
17/04/17 14:14:50 INFO MapPartitionsRDD: Removing RDD 501 from persistence list
17/04/17 14:14:50 INFO BlockManager: Removing RDD 501
17/04/17 14:14:50 INFO BlockRDD: Removing RDD 500 from persistence list
17/04/17 14:14:50 INFO BlockManager: Removing RDD 500
17/04/17 14:14:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[500] at createStream at KafkaConsumer.java:64 of time 1492418690000 ms
17/04/17 14:14:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418670000 ms)
17/04/17 14:14:50 INFO InputInfoTracker: remove old batch metadata: 1492418670000 ms
17/04/17 14:15:00 INFO JobScheduler: Added jobs for time 1492418700000 ms
17/04/17 14:15:00 INFO JobScheduler: Starting job streaming job 1492418700000 ms.0 from job set of time 1492418700000 ms
-------------------------------------------
Time: 1492418700000 ms
-------------------------------------------

17/04/17 14:15:00 INFO JobScheduler: Finished job streaming job 1492418700000 ms.0 from job set of time 1492418700000 ms
17/04/17 14:15:00 INFO MapPartitionsRDD: Removing RDD 505 from persistence list
17/04/17 14:15:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418700000 ms (execution: 0.001 s)
17/04/17 14:15:00 INFO BlockManager: Removing RDD 505
17/04/17 14:15:00 INFO MapPartitionsRDD: Removing RDD 504 from persistence list
17/04/17 14:15:00 INFO BlockManager: Removing RDD 504
17/04/17 14:15:00 INFO BlockRDD: Removing RDD 503 from persistence list
17/04/17 14:15:00 INFO BlockManager: Removing RDD 503
17/04/17 14:15:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[503] at createStream at KafkaConsumer.java:64 of time 1492418700000 ms
17/04/17 14:15:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418680000 ms)
17/04/17 14:15:00 INFO InputInfoTracker: remove old batch metadata: 1492418680000 ms
17/04/17 14:15:10 INFO JobScheduler: Added jobs for time 1492418710000 ms
17/04/17 14:15:10 INFO JobScheduler: Starting job streaming job 1492418710000 ms.0 from job set of time 1492418710000 ms
-------------------------------------------
Time: 1492418710000 ms
-------------------------------------------

17/04/17 14:15:10 INFO JobScheduler: Finished job streaming job 1492418710000 ms.0 from job set of time 1492418710000 ms
17/04/17 14:15:10 INFO MapPartitionsRDD: Removing RDD 508 from persistence list
17/04/17 14:15:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418710000 ms (execution: 0.001 s)
17/04/17 14:15:10 INFO BlockManager: Removing RDD 508
17/04/17 14:15:10 INFO MapPartitionsRDD: Removing RDD 507 from persistence list
17/04/17 14:15:10 INFO BlockManager: Removing RDD 507
17/04/17 14:15:10 INFO BlockRDD: Removing RDD 506 from persistence list
17/04/17 14:15:10 INFO BlockManager: Removing RDD 506
17/04/17 14:15:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[506] at createStream at KafkaConsumer.java:64 of time 1492418710000 ms
17/04/17 14:15:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418690000 ms)
17/04/17 14:15:10 INFO InputInfoTracker: remove old batch metadata: 1492418690000 ms
17/04/17 14:15:20 INFO JobScheduler: Added jobs for time 1492418720000 ms
17/04/17 14:15:20 INFO JobScheduler: Starting job streaming job 1492418720000 ms.0 from job set of time 1492418720000 ms
-------------------------------------------
Time: 1492418720000 ms
-------------------------------------------

17/04/17 14:15:20 INFO JobScheduler: Finished job streaming job 1492418720000 ms.0 from job set of time 1492418720000 ms
17/04/17 14:15:20 INFO MapPartitionsRDD: Removing RDD 511 from persistence list
17/04/17 14:15:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418720000 ms (execution: 0.000 s)
17/04/17 14:15:20 INFO BlockManager: Removing RDD 511
17/04/17 14:15:20 INFO MapPartitionsRDD: Removing RDD 510 from persistence list
17/04/17 14:15:20 INFO BlockManager: Removing RDD 510
17/04/17 14:15:20 INFO BlockRDD: Removing RDD 509 from persistence list
17/04/17 14:15:20 INFO BlockManager: Removing RDD 509
17/04/17 14:15:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[509] at createStream at KafkaConsumer.java:64 of time 1492418720000 ms
17/04/17 14:15:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418700000 ms)
17/04/17 14:15:20 INFO InputInfoTracker: remove old batch metadata: 1492418700000 ms
17/04/17 14:15:30 INFO JobScheduler: Added jobs for time 1492418730000 ms
-------------------------------------------
Time: 1492418730000 ms
-------------------------------------------

17/04/17 14:15:30 INFO JobScheduler: Starting job streaming job 1492418730000 ms.0 from job set of time 1492418730000 ms
17/04/17 14:15:30 INFO JobScheduler: Finished job streaming job 1492418730000 ms.0 from job set of time 1492418730000 ms
17/04/17 14:15:30 INFO MapPartitionsRDD: Removing RDD 514 from persistence list
17/04/17 14:15:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418730000 ms (execution: 0.000 s)
17/04/17 14:15:30 INFO BlockManager: Removing RDD 514
17/04/17 14:15:30 INFO MapPartitionsRDD: Removing RDD 513 from persistence list
17/04/17 14:15:30 INFO BlockManager: Removing RDD 513
17/04/17 14:15:30 INFO BlockRDD: Removing RDD 512 from persistence list
17/04/17 14:15:30 INFO BlockManager: Removing RDD 512
17/04/17 14:15:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[512] at createStream at KafkaConsumer.java:64 of time 1492418730000 ms
17/04/17 14:15:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418710000 ms)
17/04/17 14:15:30 INFO InputInfoTracker: remove old batch metadata: 1492418710000 ms
17/04/17 14:15:40 INFO JobScheduler: Added jobs for time 1492418740000 ms
17/04/17 14:15:40 INFO JobScheduler: Starting job streaming job 1492418740000 ms.0 from job set of time 1492418740000 ms
-------------------------------------------
Time: 1492418740000 ms
-------------------------------------------

17/04/17 14:15:40 INFO JobScheduler: Finished job streaming job 1492418740000 ms.0 from job set of time 1492418740000 ms
17/04/17 14:15:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418740000 ms (execution: 0.000 s)
17/04/17 14:15:40 INFO MapPartitionsRDD: Removing RDD 517 from persistence list
17/04/17 14:15:40 INFO BlockManager: Removing RDD 517
17/04/17 14:15:40 INFO MapPartitionsRDD: Removing RDD 516 from persistence list
17/04/17 14:15:40 INFO BlockManager: Removing RDD 516
17/04/17 14:15:40 INFO BlockRDD: Removing RDD 515 from persistence list
17/04/17 14:15:40 INFO BlockManager: Removing RDD 515
17/04/17 14:15:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[515] at createStream at KafkaConsumer.java:64 of time 1492418740000 ms
17/04/17 14:15:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418720000 ms)
17/04/17 14:15:40 INFO InputInfoTracker: remove old batch metadata: 1492418720000 ms
17/04/17 14:15:50 INFO JobScheduler: Added jobs for time 1492418750000 ms
17/04/17 14:15:50 INFO JobScheduler: Starting job streaming job 1492418750000 ms.0 from job set of time 1492418750000 ms
-------------------------------------------
Time: 1492418750000 ms
-------------------------------------------

17/04/17 14:15:50 INFO JobScheduler: Finished job streaming job 1492418750000 ms.0 from job set of time 1492418750000 ms
17/04/17 14:15:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418750000 ms (execution: 0.001 s)
17/04/17 14:15:50 INFO MapPartitionsRDD: Removing RDD 520 from persistence list
17/04/17 14:15:50 INFO BlockManager: Removing RDD 520
17/04/17 14:15:50 INFO MapPartitionsRDD: Removing RDD 519 from persistence list
17/04/17 14:15:50 INFO BlockManager: Removing RDD 519
17/04/17 14:15:50 INFO BlockRDD: Removing RDD 518 from persistence list
17/04/17 14:15:50 INFO BlockManager: Removing RDD 518
17/04/17 14:15:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[518] at createStream at KafkaConsumer.java:64 of time 1492418750000 ms
17/04/17 14:15:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418730000 ms)
17/04/17 14:15:50 INFO InputInfoTracker: remove old batch metadata: 1492418730000 ms
17/04/17 14:16:00 INFO JobScheduler: Added jobs for time 1492418760000 ms
17/04/17 14:16:00 INFO JobScheduler: Starting job streaming job 1492418760000 ms.0 from job set of time 1492418760000 ms
-------------------------------------------
Time: 1492418760000 ms
-------------------------------------------

17/04/17 14:16:00 INFO JobScheduler: Finished job streaming job 1492418760000 ms.0 from job set of time 1492418760000 ms
17/04/17 14:16:00 INFO MapPartitionsRDD: Removing RDD 523 from persistence list
17/04/17 14:16:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418760000 ms (execution: 0.001 s)
17/04/17 14:16:00 INFO BlockManager: Removing RDD 523
17/04/17 14:16:00 INFO MapPartitionsRDD: Removing RDD 522 from persistence list
17/04/17 14:16:00 INFO BlockManager: Removing RDD 522
17/04/17 14:16:00 INFO BlockRDD: Removing RDD 521 from persistence list
17/04/17 14:16:00 INFO BlockManager: Removing RDD 521
17/04/17 14:16:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[521] at createStream at KafkaConsumer.java:64 of time 1492418760000 ms
17/04/17 14:16:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418740000 ms)
17/04/17 14:16:00 INFO InputInfoTracker: remove old batch metadata: 1492418740000 ms
17/04/17 14:16:10 INFO JobScheduler: Added jobs for time 1492418770000 ms
17/04/17 14:16:10 INFO JobScheduler: Starting job streaming job 1492418770000 ms.0 from job set of time 1492418770000 ms
-------------------------------------------
Time: 1492418770000 ms
-------------------------------------------

17/04/17 14:16:10 INFO JobScheduler: Finished job streaming job 1492418770000 ms.0 from job set of time 1492418770000 ms
17/04/17 14:16:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418770000 ms (execution: 0.000 s)
17/04/17 14:16:10 INFO MapPartitionsRDD: Removing RDD 526 from persistence list
17/04/17 14:16:10 INFO BlockManager: Removing RDD 526
17/04/17 14:16:10 INFO MapPartitionsRDD: Removing RDD 525 from persistence list
17/04/17 14:16:10 INFO BlockManager: Removing RDD 525
17/04/17 14:16:10 INFO BlockRDD: Removing RDD 524 from persistence list
17/04/17 14:16:10 INFO BlockManager: Removing RDD 524
17/04/17 14:16:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[524] at createStream at KafkaConsumer.java:64 of time 1492418770000 ms
17/04/17 14:16:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418750000 ms)
17/04/17 14:16:10 INFO InputInfoTracker: remove old batch metadata: 1492418750000 ms
17/04/17 14:16:20 INFO JobScheduler: Added jobs for time 1492418780000 ms
17/04/17 14:16:20 INFO JobScheduler: Starting job streaming job 1492418780000 ms.0 from job set of time 1492418780000 ms
-------------------------------------------
Time: 1492418780000 ms
-------------------------------------------

17/04/17 14:16:20 INFO JobScheduler: Finished job streaming job 1492418780000 ms.0 from job set of time 1492418780000 ms
17/04/17 14:16:20 INFO MapPartitionsRDD: Removing RDD 529 from persistence list
17/04/17 14:16:20 INFO JobScheduler: Total delay: 0.003 s for time 1492418780000 ms (execution: 0.000 s)
17/04/17 14:16:20 INFO BlockManager: Removing RDD 529
17/04/17 14:16:20 INFO MapPartitionsRDD: Removing RDD 528 from persistence list
17/04/17 14:16:20 INFO BlockManager: Removing RDD 528
17/04/17 14:16:20 INFO BlockRDD: Removing RDD 527 from persistence list
17/04/17 14:16:20 INFO BlockManager: Removing RDD 527
17/04/17 14:16:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[527] at createStream at KafkaConsumer.java:64 of time 1492418780000 ms
17/04/17 14:16:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418760000 ms)
17/04/17 14:16:20 INFO InputInfoTracker: remove old batch metadata: 1492418760000 ms
17/04/17 14:16:30 INFO JobScheduler: Added jobs for time 1492418790000 ms
17/04/17 14:16:30 INFO JobScheduler: Starting job streaming job 1492418790000 ms.0 from job set of time 1492418790000 ms
-------------------------------------------
Time: 1492418790000 ms
-------------------------------------------

17/04/17 14:16:30 INFO JobScheduler: Finished job streaming job 1492418790000 ms.0 from job set of time 1492418790000 ms
17/04/17 14:16:30 INFO MapPartitionsRDD: Removing RDD 532 from persistence list
17/04/17 14:16:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418790000 ms (execution: 0.001 s)
17/04/17 14:16:30 INFO BlockManager: Removing RDD 532
17/04/17 14:16:30 INFO MapPartitionsRDD: Removing RDD 531 from persistence list
17/04/17 14:16:30 INFO BlockManager: Removing RDD 531
17/04/17 14:16:30 INFO BlockRDD: Removing RDD 530 from persistence list
17/04/17 14:16:30 INFO BlockManager: Removing RDD 530
17/04/17 14:16:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[530] at createStream at KafkaConsumer.java:64 of time 1492418790000 ms
17/04/17 14:16:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418770000 ms)
17/04/17 14:16:30 INFO InputInfoTracker: remove old batch metadata: 1492418770000 ms
17/04/17 14:16:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.1:35386 in memory (size: 1726.0 B, free: 511.1 MB)
17/04/17 14:16:32 INFO ContextCleaner: Cleaned accumulator 1
17/04/17 14:16:40 INFO JobScheduler: Added jobs for time 1492418800000 ms
-------------------------------------------
Time: 1492418800000 ms
-------------------------------------------

17/04/17 14:16:40 INFO JobScheduler: Starting job streaming job 1492418800000 ms.0 from job set of time 1492418800000 ms
17/04/17 14:16:40 INFO JobScheduler: Finished job streaming job 1492418800000 ms.0 from job set of time 1492418800000 ms
17/04/17 14:16:40 INFO JobScheduler: Total delay: 0.014 s for time 1492418800000 ms (execution: 0.000 s)
17/04/17 14:16:40 INFO MapPartitionsRDD: Removing RDD 535 from persistence list
17/04/17 14:16:40 INFO BlockManager: Removing RDD 535
17/04/17 14:16:40 INFO MapPartitionsRDD: Removing RDD 534 from persistence list
17/04/17 14:16:40 INFO BlockManager: Removing RDD 534
17/04/17 14:16:40 INFO BlockRDD: Removing RDD 533 from persistence list
17/04/17 14:16:40 INFO BlockManager: Removing RDD 533
17/04/17 14:16:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[533] at createStream at KafkaConsumer.java:64 of time 1492418800000 ms
17/04/17 14:16:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418780000 ms)
17/04/17 14:16:40 INFO InputInfoTracker: remove old batch metadata: 1492418780000 ms
17/04/17 14:16:50 INFO JobScheduler: Added jobs for time 1492418810000 ms
17/04/17 14:16:50 INFO JobScheduler: Starting job streaming job 1492418810000 ms.0 from job set of time 1492418810000 ms
-------------------------------------------
Time: 1492418810000 ms
-------------------------------------------

17/04/17 14:16:50 INFO JobScheduler: Finished job streaming job 1492418810000 ms.0 from job set of time 1492418810000 ms
17/04/17 14:16:50 INFO MapPartitionsRDD: Removing RDD 538 from persistence list
17/04/17 14:16:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418810000 ms (execution: 0.001 s)
17/04/17 14:16:50 INFO BlockManager: Removing RDD 538
17/04/17 14:16:50 INFO MapPartitionsRDD: Removing RDD 537 from persistence list
17/04/17 14:16:50 INFO BlockManager: Removing RDD 537
17/04/17 14:16:50 INFO BlockRDD: Removing RDD 536 from persistence list
17/04/17 14:16:50 INFO BlockManager: Removing RDD 536
17/04/17 14:16:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[536] at createStream at KafkaConsumer.java:64 of time 1492418810000 ms
17/04/17 14:16:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418790000 ms)
17/04/17 14:16:50 INFO InputInfoTracker: remove old batch metadata: 1492418790000 ms
17/04/17 14:17:00 INFO JobScheduler: Added jobs for time 1492418820000 ms
17/04/17 14:17:00 INFO JobScheduler: Starting job streaming job 1492418820000 ms.0 from job set of time 1492418820000 ms
-------------------------------------------
Time: 1492418820000 ms
-------------------------------------------

17/04/17 14:17:00 INFO JobScheduler: Finished job streaming job 1492418820000 ms.0 from job set of time 1492418820000 ms
17/04/17 14:17:00 INFO MapPartitionsRDD: Removing RDD 541 from persistence list
17/04/17 14:17:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418820000 ms (execution: 0.000 s)
17/04/17 14:17:00 INFO BlockManager: Removing RDD 541
17/04/17 14:17:00 INFO MapPartitionsRDD: Removing RDD 540 from persistence list
17/04/17 14:17:00 INFO BlockManager: Removing RDD 540
17/04/17 14:17:00 INFO BlockRDD: Removing RDD 539 from persistence list
17/04/17 14:17:00 INFO BlockManager: Removing RDD 539
17/04/17 14:17:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[539] at createStream at KafkaConsumer.java:64 of time 1492418820000 ms
17/04/17 14:17:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418800000 ms)
17/04/17 14:17:00 INFO InputInfoTracker: remove old batch metadata: 1492418800000 ms
17/04/17 14:17:10 INFO JobScheduler: Starting job streaming job 1492418830000 ms.0 from job set of time 1492418830000 ms
-------------------------------------------
Time: 1492418830000 ms
-------------------------------------------

17/04/17 14:17:10 INFO JobScheduler: Added jobs for time 1492418830000 ms
17/04/17 14:17:10 INFO JobScheduler: Finished job streaming job 1492418830000 ms.0 from job set of time 1492418830000 ms
17/04/17 14:17:10 INFO MapPartitionsRDD: Removing RDD 544 from persistence list
17/04/17 14:17:10 INFO JobScheduler: Total delay: 0.007 s for time 1492418830000 ms (execution: 0.001 s)
17/04/17 14:17:10 INFO BlockManager: Removing RDD 544
17/04/17 14:17:10 INFO MapPartitionsRDD: Removing RDD 543 from persistence list
17/04/17 14:17:10 INFO BlockManager: Removing RDD 543
17/04/17 14:17:10 INFO BlockRDD: Removing RDD 542 from persistence list
17/04/17 14:17:10 INFO BlockManager: Removing RDD 542
17/04/17 14:17:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[542] at createStream at KafkaConsumer.java:64 of time 1492418830000 ms
17/04/17 14:17:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418810000 ms)
17/04/17 14:17:10 INFO InputInfoTracker: remove old batch metadata: 1492418810000 ms
17/04/17 14:17:20 INFO JobScheduler: Added jobs for time 1492418840000 ms
17/04/17 14:17:20 INFO JobScheduler: Starting job streaming job 1492418840000 ms.0 from job set of time 1492418840000 ms
-------------------------------------------
Time: 1492418840000 ms
-------------------------------------------

17/04/17 14:17:20 INFO JobScheduler: Finished job streaming job 1492418840000 ms.0 from job set of time 1492418840000 ms
17/04/17 14:17:20 INFO MapPartitionsRDD: Removing RDD 547 from persistence list
17/04/17 14:17:20 INFO JobScheduler: Total delay: 0.006 s for time 1492418840000 ms (execution: 0.001 s)
17/04/17 14:17:20 INFO BlockManager: Removing RDD 547
17/04/17 14:17:20 INFO MapPartitionsRDD: Removing RDD 546 from persistence list
17/04/17 14:17:20 INFO BlockManager: Removing RDD 546
17/04/17 14:17:20 INFO BlockRDD: Removing RDD 545 from persistence list
17/04/17 14:17:20 INFO BlockManager: Removing RDD 545
17/04/17 14:17:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[545] at createStream at KafkaConsumer.java:64 of time 1492418840000 ms
17/04/17 14:17:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418820000 ms)
17/04/17 14:17:20 INFO InputInfoTracker: remove old batch metadata: 1492418820000 ms
17/04/17 14:17:30 INFO JobScheduler: Added jobs for time 1492418850000 ms
17/04/17 14:17:30 INFO JobScheduler: Starting job streaming job 1492418850000 ms.0 from job set of time 1492418850000 ms
-------------------------------------------
Time: 1492418850000 ms
-------------------------------------------

17/04/17 14:17:30 INFO JobScheduler: Finished job streaming job 1492418850000 ms.0 from job set of time 1492418850000 ms
17/04/17 14:17:30 INFO MapPartitionsRDD: Removing RDD 550 from persistence list
17/04/17 14:17:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418850000 ms (execution: 0.000 s)
17/04/17 14:17:30 INFO BlockManager: Removing RDD 550
17/04/17 14:17:30 INFO MapPartitionsRDD: Removing RDD 549 from persistence list
17/04/17 14:17:30 INFO BlockManager: Removing RDD 549
17/04/17 14:17:30 INFO BlockRDD: Removing RDD 548 from persistence list
17/04/17 14:17:30 INFO BlockManager: Removing RDD 548
17/04/17 14:17:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[548] at createStream at KafkaConsumer.java:64 of time 1492418850000 ms
17/04/17 14:17:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418830000 ms)
17/04/17 14:17:30 INFO InputInfoTracker: remove old batch metadata: 1492418830000 ms
17/04/17 14:17:40 INFO JobScheduler: Added jobs for time 1492418860000 ms
17/04/17 14:17:40 INFO JobScheduler: Starting job streaming job 1492418860000 ms.0 from job set of time 1492418860000 ms
-------------------------------------------
Time: 1492418860000 ms
-------------------------------------------

17/04/17 14:17:40 INFO JobScheduler: Finished job streaming job 1492418860000 ms.0 from job set of time 1492418860000 ms
17/04/17 14:17:40 INFO MapPartitionsRDD: Removing RDD 553 from persistence list
17/04/17 14:17:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418860000 ms (execution: 0.000 s)
17/04/17 14:17:40 INFO BlockManager: Removing RDD 553
17/04/17 14:17:40 INFO MapPartitionsRDD: Removing RDD 552 from persistence list
17/04/17 14:17:40 INFO BlockManager: Removing RDD 552
17/04/17 14:17:40 INFO BlockRDD: Removing RDD 551 from persistence list
17/04/17 14:17:40 INFO BlockManager: Removing RDD 551
17/04/17 14:17:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[551] at createStream at KafkaConsumer.java:64 of time 1492418860000 ms
17/04/17 14:17:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418840000 ms)
17/04/17 14:17:40 INFO InputInfoTracker: remove old batch metadata: 1492418840000 ms
17/04/17 14:17:50 INFO JobScheduler: Added jobs for time 1492418870000 ms
17/04/17 14:17:50 INFO JobScheduler: Starting job streaming job 1492418870000 ms.0 from job set of time 1492418870000 ms
-------------------------------------------
Time: 1492418870000 ms
-------------------------------------------

17/04/17 14:17:50 INFO JobScheduler: Finished job streaming job 1492418870000 ms.0 from job set of time 1492418870000 ms
17/04/17 14:17:50 INFO JobScheduler: Total delay: 0.006 s for time 1492418870000 ms (execution: 0.001 s)
17/04/17 14:17:50 INFO MapPartitionsRDD: Removing RDD 556 from persistence list
17/04/17 14:17:50 INFO BlockManager: Removing RDD 556
17/04/17 14:17:50 INFO MapPartitionsRDD: Removing RDD 555 from persistence list
17/04/17 14:17:50 INFO BlockManager: Removing RDD 555
17/04/17 14:17:50 INFO BlockRDD: Removing RDD 554 from persistence list
17/04/17 14:17:50 INFO BlockManager: Removing RDD 554
17/04/17 14:17:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[554] at createStream at KafkaConsumer.java:64 of time 1492418870000 ms
17/04/17 14:17:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418850000 ms)
17/04/17 14:17:50 INFO InputInfoTracker: remove old batch metadata: 1492418850000 ms
17/04/17 14:18:00 INFO JobScheduler: Added jobs for time 1492418880000 ms
17/04/17 14:18:00 INFO JobScheduler: Starting job streaming job 1492418880000 ms.0 from job set of time 1492418880000 ms
-------------------------------------------
Time: 1492418880000 ms
-------------------------------------------

17/04/17 14:18:00 INFO JobScheduler: Finished job streaming job 1492418880000 ms.0 from job set of time 1492418880000 ms
17/04/17 14:18:00 INFO MapPartitionsRDD: Removing RDD 559 from persistence list
17/04/17 14:18:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418880000 ms (execution: 0.000 s)
17/04/17 14:18:00 INFO BlockManager: Removing RDD 559
17/04/17 14:18:00 INFO MapPartitionsRDD: Removing RDD 558 from persistence list
17/04/17 14:18:00 INFO BlockManager: Removing RDD 558
17/04/17 14:18:00 INFO BlockRDD: Removing RDD 557 from persistence list
17/04/17 14:18:00 INFO BlockManager: Removing RDD 557
17/04/17 14:18:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[557] at createStream at KafkaConsumer.java:64 of time 1492418880000 ms
17/04/17 14:18:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418860000 ms)
17/04/17 14:18:00 INFO InputInfoTracker: remove old batch metadata: 1492418860000 ms
17/04/17 14:18:10 INFO JobScheduler: Added jobs for time 1492418890000 ms
17/04/17 14:18:10 INFO JobScheduler: Starting job streaming job 1492418890000 ms.0 from job set of time 1492418890000 ms
-------------------------------------------
Time: 1492418890000 ms
-------------------------------------------

17/04/17 14:18:10 INFO JobScheduler: Finished job streaming job 1492418890000 ms.0 from job set of time 1492418890000 ms
17/04/17 14:18:10 INFO MapPartitionsRDD: Removing RDD 562 from persistence list
17/04/17 14:18:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418890000 ms (execution: 0.000 s)
17/04/17 14:18:10 INFO BlockManager: Removing RDD 562
17/04/17 14:18:10 INFO MapPartitionsRDD: Removing RDD 561 from persistence list
17/04/17 14:18:10 INFO BlockManager: Removing RDD 561
17/04/17 14:18:10 INFO BlockRDD: Removing RDD 560 from persistence list
17/04/17 14:18:10 INFO BlockManager: Removing RDD 560
17/04/17 14:18:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[560] at createStream at KafkaConsumer.java:64 of time 1492418890000 ms
17/04/17 14:18:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418870000 ms)
17/04/17 14:18:10 INFO InputInfoTracker: remove old batch metadata: 1492418870000 ms
17/04/17 14:18:20 INFO JobScheduler: Added jobs for time 1492418900000 ms
17/04/17 14:18:20 INFO JobScheduler: Starting job streaming job 1492418900000 ms.0 from job set of time 1492418900000 ms
-------------------------------------------
Time: 1492418900000 ms
-------------------------------------------

17/04/17 14:18:20 INFO JobScheduler: Finished job streaming job 1492418900000 ms.0 from job set of time 1492418900000 ms
17/04/17 14:18:20 INFO MapPartitionsRDD: Removing RDD 565 from persistence list
17/04/17 14:18:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418900000 ms (execution: 0.000 s)
17/04/17 14:18:20 INFO MapPartitionsRDD: Removing RDD 564 from persistence list
17/04/17 14:18:20 INFO BlockManager: Removing RDD 565
17/04/17 14:18:20 INFO BlockManager: Removing RDD 564
17/04/17 14:18:20 INFO BlockRDD: Removing RDD 563 from persistence list
17/04/17 14:18:20 INFO BlockManager: Removing RDD 563
17/04/17 14:18:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[563] at createStream at KafkaConsumer.java:64 of time 1492418900000 ms
17/04/17 14:18:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418880000 ms)
17/04/17 14:18:20 INFO InputInfoTracker: remove old batch metadata: 1492418880000 ms
17/04/17 14:18:30 INFO JobScheduler: Added jobs for time 1492418910000 ms
17/04/17 14:18:30 INFO JobScheduler: Starting job streaming job 1492418910000 ms.0 from job set of time 1492418910000 ms
-------------------------------------------
Time: 1492418910000 ms
-------------------------------------------

17/04/17 14:18:30 INFO JobScheduler: Finished job streaming job 1492418910000 ms.0 from job set of time 1492418910000 ms
17/04/17 14:18:30 INFO MapPartitionsRDD: Removing RDD 568 from persistence list
17/04/17 14:18:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418910000 ms (execution: 0.001 s)
17/04/17 14:18:30 INFO BlockManager: Removing RDD 568
17/04/17 14:18:30 INFO MapPartitionsRDD: Removing RDD 567 from persistence list
17/04/17 14:18:30 INFO BlockManager: Removing RDD 567
17/04/17 14:18:30 INFO BlockRDD: Removing RDD 566 from persistence list
17/04/17 14:18:30 INFO BlockManager: Removing RDD 566
17/04/17 14:18:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[566] at createStream at KafkaConsumer.java:64 of time 1492418910000 ms
17/04/17 14:18:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418890000 ms)
17/04/17 14:18:30 INFO InputInfoTracker: remove old batch metadata: 1492418890000 ms
17/04/17 14:18:40 INFO JobScheduler: Added jobs for time 1492418920000 ms
17/04/17 14:18:40 INFO JobScheduler: Starting job streaming job 1492418920000 ms.0 from job set of time 1492418920000 ms
-------------------------------------------
Time: 1492418920000 ms
-------------------------------------------

17/04/17 14:18:40 INFO JobScheduler: Finished job streaming job 1492418920000 ms.0 from job set of time 1492418920000 ms
17/04/17 14:18:40 INFO MapPartitionsRDD: Removing RDD 571 from persistence list
17/04/17 14:18:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418920000 ms (execution: 0.000 s)
17/04/17 14:18:40 INFO BlockManager: Removing RDD 571
17/04/17 14:18:40 INFO MapPartitionsRDD: Removing RDD 570 from persistence list
17/04/17 14:18:40 INFO BlockManager: Removing RDD 570
17/04/17 14:18:40 INFO BlockRDD: Removing RDD 569 from persistence list
17/04/17 14:18:40 INFO BlockManager: Removing RDD 569
17/04/17 14:18:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[569] at createStream at KafkaConsumer.java:64 of time 1492418920000 ms
17/04/17 14:18:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418900000 ms)
17/04/17 14:18:40 INFO InputInfoTracker: remove old batch metadata: 1492418900000 ms
17/04/17 14:18:50 INFO JobScheduler: Added jobs for time 1492418930000 ms
17/04/17 14:18:50 INFO JobScheduler: Starting job streaming job 1492418930000 ms.0 from job set of time 1492418930000 ms
-------------------------------------------
Time: 1492418930000 ms
-------------------------------------------

17/04/17 14:18:50 INFO JobScheduler: Finished job streaming job 1492418930000 ms.0 from job set of time 1492418930000 ms
17/04/17 14:18:50 INFO MapPartitionsRDD: Removing RDD 574 from persistence list
17/04/17 14:18:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418930000 ms (execution: 0.001 s)
17/04/17 14:18:50 INFO BlockManager: Removing RDD 574
17/04/17 14:18:50 INFO MapPartitionsRDD: Removing RDD 573 from persistence list
17/04/17 14:18:50 INFO BlockManager: Removing RDD 573
17/04/17 14:18:50 INFO BlockRDD: Removing RDD 572 from persistence list
17/04/17 14:18:50 INFO BlockManager: Removing RDD 572
17/04/17 14:18:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[572] at createStream at KafkaConsumer.java:64 of time 1492418930000 ms
17/04/17 14:18:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418910000 ms)
17/04/17 14:18:50 INFO InputInfoTracker: remove old batch metadata: 1492418910000 ms
17/04/17 14:19:00 INFO JobScheduler: Starting job streaming job 1492418940000 ms.0 from job set of time 1492418940000 ms
-------------------------------------------
Time: 1492418940000 ms
-------------------------------------------

17/04/17 14:19:00 INFO JobScheduler: Added jobs for time 1492418940000 ms
17/04/17 14:19:00 INFO JobScheduler: Finished job streaming job 1492418940000 ms.0 from job set of time 1492418940000 ms
17/04/17 14:19:00 INFO MapPartitionsRDD: Removing RDD 577 from persistence list
17/04/17 14:19:00 INFO JobScheduler: Total delay: 0.005 s for time 1492418940000 ms (execution: 0.000 s)
17/04/17 14:19:00 INFO MapPartitionsRDD: Removing RDD 576 from persistence list
17/04/17 14:19:00 INFO BlockManager: Removing RDD 577
17/04/17 14:19:00 INFO BlockManager: Removing RDD 576
17/04/17 14:19:00 INFO BlockRDD: Removing RDD 575 from persistence list
17/04/17 14:19:00 INFO BlockManager: Removing RDD 575
17/04/17 14:19:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[575] at createStream at KafkaConsumer.java:64 of time 1492418940000 ms
17/04/17 14:19:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418920000 ms)
17/04/17 14:19:00 INFO InputInfoTracker: remove old batch metadata: 1492418920000 ms
17/04/17 14:19:10 INFO JobScheduler: Starting job streaming job 1492418950000 ms.0 from job set of time 1492418950000 ms
-------------------------------------------
Time: 1492418950000 ms
-------------------------------------------

17/04/17 14:19:10 INFO JobScheduler: Added jobs for time 1492418950000 ms
17/04/17 14:19:10 INFO JobScheduler: Finished job streaming job 1492418950000 ms.0 from job set of time 1492418950000 ms
17/04/17 14:19:10 INFO JobScheduler: Total delay: 0.005 s for time 1492418950000 ms (execution: 0.000 s)
17/04/17 14:19:10 INFO MapPartitionsRDD: Removing RDD 580 from persistence list
17/04/17 14:19:10 INFO BlockManager: Removing RDD 580
17/04/17 14:19:10 INFO MapPartitionsRDD: Removing RDD 579 from persistence list
17/04/17 14:19:10 INFO BlockManager: Removing RDD 579
17/04/17 14:19:10 INFO BlockRDD: Removing RDD 578 from persistence list
17/04/17 14:19:10 INFO BlockManager: Removing RDD 578
17/04/17 14:19:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[578] at createStream at KafkaConsumer.java:64 of time 1492418950000 ms
17/04/17 14:19:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418930000 ms)
17/04/17 14:19:10 INFO InputInfoTracker: remove old batch metadata: 1492418930000 ms
17/04/17 14:19:20 INFO JobScheduler: Added jobs for time 1492418960000 ms
17/04/17 14:19:20 INFO JobScheduler: Starting job streaming job 1492418960000 ms.0 from job set of time 1492418960000 ms
-------------------------------------------
Time: 1492418960000 ms
-------------------------------------------

17/04/17 14:19:20 INFO JobScheduler: Finished job streaming job 1492418960000 ms.0 from job set of time 1492418960000 ms
17/04/17 14:19:20 INFO MapPartitionsRDD: Removing RDD 583 from persistence list
17/04/17 14:19:20 INFO JobScheduler: Total delay: 0.005 s for time 1492418960000 ms (execution: 0.001 s)
17/04/17 14:19:20 INFO BlockManager: Removing RDD 583
17/04/17 14:19:20 INFO MapPartitionsRDD: Removing RDD 582 from persistence list
17/04/17 14:19:20 INFO BlockManager: Removing RDD 582
17/04/17 14:19:20 INFO BlockRDD: Removing RDD 581 from persistence list
17/04/17 14:19:20 INFO BlockManager: Removing RDD 581
17/04/17 14:19:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[581] at createStream at KafkaConsumer.java:64 of time 1492418960000 ms
17/04/17 14:19:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418940000 ms)
17/04/17 14:19:20 INFO InputInfoTracker: remove old batch metadata: 1492418940000 ms
17/04/17 14:19:30 INFO JobScheduler: Added jobs for time 1492418970000 ms
17/04/17 14:19:30 INFO JobScheduler: Starting job streaming job 1492418970000 ms.0 from job set of time 1492418970000 ms
-------------------------------------------
Time: 1492418970000 ms
-------------------------------------------

17/04/17 14:19:30 INFO JobScheduler: Finished job streaming job 1492418970000 ms.0 from job set of time 1492418970000 ms
17/04/17 14:19:30 INFO MapPartitionsRDD: Removing RDD 586 from persistence list
17/04/17 14:19:30 INFO JobScheduler: Total delay: 0.005 s for time 1492418970000 ms (execution: 0.000 s)
17/04/17 14:19:30 INFO BlockManager: Removing RDD 586
17/04/17 14:19:30 INFO MapPartitionsRDD: Removing RDD 585 from persistence list
17/04/17 14:19:30 INFO BlockManager: Removing RDD 585
17/04/17 14:19:30 INFO BlockRDD: Removing RDD 584 from persistence list
17/04/17 14:19:30 INFO BlockManager: Removing RDD 584
17/04/17 14:19:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[584] at createStream at KafkaConsumer.java:64 of time 1492418970000 ms
17/04/17 14:19:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418950000 ms)
17/04/17 14:19:30 INFO InputInfoTracker: remove old batch metadata: 1492418950000 ms
17/04/17 14:19:40 INFO JobScheduler: Added jobs for time 1492418980000 ms
-------------------------------------------
Time: 1492418980000 ms
-------------------------------------------

17/04/17 14:19:40 INFO JobScheduler: Starting job streaming job 1492418980000 ms.0 from job set of time 1492418980000 ms
17/04/17 14:19:40 INFO JobScheduler: Finished job streaming job 1492418980000 ms.0 from job set of time 1492418980000 ms
17/04/17 14:19:40 INFO MapPartitionsRDD: Removing RDD 589 from persistence list
17/04/17 14:19:40 INFO JobScheduler: Total delay: 0.005 s for time 1492418980000 ms (execution: 0.000 s)
17/04/17 14:19:40 INFO BlockManager: Removing RDD 589
17/04/17 14:19:40 INFO MapPartitionsRDD: Removing RDD 588 from persistence list
17/04/17 14:19:40 INFO BlockManager: Removing RDD 588
17/04/17 14:19:40 INFO BlockRDD: Removing RDD 587 from persistence list
17/04/17 14:19:40 INFO BlockManager: Removing RDD 587
17/04/17 14:19:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[587] at createStream at KafkaConsumer.java:64 of time 1492418980000 ms
17/04/17 14:19:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418960000 ms)
17/04/17 14:19:40 INFO InputInfoTracker: remove old batch metadata: 1492418960000 ms
17/04/17 14:19:50 INFO JobScheduler: Added jobs for time 1492418990000 ms
17/04/17 14:19:50 INFO JobScheduler: Starting job streaming job 1492418990000 ms.0 from job set of time 1492418990000 ms
-------------------------------------------
Time: 1492418990000 ms
-------------------------------------------

17/04/17 14:19:50 INFO JobScheduler: Finished job streaming job 1492418990000 ms.0 from job set of time 1492418990000 ms
17/04/17 14:19:50 INFO MapPartitionsRDD: Removing RDD 592 from persistence list
17/04/17 14:19:50 INFO JobScheduler: Total delay: 0.005 s for time 1492418990000 ms (execution: 0.001 s)
17/04/17 14:19:50 INFO BlockManager: Removing RDD 592
17/04/17 14:19:50 INFO MapPartitionsRDD: Removing RDD 591 from persistence list
17/04/17 14:19:50 INFO BlockManager: Removing RDD 591
17/04/17 14:19:50 INFO BlockRDD: Removing RDD 590 from persistence list
17/04/17 14:19:50 INFO BlockManager: Removing RDD 590
17/04/17 14:19:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[590] at createStream at KafkaConsumer.java:64 of time 1492418990000 ms
17/04/17 14:19:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418970000 ms)
17/04/17 14:19:50 INFO InputInfoTracker: remove old batch metadata: 1492418970000 ms
17/04/17 14:20:00 INFO JobScheduler: Added jobs for time 1492419000000 ms
17/04/17 14:20:00 INFO JobScheduler: Starting job streaming job 1492419000000 ms.0 from job set of time 1492419000000 ms
-------------------------------------------
Time: 1492419000000 ms
-------------------------------------------

17/04/17 14:20:00 INFO JobScheduler: Finished job streaming job 1492419000000 ms.0 from job set of time 1492419000000 ms
17/04/17 14:20:00 INFO MapPartitionsRDD: Removing RDD 595 from persistence list
17/04/17 14:20:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419000000 ms (execution: 0.000 s)
17/04/17 14:20:00 INFO BlockManager: Removing RDD 595
17/04/17 14:20:00 INFO MapPartitionsRDD: Removing RDD 594 from persistence list
17/04/17 14:20:00 INFO BlockManager: Removing RDD 594
17/04/17 14:20:00 INFO BlockRDD: Removing RDD 593 from persistence list
17/04/17 14:20:00 INFO BlockManager: Removing RDD 593
17/04/17 14:20:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[593] at createStream at KafkaConsumer.java:64 of time 1492419000000 ms
17/04/17 14:20:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418980000 ms)
17/04/17 14:20:00 INFO InputInfoTracker: remove old batch metadata: 1492418980000 ms
17/04/17 14:20:10 INFO JobScheduler: Added jobs for time 1492419010000 ms
17/04/17 14:20:10 INFO JobScheduler: Starting job streaming job 1492419010000 ms.0 from job set of time 1492419010000 ms
-------------------------------------------
Time: 1492419010000 ms
-------------------------------------------

17/04/17 14:20:10 INFO JobScheduler: Finished job streaming job 1492419010000 ms.0 from job set of time 1492419010000 ms
17/04/17 14:20:10 INFO MapPartitionsRDD: Removing RDD 598 from persistence list
17/04/17 14:20:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419010000 ms (execution: 0.000 s)
17/04/17 14:20:10 INFO BlockManager: Removing RDD 598
17/04/17 14:20:10 INFO MapPartitionsRDD: Removing RDD 597 from persistence list
17/04/17 14:20:10 INFO BlockManager: Removing RDD 597
17/04/17 14:20:10 INFO BlockRDD: Removing RDD 596 from persistence list
17/04/17 14:20:10 INFO BlockManager: Removing RDD 596
17/04/17 14:20:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[596] at createStream at KafkaConsumer.java:64 of time 1492419010000 ms
17/04/17 14:20:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492418990000 ms)
17/04/17 14:20:10 INFO InputInfoTracker: remove old batch metadata: 1492418990000 ms
17/04/17 14:20:20 INFO JobScheduler: Added jobs for time 1492419020000 ms
17/04/17 14:20:20 INFO JobScheduler: Starting job streaming job 1492419020000 ms.0 from job set of time 1492419020000 ms
-------------------------------------------
Time: 1492419020000 ms
-------------------------------------------

17/04/17 14:20:20 INFO JobScheduler: Finished job streaming job 1492419020000 ms.0 from job set of time 1492419020000 ms
17/04/17 14:20:20 INFO MapPartitionsRDD: Removing RDD 601 from persistence list
17/04/17 14:20:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419020000 ms (execution: 0.001 s)
17/04/17 14:20:20 INFO BlockManager: Removing RDD 601
17/04/17 14:20:20 INFO MapPartitionsRDD: Removing RDD 600 from persistence list
17/04/17 14:20:20 INFO BlockManager: Removing RDD 600
17/04/17 14:20:20 INFO BlockRDD: Removing RDD 599 from persistence list
17/04/17 14:20:20 INFO BlockManager: Removing RDD 599
17/04/17 14:20:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[599] at createStream at KafkaConsumer.java:64 of time 1492419020000 ms
17/04/17 14:20:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419000000 ms)
17/04/17 14:20:20 INFO InputInfoTracker: remove old batch metadata: 1492419000000 ms
17/04/17 14:20:30 INFO JobScheduler: Added jobs for time 1492419030000 ms
17/04/17 14:20:30 INFO JobScheduler: Starting job streaming job 1492419030000 ms.0 from job set of time 1492419030000 ms
-------------------------------------------
Time: 1492419030000 ms
-------------------------------------------

17/04/17 14:20:30 INFO JobScheduler: Finished job streaming job 1492419030000 ms.0 from job set of time 1492419030000 ms
17/04/17 14:20:30 INFO MapPartitionsRDD: Removing RDD 604 from persistence list
17/04/17 14:20:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419030000 ms (execution: 0.001 s)
17/04/17 14:20:30 INFO BlockManager: Removing RDD 604
17/04/17 14:20:30 INFO MapPartitionsRDD: Removing RDD 603 from persistence list
17/04/17 14:20:30 INFO BlockManager: Removing RDD 603
17/04/17 14:20:30 INFO BlockRDD: Removing RDD 602 from persistence list
17/04/17 14:20:30 INFO BlockManager: Removing RDD 602
17/04/17 14:20:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[602] at createStream at KafkaConsumer.java:64 of time 1492419030000 ms
17/04/17 14:20:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419010000 ms)
17/04/17 14:20:30 INFO InputInfoTracker: remove old batch metadata: 1492419010000 ms
17/04/17 14:20:40 INFO JobScheduler: Added jobs for time 1492419040000 ms
17/04/17 14:20:40 INFO JobScheduler: Starting job streaming job 1492419040000 ms.0 from job set of time 1492419040000 ms
-------------------------------------------
Time: 1492419040000 ms
-------------------------------------------

17/04/17 14:20:40 INFO JobScheduler: Finished job streaming job 1492419040000 ms.0 from job set of time 1492419040000 ms
17/04/17 14:20:40 INFO MapPartitionsRDD: Removing RDD 607 from persistence list
17/04/17 14:20:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419040000 ms (execution: 0.000 s)
17/04/17 14:20:40 INFO BlockManager: Removing RDD 607
17/04/17 14:20:40 INFO MapPartitionsRDD: Removing RDD 606 from persistence list
17/04/17 14:20:40 INFO BlockManager: Removing RDD 606
17/04/17 14:20:40 INFO BlockRDD: Removing RDD 605 from persistence list
17/04/17 14:20:40 INFO BlockManager: Removing RDD 605
17/04/17 14:20:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[605] at createStream at KafkaConsumer.java:64 of time 1492419040000 ms
17/04/17 14:20:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419020000 ms)
17/04/17 14:20:40 INFO InputInfoTracker: remove old batch metadata: 1492419020000 ms
17/04/17 14:20:50 INFO JobScheduler: Starting job streaming job 1492419050000 ms.0 from job set of time 1492419050000 ms
-------------------------------------------
Time: 1492419050000 ms
-------------------------------------------

17/04/17 14:20:50 INFO JobScheduler: Added jobs for time 1492419050000 ms
17/04/17 14:20:50 INFO JobScheduler: Finished job streaming job 1492419050000 ms.0 from job set of time 1492419050000 ms
17/04/17 14:20:50 INFO MapPartitionsRDD: Removing RDD 610 from persistence list
17/04/17 14:20:50 INFO JobScheduler: Total delay: 0.006 s for time 1492419050000 ms (execution: 0.001 s)
17/04/17 14:20:50 INFO BlockManager: Removing RDD 610
17/04/17 14:20:50 INFO MapPartitionsRDD: Removing RDD 609 from persistence list
17/04/17 14:20:50 INFO BlockManager: Removing RDD 609
17/04/17 14:20:50 INFO BlockRDD: Removing RDD 608 from persistence list
17/04/17 14:20:50 INFO BlockManager: Removing RDD 608
17/04/17 14:20:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[608] at createStream at KafkaConsumer.java:64 of time 1492419050000 ms
17/04/17 14:20:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419030000 ms)
17/04/17 14:20:50 INFO InputInfoTracker: remove old batch metadata: 1492419030000 ms
17/04/17 14:21:00 INFO JobScheduler: Starting job streaming job 1492419060000 ms.0 from job set of time 1492419060000 ms
-------------------------------------------
Time: 1492419060000 ms
-------------------------------------------

17/04/17 14:21:00 INFO JobScheduler: Added jobs for time 1492419060000 ms
17/04/17 14:21:00 INFO JobScheduler: Finished job streaming job 1492419060000 ms.0 from job set of time 1492419060000 ms
17/04/17 14:21:00 INFO MapPartitionsRDD: Removing RDD 613 from persistence list
17/04/17 14:21:00 INFO JobScheduler: Total delay: 0.006 s for time 1492419060000 ms (execution: 0.001 s)
17/04/17 14:21:00 INFO MapPartitionsRDD: Removing RDD 612 from persistence list
17/04/17 14:21:00 INFO BlockManager: Removing RDD 613
17/04/17 14:21:00 INFO BlockManager: Removing RDD 612
17/04/17 14:21:00 INFO BlockRDD: Removing RDD 611 from persistence list
17/04/17 14:21:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[611] at createStream at KafkaConsumer.java:64 of time 1492419060000 ms
17/04/17 14:21:00 INFO BlockManager: Removing RDD 611
17/04/17 14:21:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419040000 ms)
17/04/17 14:21:00 INFO InputInfoTracker: remove old batch metadata: 1492419040000 ms
17/04/17 14:21:10 INFO JobScheduler: Starting job streaming job 1492419070000 ms.0 from job set of time 1492419070000 ms
-------------------------------------------
Time: 1492419070000 ms
-------------------------------------------

17/04/17 14:21:10 INFO JobScheduler: Added jobs for time 1492419070000 ms
17/04/17 14:21:10 INFO JobScheduler: Finished job streaming job 1492419070000 ms.0 from job set of time 1492419070000 ms
17/04/17 14:21:10 INFO MapPartitionsRDD: Removing RDD 616 from persistence list
17/04/17 14:21:10 INFO JobScheduler: Total delay: 0.007 s for time 1492419070000 ms (execution: 0.001 s)
17/04/17 14:21:10 INFO BlockManager: Removing RDD 616
17/04/17 14:21:10 INFO MapPartitionsRDD: Removing RDD 615 from persistence list
17/04/17 14:21:10 INFO BlockManager: Removing RDD 615
17/04/17 14:21:10 INFO BlockRDD: Removing RDD 614 from persistence list
17/04/17 14:21:10 INFO BlockManager: Removing RDD 614
17/04/17 14:21:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[614] at createStream at KafkaConsumer.java:64 of time 1492419070000 ms
17/04/17 14:21:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419050000 ms)
17/04/17 14:21:10 INFO InputInfoTracker: remove old batch metadata: 1492419050000 ms
17/04/17 14:21:20 INFO JobScheduler: Added jobs for time 1492419080000 ms
17/04/17 14:21:20 INFO JobScheduler: Starting job streaming job 1492419080000 ms.0 from job set of time 1492419080000 ms
-------------------------------------------
Time: 1492419080000 ms
-------------------------------------------

17/04/17 14:21:20 INFO JobScheduler: Finished job streaming job 1492419080000 ms.0 from job set of time 1492419080000 ms
17/04/17 14:21:20 INFO MapPartitionsRDD: Removing RDD 619 from persistence list
17/04/17 14:21:20 INFO JobScheduler: Total delay: 0.004 s for time 1492419080000 ms (execution: 0.000 s)
17/04/17 14:21:20 INFO MapPartitionsRDD: Removing RDD 618 from persistence list
17/04/17 14:21:20 INFO BlockManager: Removing RDD 619
17/04/17 14:21:20 INFO BlockManager: Removing RDD 618
17/04/17 14:21:20 INFO BlockRDD: Removing RDD 617 from persistence list
17/04/17 14:21:20 INFO BlockManager: Removing RDD 617
17/04/17 14:21:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[617] at createStream at KafkaConsumer.java:64 of time 1492419080000 ms
17/04/17 14:21:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419060000 ms)
17/04/17 14:21:20 INFO InputInfoTracker: remove old batch metadata: 1492419060000 ms
17/04/17 14:21:30 INFO JobScheduler: Added jobs for time 1492419090000 ms
17/04/17 14:21:30 INFO JobScheduler: Starting job streaming job 1492419090000 ms.0 from job set of time 1492419090000 ms
-------------------------------------------
Time: 1492419090000 ms
-------------------------------------------

17/04/17 14:21:30 INFO JobScheduler: Finished job streaming job 1492419090000 ms.0 from job set of time 1492419090000 ms
17/04/17 14:21:30 INFO MapPartitionsRDD: Removing RDD 622 from persistence list
17/04/17 14:21:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419090000 ms (execution: 0.001 s)
17/04/17 14:21:30 INFO BlockManager: Removing RDD 622
17/04/17 14:21:30 INFO MapPartitionsRDD: Removing RDD 621 from persistence list
17/04/17 14:21:30 INFO BlockManager: Removing RDD 621
17/04/17 14:21:30 INFO BlockRDD: Removing RDD 620 from persistence list
17/04/17 14:21:30 INFO BlockManager: Removing RDD 620
17/04/17 14:21:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[620] at createStream at KafkaConsumer.java:64 of time 1492419090000 ms
17/04/17 14:21:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419070000 ms)
17/04/17 14:21:30 INFO InputInfoTracker: remove old batch metadata: 1492419070000 ms
17/04/17 14:21:40 INFO JobScheduler: Added jobs for time 1492419100000 ms
17/04/17 14:21:40 INFO JobScheduler: Starting job streaming job 1492419100000 ms.0 from job set of time 1492419100000 ms
-------------------------------------------
Time: 1492419100000 ms
-------------------------------------------

17/04/17 14:21:40 INFO JobScheduler: Finished job streaming job 1492419100000 ms.0 from job set of time 1492419100000 ms
17/04/17 14:21:40 INFO MapPartitionsRDD: Removing RDD 625 from persistence list
17/04/17 14:21:40 INFO JobScheduler: Total delay: 0.006 s for time 1492419100000 ms (execution: 0.001 s)
17/04/17 14:21:40 INFO BlockManager: Removing RDD 625
17/04/17 14:21:40 INFO MapPartitionsRDD: Removing RDD 624 from persistence list
17/04/17 14:21:40 INFO BlockManager: Removing RDD 624
17/04/17 14:21:40 INFO BlockRDD: Removing RDD 623 from persistence list
17/04/17 14:21:40 INFO BlockManager: Removing RDD 623
17/04/17 14:21:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[623] at createStream at KafkaConsumer.java:64 of time 1492419100000 ms
17/04/17 14:21:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419080000 ms)
17/04/17 14:21:40 INFO InputInfoTracker: remove old batch metadata: 1492419080000 ms
17/04/17 14:21:50 INFO JobScheduler: Added jobs for time 1492419110000 ms
17/04/17 14:21:50 INFO JobScheduler: Starting job streaming job 1492419110000 ms.0 from job set of time 1492419110000 ms
-------------------------------------------
Time: 1492419110000 ms
-------------------------------------------

17/04/17 14:21:50 INFO JobScheduler: Finished job streaming job 1492419110000 ms.0 from job set of time 1492419110000 ms
17/04/17 14:21:50 INFO JobScheduler: Total delay: 0.004 s for time 1492419110000 ms (execution: 0.000 s)
17/04/17 14:21:50 INFO MapPartitionsRDD: Removing RDD 628 from persistence list
17/04/17 14:21:50 INFO BlockManager: Removing RDD 628
17/04/17 14:21:50 INFO MapPartitionsRDD: Removing RDD 627 from persistence list
17/04/17 14:21:50 INFO BlockManager: Removing RDD 627
17/04/17 14:21:50 INFO BlockRDD: Removing RDD 626 from persistence list
17/04/17 14:21:50 INFO BlockManager: Removing RDD 626
17/04/17 14:21:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[626] at createStream at KafkaConsumer.java:64 of time 1492419110000 ms
17/04/17 14:21:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419090000 ms)
17/04/17 14:21:50 INFO InputInfoTracker: remove old batch metadata: 1492419090000 ms
17/04/17 14:22:00 INFO JobScheduler: Added jobs for time 1492419120000 ms
17/04/17 14:22:00 INFO JobScheduler: Starting job streaming job 1492419120000 ms.0 from job set of time 1492419120000 ms
-------------------------------------------
Time: 1492419120000 ms
-------------------------------------------

17/04/17 14:22:00 INFO JobScheduler: Finished job streaming job 1492419120000 ms.0 from job set of time 1492419120000 ms
17/04/17 14:22:00 INFO MapPartitionsRDD: Removing RDD 631 from persistence list
17/04/17 14:22:00 INFO JobScheduler: Total delay: 0.006 s for time 1492419120000 ms (execution: 0.000 s)
17/04/17 14:22:00 INFO BlockManager: Removing RDD 631
17/04/17 14:22:00 INFO MapPartitionsRDD: Removing RDD 630 from persistence list
17/04/17 14:22:00 INFO BlockManager: Removing RDD 630
17/04/17 14:22:00 INFO BlockRDD: Removing RDD 629 from persistence list
17/04/17 14:22:00 INFO BlockManager: Removing RDD 629
17/04/17 14:22:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[629] at createStream at KafkaConsumer.java:64 of time 1492419120000 ms
17/04/17 14:22:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419100000 ms)
17/04/17 14:22:00 INFO InputInfoTracker: remove old batch metadata: 1492419100000 ms
17/04/17 14:22:10 INFO JobScheduler: Added jobs for time 1492419130000 ms
17/04/17 14:22:10 INFO JobScheduler: Starting job streaming job 1492419130000 ms.0 from job set of time 1492419130000 ms
-------------------------------------------
Time: 1492419130000 ms
-------------------------------------------

17/04/17 14:22:10 INFO JobScheduler: Finished job streaming job 1492419130000 ms.0 from job set of time 1492419130000 ms
17/04/17 14:22:10 INFO MapPartitionsRDD: Removing RDD 634 from persistence list
17/04/17 14:22:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419130000 ms (execution: 0.000 s)
17/04/17 14:22:10 INFO BlockManager: Removing RDD 634
17/04/17 14:22:10 INFO MapPartitionsRDD: Removing RDD 633 from persistence list
17/04/17 14:22:10 INFO BlockManager: Removing RDD 633
17/04/17 14:22:10 INFO BlockRDD: Removing RDD 632 from persistence list
17/04/17 14:22:10 INFO BlockManager: Removing RDD 632
17/04/17 14:22:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[632] at createStream at KafkaConsumer.java:64 of time 1492419130000 ms
17/04/17 14:22:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419110000 ms)
17/04/17 14:22:10 INFO InputInfoTracker: remove old batch metadata: 1492419110000 ms
17/04/17 14:22:20 INFO JobScheduler: Added jobs for time 1492419140000 ms
-------------------------------------------
Time: 1492419140000 ms
-------------------------------------------

17/04/17 14:22:20 INFO JobScheduler: Starting job streaming job 1492419140000 ms.0 from job set of time 1492419140000 ms
17/04/17 14:22:20 INFO JobScheduler: Finished job streaming job 1492419140000 ms.0 from job set of time 1492419140000 ms
17/04/17 14:22:20 INFO MapPartitionsRDD: Removing RDD 637 from persistence list
17/04/17 14:22:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419140000 ms (execution: 0.000 s)
17/04/17 14:22:20 INFO BlockManager: Removing RDD 637
17/04/17 14:22:20 INFO MapPartitionsRDD: Removing RDD 636 from persistence list
17/04/17 14:22:20 INFO BlockManager: Removing RDD 636
17/04/17 14:22:20 INFO BlockRDD: Removing RDD 635 from persistence list
17/04/17 14:22:20 INFO BlockManager: Removing RDD 635
17/04/17 14:22:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[635] at createStream at KafkaConsumer.java:64 of time 1492419140000 ms
17/04/17 14:22:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419120000 ms)
17/04/17 14:22:20 INFO InputInfoTracker: remove old batch metadata: 1492419120000 ms
17/04/17 14:22:30 INFO JobScheduler: Added jobs for time 1492419150000 ms
17/04/17 14:22:30 INFO JobScheduler: Starting job streaming job 1492419150000 ms.0 from job set of time 1492419150000 ms
-------------------------------------------
Time: 1492419150000 ms
-------------------------------------------

17/04/17 14:22:30 INFO JobScheduler: Finished job streaming job 1492419150000 ms.0 from job set of time 1492419150000 ms
17/04/17 14:22:30 INFO MapPartitionsRDD: Removing RDD 640 from persistence list
17/04/17 14:22:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419150000 ms (execution: 0.001 s)
17/04/17 14:22:30 INFO MapPartitionsRDD: Removing RDD 639 from persistence list
17/04/17 14:22:30 INFO BlockManager: Removing RDD 640
17/04/17 14:22:30 INFO BlockManager: Removing RDD 639
17/04/17 14:22:30 INFO BlockRDD: Removing RDD 638 from persistence list
17/04/17 14:22:30 INFO BlockManager: Removing RDD 638
17/04/17 14:22:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[638] at createStream at KafkaConsumer.java:64 of time 1492419150000 ms
17/04/17 14:22:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419130000 ms)
17/04/17 14:22:30 INFO InputInfoTracker: remove old batch metadata: 1492419130000 ms
17/04/17 14:22:40 INFO JobScheduler: Added jobs for time 1492419160000 ms
-------------------------------------------
Time: 1492419160000 ms
-------------------------------------------

17/04/17 14:22:40 INFO JobScheduler: Starting job streaming job 1492419160000 ms.0 from job set of time 1492419160000 ms
17/04/17 14:22:40 INFO JobScheduler: Finished job streaming job 1492419160000 ms.0 from job set of time 1492419160000 ms
17/04/17 14:22:40 INFO MapPartitionsRDD: Removing RDD 643 from persistence list
17/04/17 14:22:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419160000 ms (execution: 0.000 s)
17/04/17 14:22:40 INFO MapPartitionsRDD: Removing RDD 642 from persistence list
17/04/17 14:22:40 INFO BlockManager: Removing RDD 643
17/04/17 14:22:40 INFO BlockManager: Removing RDD 642
17/04/17 14:22:40 INFO BlockRDD: Removing RDD 641 from persistence list
17/04/17 14:22:40 INFO BlockManager: Removing RDD 641
17/04/17 14:22:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[641] at createStream at KafkaConsumer.java:64 of time 1492419160000 ms
17/04/17 14:22:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419140000 ms)
17/04/17 14:22:40 INFO InputInfoTracker: remove old batch metadata: 1492419140000 ms
17/04/17 14:22:50 INFO JobScheduler: Added jobs for time 1492419170000 ms
17/04/17 14:22:50 INFO JobScheduler: Starting job streaming job 1492419170000 ms.0 from job set of time 1492419170000 ms
-------------------------------------------
Time: 1492419170000 ms
-------------------------------------------

17/04/17 14:22:50 INFO JobScheduler: Finished job streaming job 1492419170000 ms.0 from job set of time 1492419170000 ms
17/04/17 14:22:50 INFO MapPartitionsRDD: Removing RDD 646 from persistence list
17/04/17 14:22:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419170000 ms (execution: 0.001 s)
17/04/17 14:22:50 INFO BlockManager: Removing RDD 646
17/04/17 14:22:50 INFO MapPartitionsRDD: Removing RDD 645 from persistence list
17/04/17 14:22:50 INFO BlockManager: Removing RDD 645
17/04/17 14:22:50 INFO BlockRDD: Removing RDD 644 from persistence list
17/04/17 14:22:50 INFO BlockManager: Removing RDD 644
17/04/17 14:22:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[644] at createStream at KafkaConsumer.java:64 of time 1492419170000 ms
17/04/17 14:22:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419150000 ms)
17/04/17 14:22:50 INFO InputInfoTracker: remove old batch metadata: 1492419150000 ms
17/04/17 14:23:00 INFO JobScheduler: Added jobs for time 1492419180000 ms
17/04/17 14:23:00 INFO JobScheduler: Starting job streaming job 1492419180000 ms.0 from job set of time 1492419180000 ms
-------------------------------------------
Time: 1492419180000 ms
-------------------------------------------

17/04/17 14:23:00 INFO JobScheduler: Finished job streaming job 1492419180000 ms.0 from job set of time 1492419180000 ms
17/04/17 14:23:00 INFO MapPartitionsRDD: Removing RDD 649 from persistence list
17/04/17 14:23:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419180000 ms (execution: 0.000 s)
17/04/17 14:23:00 INFO MapPartitionsRDD: Removing RDD 648 from persistence list
17/04/17 14:23:00 INFO BlockManager: Removing RDD 649
17/04/17 14:23:00 INFO BlockRDD: Removing RDD 647 from persistence list
17/04/17 14:23:00 INFO BlockManager: Removing RDD 648
17/04/17 14:23:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[647] at createStream at KafkaConsumer.java:64 of time 1492419180000 ms
17/04/17 14:23:00 INFO BlockManager: Removing RDD 647
17/04/17 14:23:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419160000 ms)
17/04/17 14:23:00 INFO InputInfoTracker: remove old batch metadata: 1492419160000 ms
17/04/17 14:23:10 INFO JobScheduler: Added jobs for time 1492419190000 ms
17/04/17 14:23:10 INFO JobScheduler: Starting job streaming job 1492419190000 ms.0 from job set of time 1492419190000 ms
-------------------------------------------
Time: 1492419190000 ms
-------------------------------------------

17/04/17 14:23:10 INFO JobScheduler: Finished job streaming job 1492419190000 ms.0 from job set of time 1492419190000 ms
17/04/17 14:23:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419190000 ms (execution: 0.000 s)
17/04/17 14:23:10 INFO MapPartitionsRDD: Removing RDD 652 from persistence list
17/04/17 14:23:10 INFO MapPartitionsRDD: Removing RDD 651 from persistence list
17/04/17 14:23:10 INFO BlockManager: Removing RDD 652
17/04/17 14:23:10 INFO BlockManager: Removing RDD 651
17/04/17 14:23:10 INFO BlockRDD: Removing RDD 650 from persistence list
17/04/17 14:23:10 INFO BlockManager: Removing RDD 650
17/04/17 14:23:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[650] at createStream at KafkaConsumer.java:64 of time 1492419190000 ms
17/04/17 14:23:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419170000 ms)
17/04/17 14:23:10 INFO InputInfoTracker: remove old batch metadata: 1492419170000 ms
17/04/17 14:23:20 INFO JobScheduler: Added jobs for time 1492419200000 ms
17/04/17 14:23:20 INFO JobScheduler: Starting job streaming job 1492419200000 ms.0 from job set of time 1492419200000 ms
-------------------------------------------
Time: 1492419200000 ms
-------------------------------------------

17/04/17 14:23:20 INFO JobScheduler: Finished job streaming job 1492419200000 ms.0 from job set of time 1492419200000 ms
17/04/17 14:23:20 INFO MapPartitionsRDD: Removing RDD 655 from persistence list
17/04/17 14:23:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419200000 ms (execution: 0.001 s)
17/04/17 14:23:20 INFO BlockManager: Removing RDD 655
17/04/17 14:23:20 INFO MapPartitionsRDD: Removing RDD 654 from persistence list
17/04/17 14:23:20 INFO BlockManager: Removing RDD 654
17/04/17 14:23:20 INFO BlockRDD: Removing RDD 653 from persistence list
17/04/17 14:23:20 INFO BlockManager: Removing RDD 653
17/04/17 14:23:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[653] at createStream at KafkaConsumer.java:64 of time 1492419200000 ms
17/04/17 14:23:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419180000 ms)
17/04/17 14:23:20 INFO InputInfoTracker: remove old batch metadata: 1492419180000 ms
17/04/17 14:23:30 INFO JobScheduler: Added jobs for time 1492419210000 ms
17/04/17 14:23:30 INFO JobScheduler: Starting job streaming job 1492419210000 ms.0 from job set of time 1492419210000 ms
-------------------------------------------
Time: 1492419210000 ms
-------------------------------------------

17/04/17 14:23:30 INFO JobScheduler: Finished job streaming job 1492419210000 ms.0 from job set of time 1492419210000 ms
17/04/17 14:23:30 INFO JobScheduler: Total delay: 0.004 s for time 1492419210000 ms (execution: 0.000 s)
17/04/17 14:23:30 INFO MapPartitionsRDD: Removing RDD 658 from persistence list
17/04/17 14:23:30 INFO BlockManager: Removing RDD 658
17/04/17 14:23:30 INFO MapPartitionsRDD: Removing RDD 657 from persistence list
17/04/17 14:23:30 INFO BlockManager: Removing RDD 657
17/04/17 14:23:30 INFO BlockRDD: Removing RDD 656 from persistence list
17/04/17 14:23:30 INFO BlockManager: Removing RDD 656
17/04/17 14:23:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[656] at createStream at KafkaConsumer.java:64 of time 1492419210000 ms
17/04/17 14:23:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419190000 ms)
17/04/17 14:23:30 INFO InputInfoTracker: remove old batch metadata: 1492419190000 ms
17/04/17 14:23:40 INFO JobScheduler: Added jobs for time 1492419220000 ms
17/04/17 14:23:40 INFO JobScheduler: Starting job streaming job 1492419220000 ms.0 from job set of time 1492419220000 ms
-------------------------------------------
Time: 1492419220000 ms
-------------------------------------------

17/04/17 14:23:40 INFO JobScheduler: Finished job streaming job 1492419220000 ms.0 from job set of time 1492419220000 ms
17/04/17 14:23:40 INFO MapPartitionsRDD: Removing RDD 661 from persistence list
17/04/17 14:23:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419220000 ms (execution: 0.001 s)
17/04/17 14:23:40 INFO BlockManager: Removing RDD 661
17/04/17 14:23:40 INFO MapPartitionsRDD: Removing RDD 660 from persistence list
17/04/17 14:23:40 INFO BlockManager: Removing RDD 660
17/04/17 14:23:40 INFO BlockRDD: Removing RDD 659 from persistence list
17/04/17 14:23:40 INFO BlockManager: Removing RDD 659
17/04/17 14:23:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[659] at createStream at KafkaConsumer.java:64 of time 1492419220000 ms
17/04/17 14:23:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419200000 ms)
17/04/17 14:23:40 INFO InputInfoTracker: remove old batch metadata: 1492419200000 ms
17/04/17 14:23:50 INFO JobScheduler: Added jobs for time 1492419230000 ms
17/04/17 14:23:50 INFO JobScheduler: Starting job streaming job 1492419230000 ms.0 from job set of time 1492419230000 ms
-------------------------------------------
Time: 1492419230000 ms
-------------------------------------------

17/04/17 14:23:50 INFO JobScheduler: Finished job streaming job 1492419230000 ms.0 from job set of time 1492419230000 ms
17/04/17 14:23:50 INFO MapPartitionsRDD: Removing RDD 664 from persistence list
17/04/17 14:23:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419230000 ms (execution: 0.001 s)
17/04/17 14:23:50 INFO BlockManager: Removing RDD 664
17/04/17 14:23:50 INFO MapPartitionsRDD: Removing RDD 663 from persistence list
17/04/17 14:23:50 INFO BlockManager: Removing RDD 663
17/04/17 14:23:50 INFO BlockRDD: Removing RDD 662 from persistence list
17/04/17 14:23:50 INFO BlockManager: Removing RDD 662
17/04/17 14:23:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[662] at createStream at KafkaConsumer.java:64 of time 1492419230000 ms
17/04/17 14:23:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419210000 ms)
17/04/17 14:23:50 INFO InputInfoTracker: remove old batch metadata: 1492419210000 ms
17/04/17 14:24:00 INFO JobScheduler: Added jobs for time 1492419240000 ms
17/04/17 14:24:00 INFO JobScheduler: Starting job streaming job 1492419240000 ms.0 from job set of time 1492419240000 ms
-------------------------------------------
Time: 1492419240000 ms
-------------------------------------------

17/04/17 14:24:00 INFO JobScheduler: Finished job streaming job 1492419240000 ms.0 from job set of time 1492419240000 ms
17/04/17 14:24:00 INFO MapPartitionsRDD: Removing RDD 667 from persistence list
17/04/17 14:24:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419240000 ms (execution: 0.000 s)
17/04/17 14:24:00 INFO BlockManager: Removing RDD 667
17/04/17 14:24:00 INFO MapPartitionsRDD: Removing RDD 666 from persistence list
17/04/17 14:24:00 INFO BlockManager: Removing RDD 666
17/04/17 14:24:00 INFO BlockRDD: Removing RDD 665 from persistence list
17/04/17 14:24:00 INFO BlockManager: Removing RDD 665
17/04/17 14:24:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[665] at createStream at KafkaConsumer.java:64 of time 1492419240000 ms
17/04/17 14:24:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419220000 ms)
17/04/17 14:24:00 INFO InputInfoTracker: remove old batch metadata: 1492419220000 ms
17/04/17 14:24:10 INFO JobScheduler: Added jobs for time 1492419250000 ms
17/04/17 14:24:10 INFO JobScheduler: Starting job streaming job 1492419250000 ms.0 from job set of time 1492419250000 ms
-------------------------------------------
Time: 1492419250000 ms
-------------------------------------------

17/04/17 14:24:10 INFO JobScheduler: Finished job streaming job 1492419250000 ms.0 from job set of time 1492419250000 ms
17/04/17 14:24:10 INFO MapPartitionsRDD: Removing RDD 670 from persistence list
17/04/17 14:24:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419250000 ms (execution: 0.000 s)
17/04/17 14:24:10 INFO BlockManager: Removing RDD 670
17/04/17 14:24:10 INFO MapPartitionsRDD: Removing RDD 669 from persistence list
17/04/17 14:24:10 INFO BlockManager: Removing RDD 669
17/04/17 14:24:10 INFO BlockRDD: Removing RDD 668 from persistence list
17/04/17 14:24:10 INFO BlockManager: Removing RDD 668
17/04/17 14:24:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[668] at createStream at KafkaConsumer.java:64 of time 1492419250000 ms
17/04/17 14:24:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419230000 ms)
17/04/17 14:24:10 INFO InputInfoTracker: remove old batch metadata: 1492419230000 ms
17/04/17 14:24:20 INFO JobScheduler: Added jobs for time 1492419260000 ms
17/04/17 14:24:20 INFO JobScheduler: Starting job streaming job 1492419260000 ms.0 from job set of time 1492419260000 ms
-------------------------------------------
Time: 1492419260000 ms
-------------------------------------------

17/04/17 14:24:20 INFO JobScheduler: Finished job streaming job 1492419260000 ms.0 from job set of time 1492419260000 ms
17/04/17 14:24:20 INFO MapPartitionsRDD: Removing RDD 673 from persistence list
17/04/17 14:24:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419260000 ms (execution: 0.001 s)
17/04/17 14:24:20 INFO BlockManager: Removing RDD 673
17/04/17 14:24:20 INFO MapPartitionsRDD: Removing RDD 672 from persistence list
17/04/17 14:24:20 INFO BlockManager: Removing RDD 672
17/04/17 14:24:20 INFO BlockRDD: Removing RDD 671 from persistence list
17/04/17 14:24:20 INFO BlockManager: Removing RDD 671
17/04/17 14:24:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[671] at createStream at KafkaConsumer.java:64 of time 1492419260000 ms
17/04/17 14:24:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419240000 ms)
17/04/17 14:24:20 INFO InputInfoTracker: remove old batch metadata: 1492419240000 ms
17/04/17 14:24:30 INFO JobScheduler: Added jobs for time 1492419270000 ms
17/04/17 14:24:30 INFO JobScheduler: Starting job streaming job 1492419270000 ms.0 from job set of time 1492419270000 ms
-------------------------------------------
Time: 1492419270000 ms
-------------------------------------------

17/04/17 14:24:30 INFO JobScheduler: Finished job streaming job 1492419270000 ms.0 from job set of time 1492419270000 ms
17/04/17 14:24:30 INFO MapPartitionsRDD: Removing RDD 676 from persistence list
17/04/17 14:24:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419270000 ms (execution: 0.000 s)
17/04/17 14:24:30 INFO BlockManager: Removing RDD 676
17/04/17 14:24:30 INFO MapPartitionsRDD: Removing RDD 675 from persistence list
17/04/17 14:24:30 INFO BlockManager: Removing RDD 675
17/04/17 14:24:30 INFO BlockRDD: Removing RDD 674 from persistence list
17/04/17 14:24:30 INFO BlockManager: Removing RDD 674
17/04/17 14:24:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[674] at createStream at KafkaConsumer.java:64 of time 1492419270000 ms
17/04/17 14:24:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419250000 ms)
17/04/17 14:24:30 INFO InputInfoTracker: remove old batch metadata: 1492419250000 ms
17/04/17 14:24:40 INFO JobScheduler: Added jobs for time 1492419280000 ms
17/04/17 14:24:40 INFO JobScheduler: Starting job streaming job 1492419280000 ms.0 from job set of time 1492419280000 ms
-------------------------------------------
Time: 1492419280000 ms
-------------------------------------------

17/04/17 14:24:40 INFO JobScheduler: Finished job streaming job 1492419280000 ms.0 from job set of time 1492419280000 ms
17/04/17 14:24:40 INFO MapPartitionsRDD: Removing RDD 679 from persistence list
17/04/17 14:24:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419280000 ms (execution: 0.001 s)
17/04/17 14:24:40 INFO BlockManager: Removing RDD 679
17/04/17 14:24:40 INFO MapPartitionsRDD: Removing RDD 678 from persistence list
17/04/17 14:24:40 INFO BlockManager: Removing RDD 678
17/04/17 14:24:40 INFO BlockRDD: Removing RDD 677 from persistence list
17/04/17 14:24:40 INFO BlockManager: Removing RDD 677
17/04/17 14:24:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[677] at createStream at KafkaConsumer.java:64 of time 1492419280000 ms
17/04/17 14:24:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419260000 ms)
17/04/17 14:24:40 INFO InputInfoTracker: remove old batch metadata: 1492419260000 ms
17/04/17 14:24:50 INFO JobScheduler: Added jobs for time 1492419290000 ms
17/04/17 14:24:50 INFO JobScheduler: Starting job streaming job 1492419290000 ms.0 from job set of time 1492419290000 ms
-------------------------------------------
Time: 1492419290000 ms
-------------------------------------------

17/04/17 14:24:50 INFO JobScheduler: Finished job streaming job 1492419290000 ms.0 from job set of time 1492419290000 ms
17/04/17 14:24:50 INFO MapPartitionsRDD: Removing RDD 682 from persistence list
17/04/17 14:24:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419290000 ms (execution: 0.000 s)
17/04/17 14:24:50 INFO BlockManager: Removing RDD 682
17/04/17 14:24:50 INFO MapPartitionsRDD: Removing RDD 681 from persistence list
17/04/17 14:24:50 INFO BlockManager: Removing RDD 681
17/04/17 14:24:50 INFO BlockRDD: Removing RDD 680 from persistence list
17/04/17 14:24:50 INFO BlockManager: Removing RDD 680
17/04/17 14:24:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[680] at createStream at KafkaConsumer.java:64 of time 1492419290000 ms
17/04/17 14:24:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419270000 ms)
17/04/17 14:24:50 INFO InputInfoTracker: remove old batch metadata: 1492419270000 ms
17/04/17 14:25:00 INFO JobScheduler: Added jobs for time 1492419300000 ms
17/04/17 14:25:00 INFO JobScheduler: Starting job streaming job 1492419300000 ms.0 from job set of time 1492419300000 ms
-------------------------------------------
Time: 1492419300000 ms
-------------------------------------------

17/04/17 14:25:00 INFO JobScheduler: Finished job streaming job 1492419300000 ms.0 from job set of time 1492419300000 ms
17/04/17 14:25:00 INFO MapPartitionsRDD: Removing RDD 685 from persistence list
17/04/17 14:25:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419300000 ms (execution: 0.000 s)
17/04/17 14:25:00 INFO MapPartitionsRDD: Removing RDD 684 from persistence list
17/04/17 14:25:00 INFO BlockManager: Removing RDD 685
17/04/17 14:25:00 INFO BlockManager: Removing RDD 684
17/04/17 14:25:00 INFO BlockRDD: Removing RDD 683 from persistence list
17/04/17 14:25:00 INFO BlockManager: Removing RDD 683
17/04/17 14:25:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[683] at createStream at KafkaConsumer.java:64 of time 1492419300000 ms
17/04/17 14:25:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419280000 ms)
17/04/17 14:25:00 INFO InputInfoTracker: remove old batch metadata: 1492419280000 ms
17/04/17 14:25:10 INFO JobScheduler: Starting job streaming job 1492419310000 ms.0 from job set of time 1492419310000 ms
-------------------------------------------
Time: 1492419310000 ms
-------------------------------------------

17/04/17 14:25:10 INFO JobScheduler: Added jobs for time 1492419310000 ms
17/04/17 14:25:10 INFO JobScheduler: Finished job streaming job 1492419310000 ms.0 from job set of time 1492419310000 ms
17/04/17 14:25:10 INFO MapPartitionsRDD: Removing RDD 688 from persistence list
17/04/17 14:25:10 INFO JobScheduler: Total delay: 0.006 s for time 1492419310000 ms (execution: 0.001 s)
17/04/17 14:25:10 INFO BlockManager: Removing RDD 688
17/04/17 14:25:10 INFO MapPartitionsRDD: Removing RDD 687 from persistence list
17/04/17 14:25:10 INFO BlockManager: Removing RDD 687
17/04/17 14:25:10 INFO BlockRDD: Removing RDD 686 from persistence list
17/04/17 14:25:10 INFO BlockManager: Removing RDD 686
17/04/17 14:25:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[686] at createStream at KafkaConsumer.java:64 of time 1492419310000 ms
17/04/17 14:25:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419290000 ms)
17/04/17 14:25:10 INFO InputInfoTracker: remove old batch metadata: 1492419290000 ms
17/04/17 14:25:20 INFO JobScheduler: Added jobs for time 1492419320000 ms
17/04/17 14:25:20 INFO JobScheduler: Starting job streaming job 1492419320000 ms.0 from job set of time 1492419320000 ms
-------------------------------------------
Time: 1492419320000 ms
-------------------------------------------

17/04/17 14:25:20 INFO JobScheduler: Finished job streaming job 1492419320000 ms.0 from job set of time 1492419320000 ms
17/04/17 14:25:20 INFO MapPartitionsRDD: Removing RDD 691 from persistence list
17/04/17 14:25:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419320000 ms (execution: 0.001 s)
17/04/17 14:25:20 INFO MapPartitionsRDD: Removing RDD 690 from persistence list
17/04/17 14:25:20 INFO BlockManager: Removing RDD 690
17/04/17 14:25:20 INFO BlockRDD: Removing RDD 689 from persistence list
17/04/17 14:25:20 INFO BlockManager: Removing RDD 691
17/04/17 14:25:20 INFO BlockManager: Removing RDD 689
17/04/17 14:25:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[689] at createStream at KafkaConsumer.java:64 of time 1492419320000 ms
17/04/17 14:25:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419300000 ms)
17/04/17 14:25:20 INFO InputInfoTracker: remove old batch metadata: 1492419300000 ms
17/04/17 14:25:30 INFO JobScheduler: Added jobs for time 1492419330000 ms
-------------------------------------------
Time: 1492419330000 ms
-------------------------------------------

17/04/17 14:25:30 INFO JobScheduler: Starting job streaming job 1492419330000 ms.0 from job set of time 1492419330000 ms
17/04/17 14:25:30 INFO JobScheduler: Finished job streaming job 1492419330000 ms.0 from job set of time 1492419330000 ms
17/04/17 14:25:30 INFO MapPartitionsRDD: Removing RDD 694 from persistence list
17/04/17 14:25:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419330000 ms (execution: 0.000 s)
17/04/17 14:25:30 INFO BlockManager: Removing RDD 694
17/04/17 14:25:30 INFO MapPartitionsRDD: Removing RDD 693 from persistence list
17/04/17 14:25:30 INFO BlockManager: Removing RDD 693
17/04/17 14:25:30 INFO BlockRDD: Removing RDD 692 from persistence list
17/04/17 14:25:30 INFO BlockManager: Removing RDD 692
17/04/17 14:25:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[692] at createStream at KafkaConsumer.java:64 of time 1492419330000 ms
17/04/17 14:25:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419310000 ms)
17/04/17 14:25:30 INFO InputInfoTracker: remove old batch metadata: 1492419310000 ms
17/04/17 14:25:40 INFO JobScheduler: Added jobs for time 1492419340000 ms
17/04/17 14:25:40 INFO JobScheduler: Starting job streaming job 1492419340000 ms.0 from job set of time 1492419340000 ms
-------------------------------------------
Time: 1492419340000 ms
-------------------------------------------

17/04/17 14:25:40 INFO JobScheduler: Finished job streaming job 1492419340000 ms.0 from job set of time 1492419340000 ms
17/04/17 14:25:40 INFO MapPartitionsRDD: Removing RDD 697 from persistence list
17/04/17 14:25:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419340000 ms (execution: 0.001 s)
17/04/17 14:25:40 INFO BlockManager: Removing RDD 697
17/04/17 14:25:40 INFO MapPartitionsRDD: Removing RDD 696 from persistence list
17/04/17 14:25:40 INFO BlockManager: Removing RDD 696
17/04/17 14:25:40 INFO BlockRDD: Removing RDD 695 from persistence list
17/04/17 14:25:40 INFO BlockManager: Removing RDD 695
17/04/17 14:25:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[695] at createStream at KafkaConsumer.java:64 of time 1492419340000 ms
17/04/17 14:25:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419320000 ms)
17/04/17 14:25:40 INFO InputInfoTracker: remove old batch metadata: 1492419320000 ms
17/04/17 14:25:50 INFO JobScheduler: Added jobs for time 1492419350000 ms
17/04/17 14:25:50 INFO JobScheduler: Starting job streaming job 1492419350000 ms.0 from job set of time 1492419350000 ms
-------------------------------------------
Time: 1492419350000 ms
-------------------------------------------

17/04/17 14:25:50 INFO JobScheduler: Finished job streaming job 1492419350000 ms.0 from job set of time 1492419350000 ms
17/04/17 14:25:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419350000 ms (execution: 0.001 s)
17/04/17 14:25:50 INFO MapPartitionsRDD: Removing RDD 700 from persistence list
17/04/17 14:25:50 INFO BlockManager: Removing RDD 700
17/04/17 14:25:50 INFO MapPartitionsRDD: Removing RDD 699 from persistence list
17/04/17 14:25:50 INFO BlockManager: Removing RDD 699
17/04/17 14:25:50 INFO BlockRDD: Removing RDD 698 from persistence list
17/04/17 14:25:50 INFO BlockManager: Removing RDD 698
17/04/17 14:25:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[698] at createStream at KafkaConsumer.java:64 of time 1492419350000 ms
17/04/17 14:25:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419330000 ms)
17/04/17 14:25:50 INFO InputInfoTracker: remove old batch metadata: 1492419330000 ms
17/04/17 14:26:00 INFO JobScheduler: Added jobs for time 1492419360000 ms
17/04/17 14:26:00 INFO JobScheduler: Starting job streaming job 1492419360000 ms.0 from job set of time 1492419360000 ms
-------------------------------------------
Time: 1492419360000 ms
-------------------------------------------

17/04/17 14:26:00 INFO JobScheduler: Finished job streaming job 1492419360000 ms.0 from job set of time 1492419360000 ms
17/04/17 14:26:00 INFO MapPartitionsRDD: Removing RDD 703 from persistence list
17/04/17 14:26:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419360000 ms (execution: 0.001 s)
17/04/17 14:26:00 INFO BlockManager: Removing RDD 703
17/04/17 14:26:00 INFO MapPartitionsRDD: Removing RDD 702 from persistence list
17/04/17 14:26:00 INFO BlockManager: Removing RDD 702
17/04/17 14:26:00 INFO BlockRDD: Removing RDD 701 from persistence list
17/04/17 14:26:00 INFO BlockManager: Removing RDD 701
17/04/17 14:26:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[701] at createStream at KafkaConsumer.java:64 of time 1492419360000 ms
17/04/17 14:26:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419340000 ms)
17/04/17 14:26:00 INFO InputInfoTracker: remove old batch metadata: 1492419340000 ms
17/04/17 14:26:10 INFO JobScheduler: Added jobs for time 1492419370000 ms
17/04/17 14:26:10 INFO JobScheduler: Starting job streaming job 1492419370000 ms.0 from job set of time 1492419370000 ms
-------------------------------------------
Time: 1492419370000 ms
-------------------------------------------

17/04/17 14:26:10 INFO JobScheduler: Finished job streaming job 1492419370000 ms.0 from job set of time 1492419370000 ms
17/04/17 14:26:10 INFO MapPartitionsRDD: Removing RDD 706 from persistence list
17/04/17 14:26:10 INFO JobScheduler: Total delay: 0.003 s for time 1492419370000 ms (execution: 0.000 s)
17/04/17 14:26:10 INFO BlockManager: Removing RDD 706
17/04/17 14:26:10 INFO MapPartitionsRDD: Removing RDD 705 from persistence list
17/04/17 14:26:10 INFO BlockManager: Removing RDD 705
17/04/17 14:26:10 INFO BlockRDD: Removing RDD 704 from persistence list
17/04/17 14:26:10 INFO BlockManager: Removing RDD 704
17/04/17 14:26:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[704] at createStream at KafkaConsumer.java:64 of time 1492419370000 ms
17/04/17 14:26:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419350000 ms)
17/04/17 14:26:10 INFO InputInfoTracker: remove old batch metadata: 1492419350000 ms
17/04/17 14:26:20 INFO JobScheduler: Added jobs for time 1492419380000 ms
17/04/17 14:26:20 INFO JobScheduler: Starting job streaming job 1492419380000 ms.0 from job set of time 1492419380000 ms
-------------------------------------------
Time: 1492419380000 ms
-------------------------------------------

17/04/17 14:26:20 INFO JobScheduler: Finished job streaming job 1492419380000 ms.0 from job set of time 1492419380000 ms
17/04/17 14:26:20 INFO MapPartitionsRDD: Removing RDD 709 from persistence list
17/04/17 14:26:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419380000 ms (execution: 0.000 s)
17/04/17 14:26:20 INFO BlockManager: Removing RDD 709
17/04/17 14:26:20 INFO MapPartitionsRDD: Removing RDD 708 from persistence list
17/04/17 14:26:20 INFO BlockManager: Removing RDD 708
17/04/17 14:26:20 INFO BlockRDD: Removing RDD 707 from persistence list
17/04/17 14:26:20 INFO BlockManager: Removing RDD 707
17/04/17 14:26:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[707] at createStream at KafkaConsumer.java:64 of time 1492419380000 ms
17/04/17 14:26:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419360000 ms)
17/04/17 14:26:20 INFO InputInfoTracker: remove old batch metadata: 1492419360000 ms
17/04/17 14:26:30 INFO JobScheduler: Added jobs for time 1492419390000 ms
17/04/17 14:26:30 INFO JobScheduler: Starting job streaming job 1492419390000 ms.0 from job set of time 1492419390000 ms
-------------------------------------------
Time: 1492419390000 ms
-------------------------------------------

17/04/17 14:26:30 INFO JobScheduler: Finished job streaming job 1492419390000 ms.0 from job set of time 1492419390000 ms
17/04/17 14:26:30 INFO MapPartitionsRDD: Removing RDD 712 from persistence list
17/04/17 14:26:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419390000 ms (execution: 0.001 s)
17/04/17 14:26:30 INFO MapPartitionsRDD: Removing RDD 711 from persistence list
17/04/17 14:26:30 INFO BlockManager: Removing RDD 712
17/04/17 14:26:30 INFO BlockManager: Removing RDD 711
17/04/17 14:26:30 INFO BlockRDD: Removing RDD 710 from persistence list
17/04/17 14:26:30 INFO BlockManager: Removing RDD 710
17/04/17 14:26:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[710] at createStream at KafkaConsumer.java:64 of time 1492419390000 ms
17/04/17 14:26:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419370000 ms)
17/04/17 14:26:30 INFO InputInfoTracker: remove old batch metadata: 1492419370000 ms
17/04/17 14:26:40 INFO JobScheduler: Added jobs for time 1492419400000 ms
17/04/17 14:26:40 INFO JobScheduler: Starting job streaming job 1492419400000 ms.0 from job set of time 1492419400000 ms
-------------------------------------------
Time: 1492419400000 ms
-------------------------------------------

17/04/17 14:26:40 INFO JobScheduler: Finished job streaming job 1492419400000 ms.0 from job set of time 1492419400000 ms
17/04/17 14:26:40 INFO MapPartitionsRDD: Removing RDD 715 from persistence list
17/04/17 14:26:40 INFO JobScheduler: Total delay: 0.004 s for time 1492419400000 ms (execution: 0.000 s)
17/04/17 14:26:40 INFO BlockManager: Removing RDD 715
17/04/17 14:26:40 INFO MapPartitionsRDD: Removing RDD 714 from persistence list
17/04/17 14:26:40 INFO BlockManager: Removing RDD 714
17/04/17 14:26:40 INFO BlockRDD: Removing RDD 713 from persistence list
17/04/17 14:26:40 INFO BlockManager: Removing RDD 713
17/04/17 14:26:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[713] at createStream at KafkaConsumer.java:64 of time 1492419400000 ms
17/04/17 14:26:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419380000 ms)
17/04/17 14:26:40 INFO InputInfoTracker: remove old batch metadata: 1492419380000 ms
17/04/17 14:26:50 INFO JobScheduler: Added jobs for time 1492419410000 ms
17/04/17 14:26:50 INFO JobScheduler: Starting job streaming job 1492419410000 ms.0 from job set of time 1492419410000 ms
-------------------------------------------
Time: 1492419410000 ms
-------------------------------------------

17/04/17 14:26:50 INFO JobScheduler: Finished job streaming job 1492419410000 ms.0 from job set of time 1492419410000 ms
17/04/17 14:26:50 INFO JobScheduler: Total delay: 0.004 s for time 1492419410000 ms (execution: 0.000 s)
17/04/17 14:26:50 INFO MapPartitionsRDD: Removing RDD 718 from persistence list
17/04/17 14:26:50 INFO BlockManager: Removing RDD 718
17/04/17 14:26:50 INFO MapPartitionsRDD: Removing RDD 717 from persistence list
17/04/17 14:26:50 INFO BlockManager: Removing RDD 717
17/04/17 14:26:50 INFO BlockRDD: Removing RDD 716 from persistence list
17/04/17 14:26:50 INFO BlockManager: Removing RDD 716
17/04/17 14:26:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[716] at createStream at KafkaConsumer.java:64 of time 1492419410000 ms
17/04/17 14:26:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419390000 ms)
17/04/17 14:26:50 INFO InputInfoTracker: remove old batch metadata: 1492419390000 ms
17/04/17 14:27:00 INFO JobScheduler: Added jobs for time 1492419420000 ms
17/04/17 14:27:00 INFO JobScheduler: Starting job streaming job 1492419420000 ms.0 from job set of time 1492419420000 ms
-------------------------------------------
Time: 1492419420000 ms
-------------------------------------------

17/04/17 14:27:00 INFO JobScheduler: Finished job streaming job 1492419420000 ms.0 from job set of time 1492419420000 ms
17/04/17 14:27:00 INFO MapPartitionsRDD: Removing RDD 721 from persistence list
17/04/17 14:27:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419420000 ms (execution: 0.001 s)
17/04/17 14:27:00 INFO BlockManager: Removing RDD 721
17/04/17 14:27:00 INFO MapPartitionsRDD: Removing RDD 720 from persistence list
17/04/17 14:27:00 INFO BlockManager: Removing RDD 720
17/04/17 14:27:00 INFO BlockRDD: Removing RDD 719 from persistence list
17/04/17 14:27:00 INFO BlockManager: Removing RDD 719
17/04/17 14:27:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[719] at createStream at KafkaConsumer.java:64 of time 1492419420000 ms
17/04/17 14:27:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419400000 ms)
17/04/17 14:27:00 INFO InputInfoTracker: remove old batch metadata: 1492419400000 ms
17/04/17 14:27:10 INFO JobScheduler: Added jobs for time 1492419430000 ms
17/04/17 14:27:10 INFO JobScheduler: Starting job streaming job 1492419430000 ms.0 from job set of time 1492419430000 ms
-------------------------------------------
Time: 1492419430000 ms
-------------------------------------------

17/04/17 14:27:10 INFO JobScheduler: Finished job streaming job 1492419430000 ms.0 from job set of time 1492419430000 ms
17/04/17 14:27:10 INFO MapPartitionsRDD: Removing RDD 724 from persistence list
17/04/17 14:27:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419430000 ms (execution: 0.001 s)
17/04/17 14:27:10 INFO BlockManager: Removing RDD 724
17/04/17 14:27:10 INFO MapPartitionsRDD: Removing RDD 723 from persistence list
17/04/17 14:27:10 INFO BlockManager: Removing RDD 723
17/04/17 14:27:10 INFO BlockRDD: Removing RDD 722 from persistence list
17/04/17 14:27:10 INFO BlockManager: Removing RDD 722
17/04/17 14:27:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[722] at createStream at KafkaConsumer.java:64 of time 1492419430000 ms
17/04/17 14:27:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419410000 ms)
17/04/17 14:27:10 INFO InputInfoTracker: remove old batch metadata: 1492419410000 ms
17/04/17 14:27:20 INFO JobScheduler: Added jobs for time 1492419440000 ms
17/04/17 14:27:20 INFO JobScheduler: Starting job streaming job 1492419440000 ms.0 from job set of time 1492419440000 ms
-------------------------------------------
Time: 1492419440000 ms
-------------------------------------------

17/04/17 14:27:20 INFO JobScheduler: Finished job streaming job 1492419440000 ms.0 from job set of time 1492419440000 ms
17/04/17 14:27:20 INFO MapPartitionsRDD: Removing RDD 727 from persistence list
17/04/17 14:27:20 INFO JobScheduler: Total delay: 0.004 s for time 1492419440000 ms (execution: 0.000 s)
17/04/17 14:27:20 INFO BlockManager: Removing RDD 727
17/04/17 14:27:20 INFO MapPartitionsRDD: Removing RDD 726 from persistence list
17/04/17 14:27:20 INFO BlockManager: Removing RDD 726
17/04/17 14:27:20 INFO BlockRDD: Removing RDD 725 from persistence list
17/04/17 14:27:20 INFO BlockManager: Removing RDD 725
17/04/17 14:27:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[725] at createStream at KafkaConsumer.java:64 of time 1492419440000 ms
17/04/17 14:27:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419420000 ms)
17/04/17 14:27:20 INFO InputInfoTracker: remove old batch metadata: 1492419420000 ms
17/04/17 14:27:30 INFO JobScheduler: Added jobs for time 1492419450000 ms
17/04/17 14:27:30 INFO JobScheduler: Starting job streaming job 1492419450000 ms.0 from job set of time 1492419450000 ms
-------------------------------------------
Time: 1492419450000 ms
-------------------------------------------

17/04/17 14:27:30 INFO JobScheduler: Finished job streaming job 1492419450000 ms.0 from job set of time 1492419450000 ms
17/04/17 14:27:30 INFO MapPartitionsRDD: Removing RDD 730 from persistence list
17/04/17 14:27:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419450000 ms (execution: 0.001 s)
17/04/17 14:27:30 INFO BlockManager: Removing RDD 730
17/04/17 14:27:30 INFO MapPartitionsRDD: Removing RDD 729 from persistence list
17/04/17 14:27:30 INFO BlockManager: Removing RDD 729
17/04/17 14:27:30 INFO BlockRDD: Removing RDD 728 from persistence list
17/04/17 14:27:30 INFO BlockManager: Removing RDD 728
17/04/17 14:27:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[728] at createStream at KafkaConsumer.java:64 of time 1492419450000 ms
17/04/17 14:27:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419430000 ms)
17/04/17 14:27:30 INFO InputInfoTracker: remove old batch metadata: 1492419430000 ms
17/04/17 14:27:40 INFO JobScheduler: Added jobs for time 1492419460000 ms
17/04/17 14:27:40 INFO JobScheduler: Starting job streaming job 1492419460000 ms.0 from job set of time 1492419460000 ms
-------------------------------------------
Time: 1492419460000 ms
-------------------------------------------

17/04/17 14:27:40 INFO JobScheduler: Finished job streaming job 1492419460000 ms.0 from job set of time 1492419460000 ms
17/04/17 14:27:40 INFO MapPartitionsRDD: Removing RDD 733 from persistence list
17/04/17 14:27:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419460000 ms (execution: 0.001 s)
17/04/17 14:27:40 INFO BlockManager: Removing RDD 733
17/04/17 14:27:40 INFO MapPartitionsRDD: Removing RDD 732 from persistence list
17/04/17 14:27:40 INFO BlockManager: Removing RDD 732
17/04/17 14:27:40 INFO BlockRDD: Removing RDD 731 from persistence list
17/04/17 14:27:40 INFO BlockManager: Removing RDD 731
17/04/17 14:27:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[731] at createStream at KafkaConsumer.java:64 of time 1492419460000 ms
17/04/17 14:27:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419440000 ms)
17/04/17 14:27:40 INFO InputInfoTracker: remove old batch metadata: 1492419440000 ms
17/04/17 14:27:50 INFO JobScheduler: Added jobs for time 1492419470000 ms
-------------------------------------------
Time: 1492419470000 ms
-------------------------------------------

17/04/17 14:27:50 INFO JobScheduler: Starting job streaming job 1492419470000 ms.0 from job set of time 1492419470000 ms
17/04/17 14:27:50 INFO JobScheduler: Finished job streaming job 1492419470000 ms.0 from job set of time 1492419470000 ms
17/04/17 14:27:50 INFO MapPartitionsRDD: Removing RDD 736 from persistence list
17/04/17 14:27:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419470000 ms (execution: 0.000 s)
17/04/17 14:27:50 INFO BlockManager: Removing RDD 736
17/04/17 14:27:50 INFO MapPartitionsRDD: Removing RDD 735 from persistence list
17/04/17 14:27:50 INFO BlockManager: Removing RDD 735
17/04/17 14:27:50 INFO BlockRDD: Removing RDD 734 from persistence list
17/04/17 14:27:50 INFO BlockManager: Removing RDD 734
17/04/17 14:27:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[734] at createStream at KafkaConsumer.java:64 of time 1492419470000 ms
17/04/17 14:27:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419450000 ms)
17/04/17 14:27:50 INFO InputInfoTracker: remove old batch metadata: 1492419450000 ms
17/04/17 14:28:00 INFO JobScheduler: Added jobs for time 1492419480000 ms
17/04/17 14:28:00 INFO JobScheduler: Starting job streaming job 1492419480000 ms.0 from job set of time 1492419480000 ms
-------------------------------------------
Time: 1492419480000 ms
-------------------------------------------

17/04/17 14:28:00 INFO JobScheduler: Finished job streaming job 1492419480000 ms.0 from job set of time 1492419480000 ms
17/04/17 14:28:00 INFO MapPartitionsRDD: Removing RDD 739 from persistence list
17/04/17 14:28:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419480000 ms (execution: 0.001 s)
17/04/17 14:28:00 INFO BlockManager: Removing RDD 739
17/04/17 14:28:00 INFO MapPartitionsRDD: Removing RDD 738 from persistence list
17/04/17 14:28:00 INFO BlockManager: Removing RDD 738
17/04/17 14:28:00 INFO BlockRDD: Removing RDD 737 from persistence list
17/04/17 14:28:00 INFO BlockManager: Removing RDD 737
17/04/17 14:28:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[737] at createStream at KafkaConsumer.java:64 of time 1492419480000 ms
17/04/17 14:28:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419460000 ms)
17/04/17 14:28:00 INFO InputInfoTracker: remove old batch metadata: 1492419460000 ms
17/04/17 14:28:10 INFO JobScheduler: Added jobs for time 1492419490000 ms
-------------------------------------------
Time: 1492419490000 ms
-------------------------------------------

17/04/17 14:28:10 INFO JobScheduler: Starting job streaming job 1492419490000 ms.0 from job set of time 1492419490000 ms
17/04/17 14:28:10 INFO JobScheduler: Finished job streaming job 1492419490000 ms.0 from job set of time 1492419490000 ms
17/04/17 14:28:10 INFO MapPartitionsRDD: Removing RDD 742 from persistence list
17/04/17 14:28:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419490000 ms (execution: 0.000 s)
17/04/17 14:28:10 INFO BlockManager: Removing RDD 742
17/04/17 14:28:10 INFO MapPartitionsRDD: Removing RDD 741 from persistence list
17/04/17 14:28:10 INFO BlockManager: Removing RDD 741
17/04/17 14:28:10 INFO BlockRDD: Removing RDD 740 from persistence list
17/04/17 14:28:10 INFO BlockManager: Removing RDD 740
17/04/17 14:28:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[740] at createStream at KafkaConsumer.java:64 of time 1492419490000 ms
17/04/17 14:28:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419470000 ms)
17/04/17 14:28:10 INFO InputInfoTracker: remove old batch metadata: 1492419470000 ms
17/04/17 14:28:20 INFO JobScheduler: Added jobs for time 1492419500000 ms
17/04/17 14:28:20 INFO JobScheduler: Starting job streaming job 1492419500000 ms.0 from job set of time 1492419500000 ms
-------------------------------------------
Time: 1492419500000 ms
-------------------------------------------

17/04/17 14:28:20 INFO JobScheduler: Finished job streaming job 1492419500000 ms.0 from job set of time 1492419500000 ms
17/04/17 14:28:20 INFO MapPartitionsRDD: Removing RDD 745 from persistence list
17/04/17 14:28:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419500000 ms (execution: 0.000 s)
17/04/17 14:28:20 INFO MapPartitionsRDD: Removing RDD 744 from persistence list
17/04/17 14:28:20 INFO BlockManager: Removing RDD 745
17/04/17 14:28:20 INFO BlockManager: Removing RDD 744
17/04/17 14:28:20 INFO BlockRDD: Removing RDD 743 from persistence list
17/04/17 14:28:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[743] at createStream at KafkaConsumer.java:64 of time 1492419500000 ms
17/04/17 14:28:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419480000 ms)
17/04/17 14:28:20 INFO InputInfoTracker: remove old batch metadata: 1492419480000 ms
17/04/17 14:28:20 INFO BlockManager: Removing RDD 743
17/04/17 14:28:30 INFO JobScheduler: Added jobs for time 1492419510000 ms
17/04/17 14:28:30 INFO JobScheduler: Starting job streaming job 1492419510000 ms.0 from job set of time 1492419510000 ms
-------------------------------------------
Time: 1492419510000 ms
-------------------------------------------

17/04/17 14:28:30 INFO JobScheduler: Finished job streaming job 1492419510000 ms.0 from job set of time 1492419510000 ms
17/04/17 14:28:30 INFO MapPartitionsRDD: Removing RDD 748 from persistence list
17/04/17 14:28:30 INFO JobScheduler: Total delay: 0.004 s for time 1492419510000 ms (execution: 0.000 s)
17/04/17 14:28:30 INFO BlockManager: Removing RDD 748
17/04/17 14:28:30 INFO MapPartitionsRDD: Removing RDD 747 from persistence list
17/04/17 14:28:30 INFO BlockManager: Removing RDD 747
17/04/17 14:28:30 INFO BlockRDD: Removing RDD 746 from persistence list
17/04/17 14:28:30 INFO BlockManager: Removing RDD 746
17/04/17 14:28:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[746] at createStream at KafkaConsumer.java:64 of time 1492419510000 ms
17/04/17 14:28:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419490000 ms)
17/04/17 14:28:30 INFO InputInfoTracker: remove old batch metadata: 1492419490000 ms
17/04/17 14:28:40 INFO JobScheduler: Added jobs for time 1492419520000 ms
17/04/17 14:28:40 INFO JobScheduler: Starting job streaming job 1492419520000 ms.0 from job set of time 1492419520000 ms
-------------------------------------------
Time: 1492419520000 ms
-------------------------------------------

17/04/17 14:28:40 INFO JobScheduler: Finished job streaming job 1492419520000 ms.0 from job set of time 1492419520000 ms
17/04/17 14:28:40 INFO MapPartitionsRDD: Removing RDD 751 from persistence list
17/04/17 14:28:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419520000 ms (execution: 0.001 s)
17/04/17 14:28:40 INFO BlockManager: Removing RDD 751
17/04/17 14:28:40 INFO MapPartitionsRDD: Removing RDD 750 from persistence list
17/04/17 14:28:40 INFO BlockRDD: Removing RDD 749 from persistence list
17/04/17 14:28:40 INFO BlockManager: Removing RDD 750
17/04/17 14:28:40 INFO BlockManager: Removing RDD 749
17/04/17 14:28:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[749] at createStream at KafkaConsumer.java:64 of time 1492419520000 ms
17/04/17 14:28:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419500000 ms)
17/04/17 14:28:40 INFO InputInfoTracker: remove old batch metadata: 1492419500000 ms
17/04/17 14:28:50 INFO JobScheduler: Added jobs for time 1492419530000 ms
17/04/17 14:28:50 INFO JobScheduler: Starting job streaming job 1492419530000 ms.0 from job set of time 1492419530000 ms
-------------------------------------------
Time: 1492419530000 ms
-------------------------------------------

17/04/17 14:28:50 INFO JobScheduler: Finished job streaming job 1492419530000 ms.0 from job set of time 1492419530000 ms
17/04/17 14:28:50 INFO MapPartitionsRDD: Removing RDD 754 from persistence list
17/04/17 14:28:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419530000 ms (execution: 0.000 s)
17/04/17 14:28:50 INFO BlockManager: Removing RDD 754
17/04/17 14:28:50 INFO MapPartitionsRDD: Removing RDD 753 from persistence list
17/04/17 14:28:50 INFO BlockManager: Removing RDD 753
17/04/17 14:28:50 INFO BlockRDD: Removing RDD 752 from persistence list
17/04/17 14:28:50 INFO BlockManager: Removing RDD 752
17/04/17 14:28:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[752] at createStream at KafkaConsumer.java:64 of time 1492419530000 ms
17/04/17 14:28:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419510000 ms)
17/04/17 14:28:50 INFO InputInfoTracker: remove old batch metadata: 1492419510000 ms
17/04/17 14:29:00 INFO JobScheduler: Added jobs for time 1492419540000 ms
17/04/17 14:29:00 INFO JobScheduler: Starting job streaming job 1492419540000 ms.0 from job set of time 1492419540000 ms
-------------------------------------------
Time: 1492419540000 ms
-------------------------------------------

17/04/17 14:29:00 INFO JobScheduler: Finished job streaming job 1492419540000 ms.0 from job set of time 1492419540000 ms
17/04/17 14:29:00 INFO MapPartitionsRDD: Removing RDD 757 from persistence list
17/04/17 14:29:00 INFO JobScheduler: Total delay: 0.006 s for time 1492419540000 ms (execution: 0.000 s)
17/04/17 14:29:00 INFO MapPartitionsRDD: Removing RDD 756 from persistence list
17/04/17 14:29:00 INFO BlockManager: Removing RDD 757
17/04/17 14:29:00 INFO BlockManager: Removing RDD 756
17/04/17 14:29:00 INFO BlockRDD: Removing RDD 755 from persistence list
17/04/17 14:29:00 INFO BlockManager: Removing RDD 755
17/04/17 14:29:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[755] at createStream at KafkaConsumer.java:64 of time 1492419540000 ms
17/04/17 14:29:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419520000 ms)
17/04/17 14:29:00 INFO InputInfoTracker: remove old batch metadata: 1492419520000 ms
17/04/17 14:29:10 INFO JobScheduler: Added jobs for time 1492419550000 ms
17/04/17 14:29:10 INFO JobScheduler: Starting job streaming job 1492419550000 ms.0 from job set of time 1492419550000 ms
-------------------------------------------
Time: 1492419550000 ms
-------------------------------------------

17/04/17 14:29:10 INFO JobScheduler: Finished job streaming job 1492419550000 ms.0 from job set of time 1492419550000 ms
17/04/17 14:29:10 INFO MapPartitionsRDD: Removing RDD 760 from persistence list
17/04/17 14:29:10 INFO JobScheduler: Total delay: 0.004 s for time 1492419550000 ms (execution: 0.000 s)
17/04/17 14:29:10 INFO BlockManager: Removing RDD 760
17/04/17 14:29:10 INFO MapPartitionsRDD: Removing RDD 759 from persistence list
17/04/17 14:29:10 INFO BlockManager: Removing RDD 759
17/04/17 14:29:10 INFO BlockRDD: Removing RDD 758 from persistence list
17/04/17 14:29:10 INFO BlockManager: Removing RDD 758
17/04/17 14:29:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[758] at createStream at KafkaConsumer.java:64 of time 1492419550000 ms
17/04/17 14:29:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419530000 ms)
17/04/17 14:29:10 INFO InputInfoTracker: remove old batch metadata: 1492419530000 ms
17/04/17 14:29:20 INFO JobScheduler: Added jobs for time 1492419560000 ms
17/04/17 14:29:20 INFO JobScheduler: Starting job streaming job 1492419560000 ms.0 from job set of time 1492419560000 ms
-------------------------------------------
Time: 1492419560000 ms
-------------------------------------------

17/04/17 14:29:20 INFO JobScheduler: Finished job streaming job 1492419560000 ms.0 from job set of time 1492419560000 ms
17/04/17 14:29:20 INFO MapPartitionsRDD: Removing RDD 763 from persistence list
17/04/17 14:29:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419560000 ms (execution: 0.001 s)
17/04/17 14:29:20 INFO BlockManager: Removing RDD 763
17/04/17 14:29:20 INFO MapPartitionsRDD: Removing RDD 762 from persistence list
17/04/17 14:29:20 INFO BlockManager: Removing RDD 762
17/04/17 14:29:20 INFO BlockRDD: Removing RDD 761 from persistence list
17/04/17 14:29:20 INFO BlockManager: Removing RDD 761
17/04/17 14:29:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[761] at createStream at KafkaConsumer.java:64 of time 1492419560000 ms
17/04/17 14:29:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419540000 ms)
17/04/17 14:29:20 INFO InputInfoTracker: remove old batch metadata: 1492419540000 ms
17/04/17 14:29:30 INFO JobScheduler: Added jobs for time 1492419570000 ms
17/04/17 14:29:30 INFO JobScheduler: Starting job streaming job 1492419570000 ms.0 from job set of time 1492419570000 ms
-------------------------------------------
Time: 1492419570000 ms
-------------------------------------------

17/04/17 14:29:30 INFO JobScheduler: Finished job streaming job 1492419570000 ms.0 from job set of time 1492419570000 ms
17/04/17 14:29:30 INFO MapPartitionsRDD: Removing RDD 766 from persistence list
17/04/17 14:29:30 INFO JobScheduler: Total delay: 0.006 s for time 1492419570000 ms (execution: 0.001 s)
17/04/17 14:29:30 INFO BlockManager: Removing RDD 766
17/04/17 14:29:30 INFO MapPartitionsRDD: Removing RDD 765 from persistence list
17/04/17 14:29:30 INFO BlockManager: Removing RDD 765
17/04/17 14:29:30 INFO BlockRDD: Removing RDD 764 from persistence list
17/04/17 14:29:30 INFO BlockManager: Removing RDD 764
17/04/17 14:29:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[764] at createStream at KafkaConsumer.java:64 of time 1492419570000 ms
17/04/17 14:29:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419550000 ms)
17/04/17 14:29:30 INFO InputInfoTracker: remove old batch metadata: 1492419550000 ms
17/04/17 14:29:40 INFO JobScheduler: Added jobs for time 1492419580000 ms
17/04/17 14:29:40 INFO JobScheduler: Starting job streaming job 1492419580000 ms.0 from job set of time 1492419580000 ms
-------------------------------------------
Time: 1492419580000 ms
-------------------------------------------

17/04/17 14:29:40 INFO JobScheduler: Finished job streaming job 1492419580000 ms.0 from job set of time 1492419580000 ms
17/04/17 14:29:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419580000 ms (execution: 0.000 s)
17/04/17 14:29:40 INFO MapPartitionsRDD: Removing RDD 769 from persistence list
17/04/17 14:29:40 INFO BlockManager: Removing RDD 769
17/04/17 14:29:40 INFO MapPartitionsRDD: Removing RDD 768 from persistence list
17/04/17 14:29:40 INFO BlockManager: Removing RDD 768
17/04/17 14:29:40 INFO BlockRDD: Removing RDD 767 from persistence list
17/04/17 14:29:40 INFO BlockManager: Removing RDD 767
17/04/17 14:29:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[767] at createStream at KafkaConsumer.java:64 of time 1492419580000 ms
17/04/17 14:29:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419560000 ms)
17/04/17 14:29:40 INFO InputInfoTracker: remove old batch metadata: 1492419560000 ms
17/04/17 14:29:50 INFO JobScheduler: Added jobs for time 1492419590000 ms
17/04/17 14:29:50 INFO JobScheduler: Starting job streaming job 1492419590000 ms.0 from job set of time 1492419590000 ms
-------------------------------------------
Time: 1492419590000 ms
-------------------------------------------

17/04/17 14:29:50 INFO JobScheduler: Finished job streaming job 1492419590000 ms.0 from job set of time 1492419590000 ms
17/04/17 14:29:50 INFO MapPartitionsRDD: Removing RDD 772 from persistence list
17/04/17 14:29:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419590000 ms (execution: 0.001 s)
17/04/17 14:29:50 INFO BlockManager: Removing RDD 772
17/04/17 14:29:50 INFO MapPartitionsRDD: Removing RDD 771 from persistence list
17/04/17 14:29:50 INFO BlockManager: Removing RDD 771
17/04/17 14:29:50 INFO BlockRDD: Removing RDD 770 from persistence list
17/04/17 14:29:50 INFO BlockManager: Removing RDD 770
17/04/17 14:29:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[770] at createStream at KafkaConsumer.java:64 of time 1492419590000 ms
17/04/17 14:29:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419570000 ms)
17/04/17 14:29:50 INFO InputInfoTracker: remove old batch metadata: 1492419570000 ms
17/04/17 14:30:00 INFO JobScheduler: Added jobs for time 1492419600000 ms
17/04/17 14:30:00 INFO JobScheduler: Starting job streaming job 1492419600000 ms.0 from job set of time 1492419600000 ms
-------------------------------------------
Time: 1492419600000 ms
-------------------------------------------

17/04/17 14:30:00 INFO JobScheduler: Finished job streaming job 1492419600000 ms.0 from job set of time 1492419600000 ms
17/04/17 14:30:00 INFO MapPartitionsRDD: Removing RDD 775 from persistence list
17/04/17 14:30:00 INFO JobScheduler: Total delay: 0.004 s for time 1492419600000 ms (execution: 0.000 s)
17/04/17 14:30:00 INFO BlockManager: Removing RDD 775
17/04/17 14:30:00 INFO MapPartitionsRDD: Removing RDD 774 from persistence list
17/04/17 14:30:00 INFO BlockManager: Removing RDD 774
17/04/17 14:30:00 INFO BlockRDD: Removing RDD 773 from persistence list
17/04/17 14:30:00 INFO BlockManager: Removing RDD 773
17/04/17 14:30:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[773] at createStream at KafkaConsumer.java:64 of time 1492419600000 ms
17/04/17 14:30:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419580000 ms)
17/04/17 14:30:00 INFO InputInfoTracker: remove old batch metadata: 1492419580000 ms
17/04/17 14:30:10 INFO JobScheduler: Added jobs for time 1492419610000 ms
17/04/17 14:30:10 INFO JobScheduler: Starting job streaming job 1492419610000 ms.0 from job set of time 1492419610000 ms
-------------------------------------------
Time: 1492419610000 ms
-------------------------------------------

17/04/17 14:30:10 INFO JobScheduler: Finished job streaming job 1492419610000 ms.0 from job set of time 1492419610000 ms
17/04/17 14:30:10 INFO MapPartitionsRDD: Removing RDD 778 from persistence list
17/04/17 14:30:10 INFO JobScheduler: Total delay: 0.006 s for time 1492419610000 ms (execution: 0.001 s)
17/04/17 14:30:10 INFO BlockManager: Removing RDD 778
17/04/17 14:30:10 INFO MapPartitionsRDD: Removing RDD 777 from persistence list
17/04/17 14:30:10 INFO BlockManager: Removing RDD 777
17/04/17 14:30:10 INFO BlockRDD: Removing RDD 776 from persistence list
17/04/17 14:30:10 INFO BlockManager: Removing RDD 776
17/04/17 14:30:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[776] at createStream at KafkaConsumer.java:64 of time 1492419610000 ms
17/04/17 14:30:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419590000 ms)
17/04/17 14:30:10 INFO InputInfoTracker: remove old batch metadata: 1492419590000 ms
17/04/17 14:30:20 INFO JobScheduler: Added jobs for time 1492419620000 ms
17/04/17 14:30:20 INFO JobScheduler: Starting job streaming job 1492419620000 ms.0 from job set of time 1492419620000 ms
-------------------------------------------
Time: 1492419620000 ms
-------------------------------------------

17/04/17 14:30:20 INFO JobScheduler: Finished job streaming job 1492419620000 ms.0 from job set of time 1492419620000 ms
17/04/17 14:30:20 INFO MapPartitionsRDD: Removing RDD 781 from persistence list
17/04/17 14:30:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419620000 ms (execution: 0.001 s)
17/04/17 14:30:20 INFO BlockManager: Removing RDD 781
17/04/17 14:30:20 INFO MapPartitionsRDD: Removing RDD 780 from persistence list
17/04/17 14:30:20 INFO BlockManager: Removing RDD 780
17/04/17 14:30:20 INFO BlockRDD: Removing RDD 779 from persistence list
17/04/17 14:30:20 INFO BlockManager: Removing RDD 779
17/04/17 14:30:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[779] at createStream at KafkaConsumer.java:64 of time 1492419620000 ms
17/04/17 14:30:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419600000 ms)
17/04/17 14:30:20 INFO InputInfoTracker: remove old batch metadata: 1492419600000 ms
17/04/17 14:30:30 INFO JobScheduler: Added jobs for time 1492419630000 ms
17/04/17 14:30:30 INFO JobScheduler: Starting job streaming job 1492419630000 ms.0 from job set of time 1492419630000 ms
-------------------------------------------
Time: 1492419630000 ms
-------------------------------------------

17/04/17 14:30:30 INFO JobScheduler: Finished job streaming job 1492419630000 ms.0 from job set of time 1492419630000 ms
17/04/17 14:30:30 INFO MapPartitionsRDD: Removing RDD 784 from persistence list
17/04/17 14:30:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419630000 ms (execution: 0.001 s)
17/04/17 14:30:30 INFO BlockManager: Removing RDD 784
17/04/17 14:30:30 INFO MapPartitionsRDD: Removing RDD 783 from persistence list
17/04/17 14:30:30 INFO BlockManager: Removing RDD 783
17/04/17 14:30:30 INFO BlockRDD: Removing RDD 782 from persistence list
17/04/17 14:30:30 INFO BlockManager: Removing RDD 782
17/04/17 14:30:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[782] at createStream at KafkaConsumer.java:64 of time 1492419630000 ms
17/04/17 14:30:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419610000 ms)
17/04/17 14:30:30 INFO InputInfoTracker: remove old batch metadata: 1492419610000 ms
17/04/17 14:30:40 INFO JobScheduler: Added jobs for time 1492419640000 ms
17/04/17 14:30:40 INFO JobScheduler: Starting job streaming job 1492419640000 ms.0 from job set of time 1492419640000 ms
-------------------------------------------
Time: 1492419640000 ms
-------------------------------------------

17/04/17 14:30:40 INFO JobScheduler: Finished job streaming job 1492419640000 ms.0 from job set of time 1492419640000 ms
17/04/17 14:30:40 INFO JobScheduler: Total delay: 0.004 s for time 1492419640000 ms (execution: 0.000 s)
17/04/17 14:30:40 INFO MapPartitionsRDD: Removing RDD 787 from persistence list
17/04/17 14:30:40 INFO BlockManager: Removing RDD 787
17/04/17 14:30:40 INFO MapPartitionsRDD: Removing RDD 786 from persistence list
17/04/17 14:30:40 INFO BlockManager: Removing RDD 786
17/04/17 14:30:40 INFO BlockRDD: Removing RDD 785 from persistence list
17/04/17 14:30:40 INFO BlockManager: Removing RDD 785
17/04/17 14:30:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[785] at createStream at KafkaConsumer.java:64 of time 1492419640000 ms
17/04/17 14:30:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419620000 ms)
17/04/17 14:30:40 INFO InputInfoTracker: remove old batch metadata: 1492419620000 ms
17/04/17 14:30:50 INFO JobScheduler: Added jobs for time 1492419650000 ms
17/04/17 14:30:50 INFO JobScheduler: Starting job streaming job 1492419650000 ms.0 from job set of time 1492419650000 ms
-------------------------------------------
Time: 1492419650000 ms
-------------------------------------------

17/04/17 14:30:50 INFO JobScheduler: Finished job streaming job 1492419650000 ms.0 from job set of time 1492419650000 ms
17/04/17 14:30:50 INFO MapPartitionsRDD: Removing RDD 790 from persistence list
17/04/17 14:30:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419650000 ms (execution: 0.001 s)
17/04/17 14:30:50 INFO BlockManager: Removing RDD 790
17/04/17 14:30:50 INFO MapPartitionsRDD: Removing RDD 789 from persistence list
17/04/17 14:30:50 INFO BlockManager: Removing RDD 789
17/04/17 14:30:50 INFO BlockRDD: Removing RDD 788 from persistence list
17/04/17 14:30:50 INFO BlockManager: Removing RDD 788
17/04/17 14:30:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[788] at createStream at KafkaConsumer.java:64 of time 1492419650000 ms
17/04/17 14:30:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419630000 ms)
17/04/17 14:30:50 INFO InputInfoTracker: remove old batch metadata: 1492419630000 ms
17/04/17 14:31:00 INFO JobScheduler: Added jobs for time 1492419660000 ms
17/04/17 14:31:00 INFO JobScheduler: Starting job streaming job 1492419660000 ms.0 from job set of time 1492419660000 ms
-------------------------------------------
Time: 1492419660000 ms
-------------------------------------------

17/04/17 14:31:00 INFO JobScheduler: Finished job streaming job 1492419660000 ms.0 from job set of time 1492419660000 ms
17/04/17 14:31:00 INFO MapPartitionsRDD: Removing RDD 793 from persistence list
17/04/17 14:31:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419660000 ms (execution: 0.001 s)
17/04/17 14:31:00 INFO BlockManager: Removing RDD 793
17/04/17 14:31:00 INFO MapPartitionsRDD: Removing RDD 792 from persistence list
17/04/17 14:31:00 INFO BlockManager: Removing RDD 792
17/04/17 14:31:00 INFO BlockRDD: Removing RDD 791 from persistence list
17/04/17 14:31:00 INFO BlockManager: Removing RDD 791
17/04/17 14:31:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[791] at createStream at KafkaConsumer.java:64 of time 1492419660000 ms
17/04/17 14:31:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419640000 ms)
17/04/17 14:31:00 INFO InputInfoTracker: remove old batch metadata: 1492419640000 ms
17/04/17 14:31:10 INFO JobScheduler: Added jobs for time 1492419670000 ms
17/04/17 14:31:10 INFO JobScheduler: Starting job streaming job 1492419670000 ms.0 from job set of time 1492419670000 ms
-------------------------------------------
Time: 1492419670000 ms
-------------------------------------------

17/04/17 14:31:10 INFO JobScheduler: Finished job streaming job 1492419670000 ms.0 from job set of time 1492419670000 ms
17/04/17 14:31:10 INFO MapPartitionsRDD: Removing RDD 796 from persistence list
17/04/17 14:31:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419670000 ms (execution: 0.001 s)
17/04/17 14:31:10 INFO BlockManager: Removing RDD 796
17/04/17 14:31:10 INFO MapPartitionsRDD: Removing RDD 795 from persistence list
17/04/17 14:31:10 INFO BlockManager: Removing RDD 795
17/04/17 14:31:10 INFO BlockRDD: Removing RDD 794 from persistence list
17/04/17 14:31:10 INFO BlockManager: Removing RDD 794
17/04/17 14:31:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[794] at createStream at KafkaConsumer.java:64 of time 1492419670000 ms
17/04/17 14:31:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419650000 ms)
17/04/17 14:31:10 INFO InputInfoTracker: remove old batch metadata: 1492419650000 ms
17/04/17 14:31:20 INFO JobScheduler: Added jobs for time 1492419680000 ms
17/04/17 14:31:20 INFO JobScheduler: Starting job streaming job 1492419680000 ms.0 from job set of time 1492419680000 ms
-------------------------------------------
Time: 1492419680000 ms
-------------------------------------------

17/04/17 14:31:20 INFO JobScheduler: Finished job streaming job 1492419680000 ms.0 from job set of time 1492419680000 ms
17/04/17 14:31:20 INFO MapPartitionsRDD: Removing RDD 799 from persistence list
17/04/17 14:31:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419680000 ms (execution: 0.000 s)
17/04/17 14:31:20 INFO BlockManager: Removing RDD 799
17/04/17 14:31:20 INFO MapPartitionsRDD: Removing RDD 798 from persistence list
17/04/17 14:31:20 INFO BlockManager: Removing RDD 798
17/04/17 14:31:20 INFO BlockRDD: Removing RDD 797 from persistence list
17/04/17 14:31:20 INFO BlockManager: Removing RDD 797
17/04/17 14:31:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[797] at createStream at KafkaConsumer.java:64 of time 1492419680000 ms
17/04/17 14:31:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419660000 ms)
17/04/17 14:31:20 INFO InputInfoTracker: remove old batch metadata: 1492419660000 ms
17/04/17 14:31:30 INFO JobScheduler: Added jobs for time 1492419690000 ms
17/04/17 14:31:30 INFO JobScheduler: Starting job streaming job 1492419690000 ms.0 from job set of time 1492419690000 ms
-------------------------------------------
Time: 1492419690000 ms
-------------------------------------------

17/04/17 14:31:30 INFO JobScheduler: Finished job streaming job 1492419690000 ms.0 from job set of time 1492419690000 ms
17/04/17 14:31:30 INFO MapPartitionsRDD: Removing RDD 802 from persistence list
17/04/17 14:31:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419690000 ms (execution: 0.000 s)
17/04/17 14:31:30 INFO BlockManager: Removing RDD 802
17/04/17 14:31:30 INFO MapPartitionsRDD: Removing RDD 801 from persistence list
17/04/17 14:31:30 INFO BlockManager: Removing RDD 801
17/04/17 14:31:30 INFO BlockRDD: Removing RDD 800 from persistence list
17/04/17 14:31:30 INFO BlockManager: Removing RDD 800
17/04/17 14:31:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[800] at createStream at KafkaConsumer.java:64 of time 1492419690000 ms
17/04/17 14:31:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419670000 ms)
17/04/17 14:31:30 INFO InputInfoTracker: remove old batch metadata: 1492419670000 ms
17/04/17 14:31:40 INFO JobScheduler: Added jobs for time 1492419700000 ms
17/04/17 14:31:40 INFO JobScheduler: Starting job streaming job 1492419700000 ms.0 from job set of time 1492419700000 ms
-------------------------------------------
Time: 1492419700000 ms
-------------------------------------------

17/04/17 14:31:40 INFO JobScheduler: Finished job streaming job 1492419700000 ms.0 from job set of time 1492419700000 ms
17/04/17 14:31:40 INFO MapPartitionsRDD: Removing RDD 805 from persistence list
17/04/17 14:31:40 INFO JobScheduler: Total delay: 0.004 s for time 1492419700000 ms (execution: 0.000 s)
17/04/17 14:31:40 INFO BlockManager: Removing RDD 805
17/04/17 14:31:40 INFO MapPartitionsRDD: Removing RDD 804 from persistence list
17/04/17 14:31:40 INFO BlockManager: Removing RDD 804
17/04/17 14:31:40 INFO BlockRDD: Removing RDD 803 from persistence list
17/04/17 14:31:40 INFO BlockManager: Removing RDD 803
17/04/17 14:31:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[803] at createStream at KafkaConsumer.java:64 of time 1492419700000 ms
17/04/17 14:31:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419680000 ms)
17/04/17 14:31:40 INFO InputInfoTracker: remove old batch metadata: 1492419680000 ms
17/04/17 14:31:50 INFO JobScheduler: Added jobs for time 1492419710000 ms
17/04/17 14:31:50 INFO JobScheduler: Starting job streaming job 1492419710000 ms.0 from job set of time 1492419710000 ms
-------------------------------------------
Time: 1492419710000 ms
-------------------------------------------

17/04/17 14:31:50 INFO JobScheduler: Finished job streaming job 1492419710000 ms.0 from job set of time 1492419710000 ms
17/04/17 14:31:50 INFO MapPartitionsRDD: Removing RDD 808 from persistence list
17/04/17 14:31:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419710000 ms (execution: 0.001 s)
17/04/17 14:31:50 INFO BlockManager: Removing RDD 808
17/04/17 14:31:50 INFO MapPartitionsRDD: Removing RDD 807 from persistence list
17/04/17 14:31:50 INFO BlockManager: Removing RDD 807
17/04/17 14:31:50 INFO BlockRDD: Removing RDD 806 from persistence list
17/04/17 14:31:50 INFO BlockManager: Removing RDD 806
17/04/17 14:31:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[806] at createStream at KafkaConsumer.java:64 of time 1492419710000 ms
17/04/17 14:31:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419690000 ms)
17/04/17 14:31:50 INFO InputInfoTracker: remove old batch metadata: 1492419690000 ms
17/04/17 14:32:00 INFO JobScheduler: Added jobs for time 1492419720000 ms
17/04/17 14:32:00 INFO JobScheduler: Starting job streaming job 1492419720000 ms.0 from job set of time 1492419720000 ms
-------------------------------------------
Time: 1492419720000 ms
-------------------------------------------

17/04/17 14:32:00 INFO JobScheduler: Finished job streaming job 1492419720000 ms.0 from job set of time 1492419720000 ms
17/04/17 14:32:00 INFO MapPartitionsRDD: Removing RDD 811 from persistence list
17/04/17 14:32:00 INFO JobScheduler: Total delay: 0.004 s for time 1492419720000 ms (execution: 0.000 s)
17/04/17 14:32:00 INFO BlockManager: Removing RDD 811
17/04/17 14:32:00 INFO MapPartitionsRDD: Removing RDD 810 from persistence list
17/04/17 14:32:00 INFO BlockManager: Removing RDD 810
17/04/17 14:32:00 INFO BlockRDD: Removing RDD 809 from persistence list
17/04/17 14:32:00 INFO BlockManager: Removing RDD 809
17/04/17 14:32:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[809] at createStream at KafkaConsumer.java:64 of time 1492419720000 ms
17/04/17 14:32:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419700000 ms)
17/04/17 14:32:00 INFO InputInfoTracker: remove old batch metadata: 1492419700000 ms
17/04/17 14:32:10 INFO JobScheduler: Added jobs for time 1492419730000 ms
17/04/17 14:32:10 INFO JobScheduler: Starting job streaming job 1492419730000 ms.0 from job set of time 1492419730000 ms
-------------------------------------------
Time: 1492419730000 ms
-------------------------------------------

17/04/17 14:32:10 INFO JobScheduler: Finished job streaming job 1492419730000 ms.0 from job set of time 1492419730000 ms
17/04/17 14:32:10 INFO MapPartitionsRDD: Removing RDD 814 from persistence list
17/04/17 14:32:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419730000 ms (execution: 0.001 s)
17/04/17 14:32:10 INFO BlockManager: Removing RDD 814
17/04/17 14:32:10 INFO MapPartitionsRDD: Removing RDD 813 from persistence list
17/04/17 14:32:10 INFO BlockManager: Removing RDD 813
17/04/17 14:32:10 INFO BlockRDD: Removing RDD 812 from persistence list
17/04/17 14:32:10 INFO BlockManager: Removing RDD 812
17/04/17 14:32:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[812] at createStream at KafkaConsumer.java:64 of time 1492419730000 ms
17/04/17 14:32:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419710000 ms)
17/04/17 14:32:10 INFO InputInfoTracker: remove old batch metadata: 1492419710000 ms
17/04/17 14:32:20 INFO JobScheduler: Added jobs for time 1492419740000 ms
17/04/17 14:32:20 INFO JobScheduler: Starting job streaming job 1492419740000 ms.0 from job set of time 1492419740000 ms
-------------------------------------------
Time: 1492419740000 ms
-------------------------------------------

17/04/17 14:32:20 INFO JobScheduler: Finished job streaming job 1492419740000 ms.0 from job set of time 1492419740000 ms
17/04/17 14:32:20 INFO MapPartitionsRDD: Removing RDD 817 from persistence list
17/04/17 14:32:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419740000 ms (execution: 0.001 s)
17/04/17 14:32:20 INFO BlockManager: Removing RDD 817
17/04/17 14:32:20 INFO MapPartitionsRDD: Removing RDD 816 from persistence list
17/04/17 14:32:20 INFO BlockManager: Removing RDD 816
17/04/17 14:32:20 INFO BlockRDD: Removing RDD 815 from persistence list
17/04/17 14:32:20 INFO BlockManager: Removing RDD 815
17/04/17 14:32:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[815] at createStream at KafkaConsumer.java:64 of time 1492419740000 ms
17/04/17 14:32:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419720000 ms)
17/04/17 14:32:20 INFO InputInfoTracker: remove old batch metadata: 1492419720000 ms
17/04/17 14:32:30 INFO JobScheduler: Added jobs for time 1492419750000 ms
17/04/17 14:32:30 INFO JobScheduler: Starting job streaming job 1492419750000 ms.0 from job set of time 1492419750000 ms
-------------------------------------------
Time: 1492419750000 ms
-------------------------------------------

17/04/17 14:32:30 INFO JobScheduler: Finished job streaming job 1492419750000 ms.0 from job set of time 1492419750000 ms
17/04/17 14:32:30 INFO MapPartitionsRDD: Removing RDD 820 from persistence list
17/04/17 14:32:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419750000 ms (execution: 0.001 s)
17/04/17 14:32:30 INFO BlockManager: Removing RDD 820
17/04/17 14:32:30 INFO MapPartitionsRDD: Removing RDD 819 from persistence list
17/04/17 14:32:30 INFO BlockManager: Removing RDD 819
17/04/17 14:32:30 INFO BlockRDD: Removing RDD 818 from persistence list
17/04/17 14:32:30 INFO BlockManager: Removing RDD 818
17/04/17 14:32:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[818] at createStream at KafkaConsumer.java:64 of time 1492419750000 ms
17/04/17 14:32:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419730000 ms)
17/04/17 14:32:30 INFO InputInfoTracker: remove old batch metadata: 1492419730000 ms
17/04/17 14:32:40 INFO JobScheduler: Added jobs for time 1492419760000 ms
17/04/17 14:32:40 INFO JobScheduler: Starting job streaming job 1492419760000 ms.0 from job set of time 1492419760000 ms
-------------------------------------------
Time: 1492419760000 ms
-------------------------------------------

17/04/17 14:32:40 INFO JobScheduler: Finished job streaming job 1492419760000 ms.0 from job set of time 1492419760000 ms
17/04/17 14:32:40 INFO MapPartitionsRDD: Removing RDD 823 from persistence list
17/04/17 14:32:40 INFO JobScheduler: Total delay: 0.036 s for time 1492419760000 ms (execution: 0.031 s)
17/04/17 14:32:40 INFO BlockManager: Removing RDD 823
17/04/17 14:32:40 INFO MapPartitionsRDD: Removing RDD 822 from persistence list
17/04/17 14:32:40 INFO BlockManager: Removing RDD 822
17/04/17 14:32:40 INFO BlockRDD: Removing RDD 821 from persistence list
17/04/17 14:32:40 INFO BlockManager: Removing RDD 821
17/04/17 14:32:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[821] at createStream at KafkaConsumer.java:64 of time 1492419760000 ms
17/04/17 14:32:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419740000 ms)
17/04/17 14:32:40 INFO InputInfoTracker: remove old batch metadata: 1492419740000 ms
17/04/17 14:32:50 INFO JobScheduler: Added jobs for time 1492419770000 ms
17/04/17 14:32:50 INFO JobScheduler: Starting job streaming job 1492419770000 ms.0 from job set of time 1492419770000 ms
-------------------------------------------
Time: 1492419770000 ms
-------------------------------------------

17/04/17 14:32:50 INFO JobScheduler: Finished job streaming job 1492419770000 ms.0 from job set of time 1492419770000 ms
17/04/17 14:32:50 INFO MapPartitionsRDD: Removing RDD 826 from persistence list
17/04/17 14:32:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419770000 ms (execution: 0.001 s)
17/04/17 14:32:50 INFO BlockManager: Removing RDD 826
17/04/17 14:32:50 INFO MapPartitionsRDD: Removing RDD 825 from persistence list
17/04/17 14:32:50 INFO BlockManager: Removing RDD 825
17/04/17 14:32:50 INFO BlockRDD: Removing RDD 824 from persistence list
17/04/17 14:32:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[824] at createStream at KafkaConsumer.java:64 of time 1492419770000 ms
17/04/17 14:32:50 INFO BlockManager: Removing RDD 824
17/04/17 14:32:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419750000 ms)
17/04/17 14:32:50 INFO InputInfoTracker: remove old batch metadata: 1492419750000 ms
17/04/17 14:33:00 INFO JobScheduler: Added jobs for time 1492419780000 ms
17/04/17 14:33:00 INFO JobScheduler: Starting job streaming job 1492419780000 ms.0 from job set of time 1492419780000 ms
-------------------------------------------
Time: 1492419780000 ms
-------------------------------------------

17/04/17 14:33:00 INFO JobScheduler: Finished job streaming job 1492419780000 ms.0 from job set of time 1492419780000 ms
17/04/17 14:33:00 INFO MapPartitionsRDD: Removing RDD 829 from persistence list
17/04/17 14:33:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419780000 ms (execution: 0.001 s)
17/04/17 14:33:00 INFO BlockManager: Removing RDD 829
17/04/17 14:33:00 INFO MapPartitionsRDD: Removing RDD 828 from persistence list
17/04/17 14:33:00 INFO BlockManager: Removing RDD 828
17/04/17 14:33:00 INFO BlockRDD: Removing RDD 827 from persistence list
17/04/17 14:33:00 INFO BlockManager: Removing RDD 827
17/04/17 14:33:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[827] at createStream at KafkaConsumer.java:64 of time 1492419780000 ms
17/04/17 14:33:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419760000 ms)
17/04/17 14:33:00 INFO InputInfoTracker: remove old batch metadata: 1492419760000 ms
17/04/17 14:33:10 INFO JobScheduler: Added jobs for time 1492419790000 ms
17/04/17 14:33:10 INFO JobScheduler: Starting job streaming job 1492419790000 ms.0 from job set of time 1492419790000 ms
-------------------------------------------
Time: 1492419790000 ms
-------------------------------------------

17/04/17 14:33:10 INFO JobScheduler: Finished job streaming job 1492419790000 ms.0 from job set of time 1492419790000 ms
17/04/17 14:33:10 INFO MapPartitionsRDD: Removing RDD 832 from persistence list
17/04/17 14:33:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419790000 ms (execution: 0.001 s)
17/04/17 14:33:10 INFO BlockManager: Removing RDD 832
17/04/17 14:33:10 INFO MapPartitionsRDD: Removing RDD 831 from persistence list
17/04/17 14:33:10 INFO BlockManager: Removing RDD 831
17/04/17 14:33:10 INFO BlockRDD: Removing RDD 830 from persistence list
17/04/17 14:33:10 INFO BlockManager: Removing RDD 830
17/04/17 14:33:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[830] at createStream at KafkaConsumer.java:64 of time 1492419790000 ms
17/04/17 14:33:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419770000 ms)
17/04/17 14:33:10 INFO InputInfoTracker: remove old batch metadata: 1492419770000 ms
17/04/17 14:33:20 INFO JobScheduler: Added jobs for time 1492419800000 ms
17/04/17 14:33:20 INFO JobScheduler: Starting job streaming job 1492419800000 ms.0 from job set of time 1492419800000 ms
-------------------------------------------
Time: 1492419800000 ms
-------------------------------------------

17/04/17 14:33:20 INFO JobScheduler: Finished job streaming job 1492419800000 ms.0 from job set of time 1492419800000 ms
17/04/17 14:33:20 INFO MapPartitionsRDD: Removing RDD 835 from persistence list
17/04/17 14:33:20 INFO JobScheduler: Total delay: 0.004 s for time 1492419800000 ms (execution: 0.000 s)
17/04/17 14:33:20 INFO BlockManager: Removing RDD 835
17/04/17 14:33:20 INFO MapPartitionsRDD: Removing RDD 834 from persistence list
17/04/17 14:33:20 INFO BlockManager: Removing RDD 834
17/04/17 14:33:20 INFO BlockRDD: Removing RDD 833 from persistence list
17/04/17 14:33:20 INFO BlockManager: Removing RDD 833
17/04/17 14:33:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[833] at createStream at KafkaConsumer.java:64 of time 1492419800000 ms
17/04/17 14:33:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419780000 ms)
17/04/17 14:33:20 INFO InputInfoTracker: remove old batch metadata: 1492419780000 ms
17/04/17 14:33:30 INFO JobScheduler: Added jobs for time 1492419810000 ms
17/04/17 14:33:30 INFO JobScheduler: Starting job streaming job 1492419810000 ms.0 from job set of time 1492419810000 ms
-------------------------------------------
Time: 1492419810000 ms
-------------------------------------------

17/04/17 14:33:30 INFO JobScheduler: Finished job streaming job 1492419810000 ms.0 from job set of time 1492419810000 ms
17/04/17 14:33:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419810000 ms (execution: 0.000 s)
17/04/17 14:33:30 INFO MapPartitionsRDD: Removing RDD 838 from persistence list
17/04/17 14:33:30 INFO BlockManager: Removing RDD 838
17/04/17 14:33:30 INFO MapPartitionsRDD: Removing RDD 837 from persistence list
17/04/17 14:33:30 INFO BlockRDD: Removing RDD 836 from persistence list
17/04/17 14:33:30 INFO BlockManager: Removing RDD 837
17/04/17 14:33:30 INFO BlockManager: Removing RDD 836
17/04/17 14:33:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[836] at createStream at KafkaConsumer.java:64 of time 1492419810000 ms
17/04/17 14:33:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419790000 ms)
17/04/17 14:33:30 INFO InputInfoTracker: remove old batch metadata: 1492419790000 ms
17/04/17 14:33:40 INFO JobScheduler: Added jobs for time 1492419820000 ms
17/04/17 14:33:40 INFO JobScheduler: Starting job streaming job 1492419820000 ms.0 from job set of time 1492419820000 ms
-------------------------------------------
Time: 1492419820000 ms
-------------------------------------------

17/04/17 14:33:40 INFO JobScheduler: Finished job streaming job 1492419820000 ms.0 from job set of time 1492419820000 ms
17/04/17 14:33:40 INFO MapPartitionsRDD: Removing RDD 841 from persistence list
17/04/17 14:33:40 INFO JobScheduler: Total delay: 0.004 s for time 1492419820000 ms (execution: 0.000 s)
17/04/17 14:33:40 INFO BlockManager: Removing RDD 841
17/04/17 14:33:40 INFO MapPartitionsRDD: Removing RDD 840 from persistence list
17/04/17 14:33:40 INFO BlockManager: Removing RDD 840
17/04/17 14:33:40 INFO BlockRDD: Removing RDD 839 from persistence list
17/04/17 14:33:40 INFO BlockManager: Removing RDD 839
17/04/17 14:33:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[839] at createStream at KafkaConsumer.java:64 of time 1492419820000 ms
17/04/17 14:33:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419800000 ms)
17/04/17 14:33:40 INFO InputInfoTracker: remove old batch metadata: 1492419800000 ms
17/04/17 14:33:50 INFO JobScheduler: Added jobs for time 1492419830000 ms
17/04/17 14:33:50 INFO JobScheduler: Starting job streaming job 1492419830000 ms.0 from job set of time 1492419830000 ms
-------------------------------------------
Time: 1492419830000 ms
-------------------------------------------

17/04/17 14:33:50 INFO JobScheduler: Finished job streaming job 1492419830000 ms.0 from job set of time 1492419830000 ms
17/04/17 14:33:50 INFO MapPartitionsRDD: Removing RDD 844 from persistence list
17/04/17 14:33:50 INFO JobScheduler: Total delay: 0.006 s for time 1492419830000 ms (execution: 0.001 s)
17/04/17 14:33:50 INFO BlockManager: Removing RDD 844
17/04/17 14:33:50 INFO MapPartitionsRDD: Removing RDD 843 from persistence list
17/04/17 14:33:50 INFO BlockManager: Removing RDD 843
17/04/17 14:33:50 INFO BlockRDD: Removing RDD 842 from persistence list
17/04/17 14:33:50 INFO BlockManager: Removing RDD 842
17/04/17 14:33:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[842] at createStream at KafkaConsumer.java:64 of time 1492419830000 ms
17/04/17 14:33:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419810000 ms)
17/04/17 14:33:50 INFO InputInfoTracker: remove old batch metadata: 1492419810000 ms
17/04/17 14:34:00 INFO JobScheduler: Added jobs for time 1492419840000 ms
17/04/17 14:34:00 INFO JobScheduler: Starting job streaming job 1492419840000 ms.0 from job set of time 1492419840000 ms
-------------------------------------------
Time: 1492419840000 ms
-------------------------------------------

17/04/17 14:34:00 INFO JobScheduler: Finished job streaming job 1492419840000 ms.0 from job set of time 1492419840000 ms
17/04/17 14:34:00 INFO MapPartitionsRDD: Removing RDD 847 from persistence list
17/04/17 14:34:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419840000 ms (execution: 0.001 s)
17/04/17 14:34:00 INFO BlockManager: Removing RDD 847
17/04/17 14:34:00 INFO MapPartitionsRDD: Removing RDD 846 from persistence list
17/04/17 14:34:00 INFO BlockManager: Removing RDD 846
17/04/17 14:34:00 INFO BlockRDD: Removing RDD 845 from persistence list
17/04/17 14:34:00 INFO BlockManager: Removing RDD 845
17/04/17 14:34:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[845] at createStream at KafkaConsumer.java:64 of time 1492419840000 ms
17/04/17 14:34:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419820000 ms)
17/04/17 14:34:00 INFO InputInfoTracker: remove old batch metadata: 1492419820000 ms
17/04/17 14:34:10 INFO JobScheduler: Added jobs for time 1492419850000 ms
17/04/17 14:34:10 INFO JobScheduler: Starting job streaming job 1492419850000 ms.0 from job set of time 1492419850000 ms
-------------------------------------------
Time: 1492419850000 ms
-------------------------------------------

17/04/17 14:34:10 INFO JobScheduler: Finished job streaming job 1492419850000 ms.0 from job set of time 1492419850000 ms
17/04/17 14:34:10 INFO MapPartitionsRDD: Removing RDD 850 from persistence list
17/04/17 14:34:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419850000 ms (execution: 0.000 s)
17/04/17 14:34:10 INFO BlockManager: Removing RDD 850
17/04/17 14:34:10 INFO MapPartitionsRDD: Removing RDD 849 from persistence list
17/04/17 14:34:10 INFO BlockManager: Removing RDD 849
17/04/17 14:34:10 INFO BlockRDD: Removing RDD 848 from persistence list
17/04/17 14:34:10 INFO BlockManager: Removing RDD 848
17/04/17 14:34:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[848] at createStream at KafkaConsumer.java:64 of time 1492419850000 ms
17/04/17 14:34:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419830000 ms)
17/04/17 14:34:10 INFO InputInfoTracker: remove old batch metadata: 1492419830000 ms
17/04/17 14:34:20 INFO JobScheduler: Added jobs for time 1492419860000 ms
17/04/17 14:34:20 INFO JobScheduler: Starting job streaming job 1492419860000 ms.0 from job set of time 1492419860000 ms
-------------------------------------------
Time: 1492419860000 ms
-------------------------------------------

17/04/17 14:34:20 INFO JobScheduler: Finished job streaming job 1492419860000 ms.0 from job set of time 1492419860000 ms
17/04/17 14:34:20 INFO MapPartitionsRDD: Removing RDD 853 from persistence list
17/04/17 14:34:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419860000 ms (execution: 0.001 s)
17/04/17 14:34:20 INFO BlockManager: Removing RDD 853
17/04/17 14:34:20 INFO MapPartitionsRDD: Removing RDD 852 from persistence list
17/04/17 14:34:20 INFO BlockManager: Removing RDD 852
17/04/17 14:34:20 INFO BlockRDD: Removing RDD 851 from persistence list
17/04/17 14:34:20 INFO BlockManager: Removing RDD 851
17/04/17 14:34:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[851] at createStream at KafkaConsumer.java:64 of time 1492419860000 ms
17/04/17 14:34:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419840000 ms)
17/04/17 14:34:20 INFO InputInfoTracker: remove old batch metadata: 1492419840000 ms
17/04/17 14:34:30 INFO JobScheduler: Added jobs for time 1492419870000 ms
-------------------------------------------
Time: 1492419870000 ms
-------------------------------------------

17/04/17 14:34:30 INFO JobScheduler: Starting job streaming job 1492419870000 ms.0 from job set of time 1492419870000 ms
17/04/17 14:34:30 INFO JobScheduler: Finished job streaming job 1492419870000 ms.0 from job set of time 1492419870000 ms
17/04/17 14:34:30 INFO MapPartitionsRDD: Removing RDD 856 from persistence list
17/04/17 14:34:30 INFO JobScheduler: Total delay: 0.005 s for time 1492419870000 ms (execution: 0.000 s)
17/04/17 14:34:30 INFO BlockManager: Removing RDD 856
17/04/17 14:34:30 INFO MapPartitionsRDD: Removing RDD 855 from persistence list
17/04/17 14:34:30 INFO BlockManager: Removing RDD 855
17/04/17 14:34:30 INFO BlockRDD: Removing RDD 854 from persistence list
17/04/17 14:34:30 INFO BlockManager: Removing RDD 854
17/04/17 14:34:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[854] at createStream at KafkaConsumer.java:64 of time 1492419870000 ms
17/04/17 14:34:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419850000 ms)
17/04/17 14:34:30 INFO InputInfoTracker: remove old batch metadata: 1492419850000 ms
-------------------------------------------
Time: 1492419880000 ms
-------------------------------------------

17/04/17 14:34:40 INFO JobScheduler: Starting job streaming job 1492419880000 ms.0 from job set of time 1492419880000 ms
17/04/17 14:34:40 INFO JobScheduler: Finished job streaming job 1492419880000 ms.0 from job set of time 1492419880000 ms
17/04/17 14:34:40 INFO JobScheduler: Total delay: 0.005 s for time 1492419880000 ms (execution: 0.000 s)
17/04/17 14:34:40 INFO JobScheduler: Added jobs for time 1492419880000 ms
17/04/17 14:34:40 INFO MapPartitionsRDD: Removing RDD 859 from persistence list
17/04/17 14:34:40 INFO BlockManager: Removing RDD 859
17/04/17 14:34:40 INFO MapPartitionsRDD: Removing RDD 858 from persistence list
17/04/17 14:34:40 INFO BlockManager: Removing RDD 858
17/04/17 14:34:40 INFO BlockRDD: Removing RDD 857 from persistence list
17/04/17 14:34:40 INFO BlockManager: Removing RDD 857
17/04/17 14:34:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[857] at createStream at KafkaConsumer.java:64 of time 1492419880000 ms
17/04/17 14:34:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419860000 ms)
17/04/17 14:34:40 INFO InputInfoTracker: remove old batch metadata: 1492419860000 ms
17/04/17 14:34:50 INFO JobScheduler: Added jobs for time 1492419890000 ms
-------------------------------------------
Time: 1492419890000 ms
-------------------------------------------

17/04/17 14:34:50 INFO JobScheduler: Starting job streaming job 1492419890000 ms.0 from job set of time 1492419890000 ms
17/04/17 14:34:50 INFO JobScheduler: Finished job streaming job 1492419890000 ms.0 from job set of time 1492419890000 ms
17/04/17 14:34:50 INFO MapPartitionsRDD: Removing RDD 862 from persistence list
17/04/17 14:34:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419890000 ms (execution: 0.001 s)
17/04/17 14:34:50 INFO BlockManager: Removing RDD 862
17/04/17 14:34:50 INFO MapPartitionsRDD: Removing RDD 861 from persistence list
17/04/17 14:34:50 INFO BlockManager: Removing RDD 861
17/04/17 14:34:50 INFO BlockRDD: Removing RDD 860 from persistence list
17/04/17 14:34:50 INFO BlockManager: Removing RDD 860
17/04/17 14:34:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[860] at createStream at KafkaConsumer.java:64 of time 1492419890000 ms
17/04/17 14:34:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419870000 ms)
17/04/17 14:34:50 INFO InputInfoTracker: remove old batch metadata: 1492419870000 ms
17/04/17 14:35:00 INFO JobScheduler: Added jobs for time 1492419900000 ms
17/04/17 14:35:00 INFO JobScheduler: Starting job streaming job 1492419900000 ms.0 from job set of time 1492419900000 ms
-------------------------------------------
Time: 1492419900000 ms
-------------------------------------------

17/04/17 14:35:00 INFO JobScheduler: Finished job streaming job 1492419900000 ms.0 from job set of time 1492419900000 ms
17/04/17 14:35:00 INFO MapPartitionsRDD: Removing RDD 865 from persistence list
17/04/17 14:35:00 INFO JobScheduler: Total delay: 0.004 s for time 1492419900000 ms (execution: 0.000 s)
17/04/17 14:35:00 INFO BlockManager: Removing RDD 865
17/04/17 14:35:00 INFO MapPartitionsRDD: Removing RDD 864 from persistence list
17/04/17 14:35:00 INFO BlockManager: Removing RDD 864
17/04/17 14:35:00 INFO BlockRDD: Removing RDD 863 from persistence list
17/04/17 14:35:00 INFO BlockManager: Removing RDD 863
17/04/17 14:35:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[863] at createStream at KafkaConsumer.java:64 of time 1492419900000 ms
17/04/17 14:35:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419880000 ms)
17/04/17 14:35:00 INFO InputInfoTracker: remove old batch metadata: 1492419880000 ms
17/04/17 14:35:10 INFO JobScheduler: Added jobs for time 1492419910000 ms
17/04/17 14:35:10 INFO JobScheduler: Starting job streaming job 1492419910000 ms.0 from job set of time 1492419910000 ms
-------------------------------------------
Time: 1492419910000 ms
-------------------------------------------

17/04/17 14:35:10 INFO JobScheduler: Finished job streaming job 1492419910000 ms.0 from job set of time 1492419910000 ms
17/04/17 14:35:10 INFO MapPartitionsRDD: Removing RDD 868 from persistence list
17/04/17 14:35:10 INFO JobScheduler: Total delay: 0.005 s for time 1492419910000 ms (execution: 0.001 s)
17/04/17 14:35:10 INFO BlockManager: Removing RDD 868
17/04/17 14:35:10 INFO MapPartitionsRDD: Removing RDD 867 from persistence list
17/04/17 14:35:10 INFO BlockManager: Removing RDD 867
17/04/17 14:35:10 INFO BlockRDD: Removing RDD 866 from persistence list
17/04/17 14:35:10 INFO BlockManager: Removing RDD 866
17/04/17 14:35:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[866] at createStream at KafkaConsumer.java:64 of time 1492419910000 ms
17/04/17 14:35:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419890000 ms)
17/04/17 14:35:10 INFO InputInfoTracker: remove old batch metadata: 1492419890000 ms
17/04/17 14:35:20 INFO JobScheduler: Added jobs for time 1492419920000 ms
-------------------------------------------
Time: 1492419920000 ms
-------------------------------------------

17/04/17 14:35:20 INFO JobScheduler: Starting job streaming job 1492419920000 ms.0 from job set of time 1492419920000 ms
17/04/17 14:35:20 INFO JobScheduler: Finished job streaming job 1492419920000 ms.0 from job set of time 1492419920000 ms
17/04/17 14:35:20 INFO MapPartitionsRDD: Removing RDD 871 from persistence list
17/04/17 14:35:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419920000 ms (execution: 0.000 s)
17/04/17 14:35:20 INFO BlockManager: Removing RDD 871
17/04/17 14:35:20 INFO MapPartitionsRDD: Removing RDD 870 from persistence list
17/04/17 14:35:20 INFO BlockManager: Removing RDD 870
17/04/17 14:35:20 INFO BlockRDD: Removing RDD 869 from persistence list
17/04/17 14:35:20 INFO BlockManager: Removing RDD 869
17/04/17 14:35:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[869] at createStream at KafkaConsumer.java:64 of time 1492419920000 ms
17/04/17 14:35:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419900000 ms)
17/04/17 14:35:20 INFO InputInfoTracker: remove old batch metadata: 1492419900000 ms
17/04/17 14:35:30 INFO JobScheduler: Added jobs for time 1492419930000 ms
17/04/17 14:35:30 INFO JobScheduler: Starting job streaming job 1492419930000 ms.0 from job set of time 1492419930000 ms
-------------------------------------------
Time: 1492419930000 ms
-------------------------------------------

17/04/17 14:35:30 INFO JobScheduler: Finished job streaming job 1492419930000 ms.0 from job set of time 1492419930000 ms
17/04/17 14:35:30 INFO MapPartitionsRDD: Removing RDD 874 from persistence list
17/04/17 14:35:30 INFO JobScheduler: Total delay: 0.004 s for time 1492419930000 ms (execution: 0.000 s)
17/04/17 14:35:30 INFO BlockManager: Removing RDD 874
17/04/17 14:35:30 INFO MapPartitionsRDD: Removing RDD 873 from persistence list
17/04/17 14:35:30 INFO BlockManager: Removing RDD 873
17/04/17 14:35:30 INFO BlockRDD: Removing RDD 872 from persistence list
17/04/17 14:35:30 INFO BlockManager: Removing RDD 872
17/04/17 14:35:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[872] at createStream at KafkaConsumer.java:64 of time 1492419930000 ms
17/04/17 14:35:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419910000 ms)
17/04/17 14:35:30 INFO InputInfoTracker: remove old batch metadata: 1492419910000 ms
17/04/17 14:35:40 INFO JobScheduler: Added jobs for time 1492419940000 ms
17/04/17 14:35:40 INFO JobScheduler: Starting job streaming job 1492419940000 ms.0 from job set of time 1492419940000 ms
-------------------------------------------
Time: 1492419940000 ms
-------------------------------------------

17/04/17 14:35:40 INFO JobScheduler: Finished job streaming job 1492419940000 ms.0 from job set of time 1492419940000 ms
17/04/17 14:35:40 INFO MapPartitionsRDD: Removing RDD 877 from persistence list
17/04/17 14:35:40 INFO JobScheduler: Total delay: 0.004 s for time 1492419940000 ms (execution: 0.001 s)
17/04/17 14:35:40 INFO BlockManager: Removing RDD 877
17/04/17 14:35:40 INFO MapPartitionsRDD: Removing RDD 876 from persistence list
17/04/17 14:35:40 INFO BlockManager: Removing RDD 876
17/04/17 14:35:40 INFO BlockRDD: Removing RDD 875 from persistence list
17/04/17 14:35:40 INFO BlockManager: Removing RDD 875
17/04/17 14:35:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[875] at createStream at KafkaConsumer.java:64 of time 1492419940000 ms
17/04/17 14:35:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419920000 ms)
17/04/17 14:35:40 INFO InputInfoTracker: remove old batch metadata: 1492419920000 ms
17/04/17 14:35:50 INFO JobScheduler: Added jobs for time 1492419950000 ms
17/04/17 14:35:50 INFO JobScheduler: Starting job streaming job 1492419950000 ms.0 from job set of time 1492419950000 ms
-------------------------------------------
Time: 1492419950000 ms
-------------------------------------------

17/04/17 14:35:50 INFO JobScheduler: Finished job streaming job 1492419950000 ms.0 from job set of time 1492419950000 ms
17/04/17 14:35:50 INFO MapPartitionsRDD: Removing RDD 880 from persistence list
17/04/17 14:35:50 INFO JobScheduler: Total delay: 0.005 s for time 1492419950000 ms (execution: 0.001 s)
17/04/17 14:35:50 INFO BlockManager: Removing RDD 880
17/04/17 14:35:50 INFO MapPartitionsRDD: Removing RDD 879 from persistence list
17/04/17 14:35:50 INFO BlockManager: Removing RDD 879
17/04/17 14:35:50 INFO BlockRDD: Removing RDD 878 from persistence list
17/04/17 14:35:50 INFO BlockManager: Removing RDD 878
17/04/17 14:35:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[878] at createStream at KafkaConsumer.java:64 of time 1492419950000 ms
17/04/17 14:35:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419930000 ms)
17/04/17 14:35:50 INFO InputInfoTracker: remove old batch metadata: 1492419930000 ms
17/04/17 14:36:00 INFO JobScheduler: Added jobs for time 1492419960000 ms
-------------------------------------------
Time: 1492419960000 ms
-------------------------------------------

17/04/17 14:36:00 INFO JobScheduler: Starting job streaming job 1492419960000 ms.0 from job set of time 1492419960000 ms
17/04/17 14:36:00 INFO JobScheduler: Finished job streaming job 1492419960000 ms.0 from job set of time 1492419960000 ms
17/04/17 14:36:00 INFO MapPartitionsRDD: Removing RDD 883 from persistence list
17/04/17 14:36:00 INFO JobScheduler: Total delay: 0.005 s for time 1492419960000 ms (execution: 0.000 s)
17/04/17 14:36:00 INFO BlockManager: Removing RDD 883
17/04/17 14:36:00 INFO MapPartitionsRDD: Removing RDD 882 from persistence list
17/04/17 14:36:00 INFO BlockManager: Removing RDD 882
17/04/17 14:36:00 INFO BlockRDD: Removing RDD 881 from persistence list
17/04/17 14:36:00 INFO BlockManager: Removing RDD 881
17/04/17 14:36:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[881] at createStream at KafkaConsumer.java:64 of time 1492419960000 ms
17/04/17 14:36:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419940000 ms)
17/04/17 14:36:00 INFO InputInfoTracker: remove old batch metadata: 1492419940000 ms
17/04/17 14:36:10 INFO JobScheduler: Added jobs for time 1492419970000 ms
-------------------------------------------
Time: 1492419970000 ms
-------------------------------------------

17/04/17 14:36:10 INFO JobScheduler: Starting job streaming job 1492419970000 ms.0 from job set of time 1492419970000 ms
17/04/17 14:36:10 INFO JobScheduler: Finished job streaming job 1492419970000 ms.0 from job set of time 1492419970000 ms
17/04/17 14:36:10 INFO MapPartitionsRDD: Removing RDD 886 from persistence list
17/04/17 14:36:10 INFO JobScheduler: Total delay: 0.002 s for time 1492419970000 ms (execution: 0.000 s)
17/04/17 14:36:10 INFO BlockManager: Removing RDD 886
17/04/17 14:36:10 INFO MapPartitionsRDD: Removing RDD 885 from persistence list
17/04/17 14:36:10 INFO BlockManager: Removing RDD 885
17/04/17 14:36:10 INFO BlockRDD: Removing RDD 884 from persistence list
17/04/17 14:36:10 INFO BlockManager: Removing RDD 884
17/04/17 14:36:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[884] at createStream at KafkaConsumer.java:64 of time 1492419970000 ms
17/04/17 14:36:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419950000 ms)
17/04/17 14:36:10 INFO InputInfoTracker: remove old batch metadata: 1492419950000 ms
17/04/17 14:36:20 INFO JobScheduler: Added jobs for time 1492419980000 ms
17/04/17 14:36:20 INFO JobScheduler: Starting job streaming job 1492419980000 ms.0 from job set of time 1492419980000 ms
-------------------------------------------
Time: 1492419980000 ms
-------------------------------------------

17/04/17 14:36:20 INFO JobScheduler: Finished job streaming job 1492419980000 ms.0 from job set of time 1492419980000 ms
17/04/17 14:36:20 INFO JobScheduler: Total delay: 0.005 s for time 1492419980000 ms (execution: 0.001 s)
17/04/17 14:36:20 INFO MapPartitionsRDD: Removing RDD 889 from persistence list
17/04/17 14:36:20 INFO BlockManager: Removing RDD 889
17/04/17 14:36:20 INFO MapPartitionsRDD: Removing RDD 888 from persistence list
17/04/17 14:36:20 INFO BlockManager: Removing RDD 888
17/04/17 14:36:20 INFO BlockRDD: Removing RDD 887 from persistence list
17/04/17 14:36:20 INFO BlockManager: Removing RDD 887
17/04/17 14:36:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[887] at createStream at KafkaConsumer.java:64 of time 1492419980000 ms
17/04/17 14:36:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419960000 ms)
17/04/17 14:36:20 INFO InputInfoTracker: remove old batch metadata: 1492419960000 ms
17/04/17 14:36:30 INFO JobScheduler: Added jobs for time 1492419990000 ms
17/04/17 14:36:30 INFO JobScheduler: Starting job streaming job 1492419990000 ms.0 from job set of time 1492419990000 ms
-------------------------------------------
Time: 1492419990000 ms
-------------------------------------------

17/04/17 14:36:30 INFO JobScheduler: Finished job streaming job 1492419990000 ms.0 from job set of time 1492419990000 ms
17/04/17 14:36:30 INFO JobScheduler: Total delay: 0.004 s for time 1492419990000 ms (execution: 0.000 s)
17/04/17 14:36:30 INFO MapPartitionsRDD: Removing RDD 892 from persistence list
17/04/17 14:36:30 INFO BlockManager: Removing RDD 892
17/04/17 14:36:30 INFO MapPartitionsRDD: Removing RDD 891 from persistence list
17/04/17 14:36:30 INFO BlockManager: Removing RDD 891
17/04/17 14:36:30 INFO BlockRDD: Removing RDD 890 from persistence list
17/04/17 14:36:30 INFO BlockManager: Removing RDD 890
17/04/17 14:36:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[890] at createStream at KafkaConsumer.java:64 of time 1492419990000 ms
17/04/17 14:36:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419970000 ms)
17/04/17 14:36:30 INFO InputInfoTracker: remove old batch metadata: 1492419970000 ms
17/04/17 14:36:40 INFO JobScheduler: Added jobs for time 1492420000000 ms
17/04/17 14:36:40 INFO JobScheduler: Starting job streaming job 1492420000000 ms.0 from job set of time 1492420000000 ms
-------------------------------------------
Time: 1492420000000 ms
-------------------------------------------

17/04/17 14:36:40 INFO JobScheduler: Finished job streaming job 1492420000000 ms.0 from job set of time 1492420000000 ms
17/04/17 14:36:40 INFO MapPartitionsRDD: Removing RDD 895 from persistence list
17/04/17 14:36:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420000000 ms (execution: 0.001 s)
17/04/17 14:36:40 INFO BlockManager: Removing RDD 895
17/04/17 14:36:40 INFO MapPartitionsRDD: Removing RDD 894 from persistence list
17/04/17 14:36:40 INFO BlockManager: Removing RDD 894
17/04/17 14:36:40 INFO BlockRDD: Removing RDD 893 from persistence list
17/04/17 14:36:40 INFO BlockManager: Removing RDD 893
17/04/17 14:36:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[893] at createStream at KafkaConsumer.java:64 of time 1492420000000 ms
17/04/17 14:36:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419980000 ms)
17/04/17 14:36:40 INFO InputInfoTracker: remove old batch metadata: 1492419980000 ms
17/04/17 14:36:50 INFO JobScheduler: Added jobs for time 1492420010000 ms
-------------------------------------------
Time: 1492420010000 ms
-------------------------------------------

17/04/17 14:36:50 INFO JobScheduler: Starting job streaming job 1492420010000 ms.0 from job set of time 1492420010000 ms
17/04/17 14:36:50 INFO JobScheduler: Finished job streaming job 1492420010000 ms.0 from job set of time 1492420010000 ms
17/04/17 14:36:50 INFO MapPartitionsRDD: Removing RDD 898 from persistence list
17/04/17 14:36:50 INFO JobScheduler: Total delay: 0.005 s for time 1492420010000 ms (execution: 0.000 s)
17/04/17 14:36:50 INFO BlockManager: Removing RDD 898
17/04/17 14:36:50 INFO MapPartitionsRDD: Removing RDD 897 from persistence list
17/04/17 14:36:50 INFO BlockManager: Removing RDD 897
17/04/17 14:36:50 INFO BlockRDD: Removing RDD 896 from persistence list
17/04/17 14:36:50 INFO BlockManager: Removing RDD 896
17/04/17 14:36:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[896] at createStream at KafkaConsumer.java:64 of time 1492420010000 ms
17/04/17 14:36:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492419990000 ms)
17/04/17 14:36:50 INFO InputInfoTracker: remove old batch metadata: 1492419990000 ms
17/04/17 14:37:00 INFO JobScheduler: Added jobs for time 1492420020000 ms
17/04/17 14:37:00 INFO JobScheduler: Starting job streaming job 1492420020000 ms.0 from job set of time 1492420020000 ms
-------------------------------------------
Time: 1492420020000 ms
-------------------------------------------

17/04/17 14:37:00 INFO JobScheduler: Finished job streaming job 1492420020000 ms.0 from job set of time 1492420020000 ms
17/04/17 14:37:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420020000 ms (execution: 0.000 s)
17/04/17 14:37:00 INFO MapPartitionsRDD: Removing RDD 901 from persistence list
17/04/17 14:37:00 INFO BlockManager: Removing RDD 901
17/04/17 14:37:00 INFO MapPartitionsRDD: Removing RDD 900 from persistence list
17/04/17 14:37:00 INFO BlockManager: Removing RDD 900
17/04/17 14:37:00 INFO BlockRDD: Removing RDD 899 from persistence list
17/04/17 14:37:00 INFO BlockManager: Removing RDD 899
17/04/17 14:37:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[899] at createStream at KafkaConsumer.java:64 of time 1492420020000 ms
17/04/17 14:37:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420000000 ms)
17/04/17 14:37:00 INFO InputInfoTracker: remove old batch metadata: 1492420000000 ms
17/04/17 14:37:10 INFO JobScheduler: Added jobs for time 1492420030000 ms
17/04/17 14:37:10 INFO JobScheduler: Starting job streaming job 1492420030000 ms.0 from job set of time 1492420030000 ms
-------------------------------------------
Time: 1492420030000 ms
-------------------------------------------

17/04/17 14:37:10 INFO JobScheduler: Finished job streaming job 1492420030000 ms.0 from job set of time 1492420030000 ms
17/04/17 14:37:10 INFO JobScheduler: Total delay: 0.006 s for time 1492420030000 ms (execution: 0.001 s)
17/04/17 14:37:10 INFO MapPartitionsRDD: Removing RDD 904 from persistence list
17/04/17 14:37:10 INFO BlockManager: Removing RDD 904
17/04/17 14:37:10 INFO MapPartitionsRDD: Removing RDD 903 from persistence list
17/04/17 14:37:10 INFO BlockManager: Removing RDD 903
17/04/17 14:37:10 INFO BlockRDD: Removing RDD 902 from persistence list
17/04/17 14:37:10 INFO BlockManager: Removing RDD 902
17/04/17 14:37:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[902] at createStream at KafkaConsumer.java:64 of time 1492420030000 ms
17/04/17 14:37:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420010000 ms)
17/04/17 14:37:10 INFO InputInfoTracker: remove old batch metadata: 1492420010000 ms
17/04/17 14:37:20 INFO JobScheduler: Added jobs for time 1492420040000 ms
17/04/17 14:37:20 INFO JobScheduler: Starting job streaming job 1492420040000 ms.0 from job set of time 1492420040000 ms
-------------------------------------------
Time: 1492420040000 ms
-------------------------------------------

17/04/17 14:37:20 INFO JobScheduler: Finished job streaming job 1492420040000 ms.0 from job set of time 1492420040000 ms
17/04/17 14:37:20 INFO MapPartitionsRDD: Removing RDD 907 from persistence list
17/04/17 14:37:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420040000 ms (execution: 0.001 s)
17/04/17 14:37:20 INFO BlockManager: Removing RDD 907
17/04/17 14:37:20 INFO MapPartitionsRDD: Removing RDD 906 from persistence list
17/04/17 14:37:20 INFO BlockManager: Removing RDD 906
17/04/17 14:37:20 INFO BlockRDD: Removing RDD 905 from persistence list
17/04/17 14:37:20 INFO BlockManager: Removing RDD 905
17/04/17 14:37:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[905] at createStream at KafkaConsumer.java:64 of time 1492420040000 ms
17/04/17 14:37:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420020000 ms)
17/04/17 14:37:20 INFO InputInfoTracker: remove old batch metadata: 1492420020000 ms
17/04/17 14:37:30 INFO JobScheduler: Added jobs for time 1492420050000 ms
17/04/17 14:37:30 INFO JobScheduler: Starting job streaming job 1492420050000 ms.0 from job set of time 1492420050000 ms
-------------------------------------------
Time: 1492420050000 ms
-------------------------------------------

17/04/17 14:37:30 INFO JobScheduler: Finished job streaming job 1492420050000 ms.0 from job set of time 1492420050000 ms
17/04/17 14:37:30 INFO MapPartitionsRDD: Removing RDD 910 from persistence list
17/04/17 14:37:30 INFO JobScheduler: Total delay: 0.005 s for time 1492420050000 ms (execution: 0.001 s)
17/04/17 14:37:30 INFO BlockManager: Removing RDD 910
17/04/17 14:37:30 INFO MapPartitionsRDD: Removing RDD 909 from persistence list
17/04/17 14:37:30 INFO BlockManager: Removing RDD 909
17/04/17 14:37:30 INFO BlockRDD: Removing RDD 908 from persistence list
17/04/17 14:37:30 INFO BlockManager: Removing RDD 908
17/04/17 14:37:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[908] at createStream at KafkaConsumer.java:64 of time 1492420050000 ms
17/04/17 14:37:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420030000 ms)
17/04/17 14:37:30 INFO InputInfoTracker: remove old batch metadata: 1492420030000 ms
17/04/17 14:37:40 INFO JobScheduler: Added jobs for time 1492420060000 ms
17/04/17 14:37:40 INFO JobScheduler: Starting job streaming job 1492420060000 ms.0 from job set of time 1492420060000 ms
-------------------------------------------
Time: 1492420060000 ms
-------------------------------------------

17/04/17 14:37:40 INFO JobScheduler: Finished job streaming job 1492420060000 ms.0 from job set of time 1492420060000 ms
17/04/17 14:37:40 INFO MapPartitionsRDD: Removing RDD 913 from persistence list
17/04/17 14:37:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420060000 ms (execution: 0.001 s)
17/04/17 14:37:40 INFO MapPartitionsRDD: Removing RDD 912 from persistence list
17/04/17 14:37:40 INFO BlockManager: Removing RDD 913
17/04/17 14:37:40 INFO BlockManager: Removing RDD 912
17/04/17 14:37:40 INFO BlockRDD: Removing RDD 911 from persistence list
17/04/17 14:37:40 INFO BlockManager: Removing RDD 911
17/04/17 14:37:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[911] at createStream at KafkaConsumer.java:64 of time 1492420060000 ms
17/04/17 14:37:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420040000 ms)
17/04/17 14:37:40 INFO InputInfoTracker: remove old batch metadata: 1492420040000 ms
17/04/17 14:37:50 INFO JobScheduler: Added jobs for time 1492420070000 ms
17/04/17 14:37:50 INFO JobScheduler: Starting job streaming job 1492420070000 ms.0 from job set of time 1492420070000 ms
-------------------------------------------
Time: 1492420070000 ms
-------------------------------------------

17/04/17 14:37:50 INFO JobScheduler: Finished job streaming job 1492420070000 ms.0 from job set of time 1492420070000 ms
17/04/17 14:37:50 INFO MapPartitionsRDD: Removing RDD 916 from persistence list
17/04/17 14:37:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420070000 ms (execution: 0.000 s)
17/04/17 14:37:50 INFO BlockManager: Removing RDD 916
17/04/17 14:37:50 INFO MapPartitionsRDD: Removing RDD 915 from persistence list
17/04/17 14:37:50 INFO BlockManager: Removing RDD 915
17/04/17 14:37:50 INFO BlockRDD: Removing RDD 914 from persistence list
17/04/17 14:37:50 INFO BlockManager: Removing RDD 914
17/04/17 14:37:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[914] at createStream at KafkaConsumer.java:64 of time 1492420070000 ms
17/04/17 14:37:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420050000 ms)
17/04/17 14:37:50 INFO InputInfoTracker: remove old batch metadata: 1492420050000 ms
17/04/17 14:38:00 INFO JobScheduler: Added jobs for time 1492420080000 ms
17/04/17 14:38:00 INFO JobScheduler: Starting job streaming job 1492420080000 ms.0 from job set of time 1492420080000 ms
-------------------------------------------
Time: 1492420080000 ms
-------------------------------------------

17/04/17 14:38:00 INFO JobScheduler: Finished job streaming job 1492420080000 ms.0 from job set of time 1492420080000 ms
17/04/17 14:38:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420080000 ms (execution: 0.000 s)
17/04/17 14:38:00 INFO MapPartitionsRDD: Removing RDD 919 from persistence list
17/04/17 14:38:00 INFO BlockManager: Removing RDD 919
17/04/17 14:38:00 INFO MapPartitionsRDD: Removing RDD 918 from persistence list
17/04/17 14:38:00 INFO BlockManager: Removing RDD 918
17/04/17 14:38:00 INFO BlockRDD: Removing RDD 917 from persistence list
17/04/17 14:38:00 INFO BlockManager: Removing RDD 917
17/04/17 14:38:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[917] at createStream at KafkaConsumer.java:64 of time 1492420080000 ms
17/04/17 14:38:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420060000 ms)
17/04/17 14:38:00 INFO InputInfoTracker: remove old batch metadata: 1492420060000 ms
17/04/17 14:38:10 INFO JobScheduler: Added jobs for time 1492420090000 ms
17/04/17 14:38:10 INFO JobScheduler: Starting job streaming job 1492420090000 ms.0 from job set of time 1492420090000 ms
-------------------------------------------
Time: 1492420090000 ms
-------------------------------------------

17/04/17 14:38:10 INFO JobScheduler: Finished job streaming job 1492420090000 ms.0 from job set of time 1492420090000 ms
17/04/17 14:38:10 INFO MapPartitionsRDD: Removing RDD 922 from persistence list
17/04/17 14:38:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420090000 ms (execution: 0.001 s)
17/04/17 14:38:10 INFO BlockManager: Removing RDD 922
17/04/17 14:38:10 INFO MapPartitionsRDD: Removing RDD 921 from persistence list
17/04/17 14:38:10 INFO BlockManager: Removing RDD 921
17/04/17 14:38:10 INFO BlockRDD: Removing RDD 920 from persistence list
17/04/17 14:38:10 INFO BlockManager: Removing RDD 920
17/04/17 14:38:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[920] at createStream at KafkaConsumer.java:64 of time 1492420090000 ms
17/04/17 14:38:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420070000 ms)
17/04/17 14:38:10 INFO InputInfoTracker: remove old batch metadata: 1492420070000 ms
17/04/17 14:38:20 INFO JobScheduler: Added jobs for time 1492420100000 ms
17/04/17 14:38:20 INFO JobScheduler: Starting job streaming job 1492420100000 ms.0 from job set of time 1492420100000 ms
-------------------------------------------
Time: 1492420100000 ms
-------------------------------------------

17/04/17 14:38:20 INFO JobScheduler: Finished job streaming job 1492420100000 ms.0 from job set of time 1492420100000 ms
17/04/17 14:38:20 INFO MapPartitionsRDD: Removing RDD 925 from persistence list
17/04/17 14:38:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420100000 ms (execution: 0.000 s)
17/04/17 14:38:20 INFO MapPartitionsRDD: Removing RDD 924 from persistence list
17/04/17 14:38:20 INFO BlockManager: Removing RDD 925
17/04/17 14:38:20 INFO BlockManager: Removing RDD 924
17/04/17 14:38:20 INFO BlockRDD: Removing RDD 923 from persistence list
17/04/17 14:38:20 INFO BlockManager: Removing RDD 923
17/04/17 14:38:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[923] at createStream at KafkaConsumer.java:64 of time 1492420100000 ms
17/04/17 14:38:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420080000 ms)
17/04/17 14:38:20 INFO InputInfoTracker: remove old batch metadata: 1492420080000 ms
17/04/17 14:38:30 INFO JobScheduler: Added jobs for time 1492420110000 ms
17/04/17 14:38:30 INFO JobScheduler: Starting job streaming job 1492420110000 ms.0 from job set of time 1492420110000 ms
-------------------------------------------
Time: 1492420110000 ms
-------------------------------------------

17/04/17 14:38:30 INFO JobScheduler: Finished job streaming job 1492420110000 ms.0 from job set of time 1492420110000 ms
17/04/17 14:38:30 INFO MapPartitionsRDD: Removing RDD 928 from persistence list
17/04/17 14:38:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420110000 ms (execution: 0.001 s)
17/04/17 14:38:30 INFO BlockManager: Removing RDD 928
17/04/17 14:38:30 INFO MapPartitionsRDD: Removing RDD 927 from persistence list
17/04/17 14:38:30 INFO BlockManager: Removing RDD 927
17/04/17 14:38:30 INFO BlockRDD: Removing RDD 926 from persistence list
17/04/17 14:38:30 INFO BlockManager: Removing RDD 926
17/04/17 14:38:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[926] at createStream at KafkaConsumer.java:64 of time 1492420110000 ms
17/04/17 14:38:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420090000 ms)
17/04/17 14:38:30 INFO InputInfoTracker: remove old batch metadata: 1492420090000 ms
17/04/17 14:38:40 INFO JobScheduler: Added jobs for time 1492420120000 ms
17/04/17 14:38:40 INFO JobScheduler: Starting job streaming job 1492420120000 ms.0 from job set of time 1492420120000 ms
-------------------------------------------
Time: 1492420120000 ms
-------------------------------------------

17/04/17 14:38:40 INFO JobScheduler: Finished job streaming job 1492420120000 ms.0 from job set of time 1492420120000 ms
17/04/17 14:38:40 INFO MapPartitionsRDD: Removing RDD 931 from persistence list
17/04/17 14:38:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420120000 ms (execution: 0.001 s)
17/04/17 14:38:40 INFO BlockManager: Removing RDD 931
17/04/17 14:38:40 INFO MapPartitionsRDD: Removing RDD 930 from persistence list
17/04/17 14:38:40 INFO BlockManager: Removing RDD 930
17/04/17 14:38:40 INFO BlockRDD: Removing RDD 929 from persistence list
17/04/17 14:38:40 INFO BlockManager: Removing RDD 929
17/04/17 14:38:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[929] at createStream at KafkaConsumer.java:64 of time 1492420120000 ms
17/04/17 14:38:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420100000 ms)
17/04/17 14:38:40 INFO InputInfoTracker: remove old batch metadata: 1492420100000 ms
17/04/17 14:38:50 INFO JobScheduler: Added jobs for time 1492420130000 ms
17/04/17 14:38:50 INFO JobScheduler: Starting job streaming job 1492420130000 ms.0 from job set of time 1492420130000 ms
-------------------------------------------
Time: 1492420130000 ms
-------------------------------------------

17/04/17 14:38:50 INFO JobScheduler: Finished job streaming job 1492420130000 ms.0 from job set of time 1492420130000 ms
17/04/17 14:38:50 INFO MapPartitionsRDD: Removing RDD 934 from persistence list
17/04/17 14:38:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420130000 ms (execution: 0.000 s)
17/04/17 14:38:50 INFO BlockManager: Removing RDD 934
17/04/17 14:38:50 INFO MapPartitionsRDD: Removing RDD 933 from persistence list
17/04/17 14:38:50 INFO BlockManager: Removing RDD 933
17/04/17 14:38:50 INFO BlockRDD: Removing RDD 932 from persistence list
17/04/17 14:38:50 INFO BlockManager: Removing RDD 932
17/04/17 14:38:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[932] at createStream at KafkaConsumer.java:64 of time 1492420130000 ms
17/04/17 14:38:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420110000 ms)
17/04/17 14:38:50 INFO InputInfoTracker: remove old batch metadata: 1492420110000 ms
17/04/17 14:39:00 INFO JobScheduler: Added jobs for time 1492420140000 ms
-------------------------------------------
Time: 1492420140000 ms
-------------------------------------------

17/04/17 14:39:00 INFO JobScheduler: Starting job streaming job 1492420140000 ms.0 from job set of time 1492420140000 ms
17/04/17 14:39:00 INFO JobScheduler: Finished job streaming job 1492420140000 ms.0 from job set of time 1492420140000 ms
17/04/17 14:39:00 INFO MapPartitionsRDD: Removing RDD 937 from persistence list
17/04/17 14:39:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420140000 ms (execution: 0.000 s)
17/04/17 14:39:00 INFO BlockManager: Removing RDD 937
17/04/17 14:39:00 INFO MapPartitionsRDD: Removing RDD 936 from persistence list
17/04/17 14:39:00 INFO BlockManager: Removing RDD 936
17/04/17 14:39:00 INFO BlockRDD: Removing RDD 935 from persistence list
17/04/17 14:39:00 INFO BlockManager: Removing RDD 935
17/04/17 14:39:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[935] at createStream at KafkaConsumer.java:64 of time 1492420140000 ms
17/04/17 14:39:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420120000 ms)
17/04/17 14:39:00 INFO InputInfoTracker: remove old batch metadata: 1492420120000 ms
17/04/17 14:39:10 INFO JobScheduler: Added jobs for time 1492420150000 ms
17/04/17 14:39:10 INFO JobScheduler: Starting job streaming job 1492420150000 ms.0 from job set of time 1492420150000 ms
-------------------------------------------
Time: 1492420150000 ms
-------------------------------------------

17/04/17 14:39:10 INFO JobScheduler: Finished job streaming job 1492420150000 ms.0 from job set of time 1492420150000 ms
17/04/17 14:39:10 INFO MapPartitionsRDD: Removing RDD 940 from persistence list
17/04/17 14:39:10 INFO JobScheduler: Total delay: 0.005 s for time 1492420150000 ms (execution: 0.001 s)
17/04/17 14:39:10 INFO BlockManager: Removing RDD 940
17/04/17 14:39:10 INFO MapPartitionsRDD: Removing RDD 939 from persistence list
17/04/17 14:39:10 INFO BlockManager: Removing RDD 939
17/04/17 14:39:10 INFO BlockRDD: Removing RDD 938 from persistence list
17/04/17 14:39:10 INFO BlockManager: Removing RDD 938
17/04/17 14:39:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[938] at createStream at KafkaConsumer.java:64 of time 1492420150000 ms
17/04/17 14:39:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420130000 ms)
17/04/17 14:39:10 INFO InputInfoTracker: remove old batch metadata: 1492420130000 ms
17/04/17 14:39:20 INFO JobScheduler: Added jobs for time 1492420160000 ms
17/04/17 14:39:20 INFO JobScheduler: Starting job streaming job 1492420160000 ms.0 from job set of time 1492420160000 ms
-------------------------------------------
Time: 1492420160000 ms
-------------------------------------------

17/04/17 14:39:20 INFO JobScheduler: Finished job streaming job 1492420160000 ms.0 from job set of time 1492420160000 ms
17/04/17 14:39:20 INFO MapPartitionsRDD: Removing RDD 943 from persistence list
17/04/17 14:39:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420160000 ms (execution: 0.001 s)
17/04/17 14:39:20 INFO BlockManager: Removing RDD 943
17/04/17 14:39:20 INFO MapPartitionsRDD: Removing RDD 942 from persistence list
17/04/17 14:39:20 INFO BlockManager: Removing RDD 942
17/04/17 14:39:20 INFO BlockRDD: Removing RDD 941 from persistence list
17/04/17 14:39:20 INFO BlockManager: Removing RDD 941
17/04/17 14:39:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[941] at createStream at KafkaConsumer.java:64 of time 1492420160000 ms
17/04/17 14:39:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420140000 ms)
17/04/17 14:39:20 INFO InputInfoTracker: remove old batch metadata: 1492420140000 ms
17/04/17 14:39:30 INFO JobScheduler: Added jobs for time 1492420170000 ms
17/04/17 14:39:30 INFO JobScheduler: Starting job streaming job 1492420170000 ms.0 from job set of time 1492420170000 ms
-------------------------------------------
Time: 1492420170000 ms
-------------------------------------------

17/04/17 14:39:30 INFO JobScheduler: Finished job streaming job 1492420170000 ms.0 from job set of time 1492420170000 ms
17/04/17 14:39:30 INFO MapPartitionsRDD: Removing RDD 946 from persistence list
17/04/17 14:39:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420170000 ms (execution: 0.000 s)
17/04/17 14:39:30 INFO BlockManager: Removing RDD 946
17/04/17 14:39:30 INFO MapPartitionsRDD: Removing RDD 945 from persistence list
17/04/17 14:39:30 INFO BlockManager: Removing RDD 945
17/04/17 14:39:30 INFO BlockRDD: Removing RDD 944 from persistence list
17/04/17 14:39:30 INFO BlockManager: Removing RDD 944
17/04/17 14:39:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[944] at createStream at KafkaConsumer.java:64 of time 1492420170000 ms
17/04/17 14:39:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420150000 ms)
17/04/17 14:39:30 INFO InputInfoTracker: remove old batch metadata: 1492420150000 ms
17/04/17 14:39:40 INFO JobScheduler: Added jobs for time 1492420180000 ms
17/04/17 14:39:40 INFO JobScheduler: Starting job streaming job 1492420180000 ms.0 from job set of time 1492420180000 ms
-------------------------------------------
Time: 1492420180000 ms
-------------------------------------------

17/04/17 14:39:40 INFO JobScheduler: Finished job streaming job 1492420180000 ms.0 from job set of time 1492420180000 ms
17/04/17 14:39:40 INFO MapPartitionsRDD: Removing RDD 949 from persistence list
17/04/17 14:39:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420180000 ms (execution: 0.001 s)
17/04/17 14:39:40 INFO BlockManager: Removing RDD 949
17/04/17 14:39:40 INFO MapPartitionsRDD: Removing RDD 948 from persistence list
17/04/17 14:39:40 INFO BlockManager: Removing RDD 948
17/04/17 14:39:40 INFO BlockRDD: Removing RDD 947 from persistence list
17/04/17 14:39:40 INFO BlockManager: Removing RDD 947
17/04/17 14:39:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[947] at createStream at KafkaConsumer.java:64 of time 1492420180000 ms
17/04/17 14:39:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420160000 ms)
17/04/17 14:39:40 INFO InputInfoTracker: remove old batch metadata: 1492420160000 ms
17/04/17 14:39:50 INFO JobScheduler: Added jobs for time 1492420190000 ms
-------------------------------------------
Time: 1492420190000 ms
-------------------------------------------

17/04/17 14:39:50 INFO JobScheduler: Starting job streaming job 1492420190000 ms.0 from job set of time 1492420190000 ms
17/04/17 14:39:50 INFO JobScheduler: Finished job streaming job 1492420190000 ms.0 from job set of time 1492420190000 ms
17/04/17 14:39:50 INFO MapPartitionsRDD: Removing RDD 952 from persistence list
17/04/17 14:39:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420190000 ms (execution: 0.000 s)
17/04/17 14:39:50 INFO BlockManager: Removing RDD 952
17/04/17 14:39:50 INFO MapPartitionsRDD: Removing RDD 951 from persistence list
17/04/17 14:39:50 INFO BlockManager: Removing RDD 951
17/04/17 14:39:50 INFO BlockRDD: Removing RDD 950 from persistence list
17/04/17 14:39:50 INFO BlockManager: Removing RDD 950
17/04/17 14:39:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[950] at createStream at KafkaConsumer.java:64 of time 1492420190000 ms
17/04/17 14:39:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420170000 ms)
17/04/17 14:39:50 INFO InputInfoTracker: remove old batch metadata: 1492420170000 ms
17/04/17 14:40:00 INFO JobScheduler: Added jobs for time 1492420200000 ms
17/04/17 14:40:00 INFO JobScheduler: Starting job streaming job 1492420200000 ms.0 from job set of time 1492420200000 ms
-------------------------------------------
Time: 1492420200000 ms
-------------------------------------------

17/04/17 14:40:00 INFO JobScheduler: Finished job streaming job 1492420200000 ms.0 from job set of time 1492420200000 ms
17/04/17 14:40:00 INFO MapPartitionsRDD: Removing RDD 955 from persistence list
17/04/17 14:40:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420200000 ms (execution: 0.000 s)
17/04/17 14:40:00 INFO BlockManager: Removing RDD 955
17/04/17 14:40:00 INFO MapPartitionsRDD: Removing RDD 954 from persistence list
17/04/17 14:40:00 INFO BlockManager: Removing RDD 954
17/04/17 14:40:00 INFO BlockRDD: Removing RDD 953 from persistence list
17/04/17 14:40:00 INFO BlockManager: Removing RDD 953
17/04/17 14:40:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[953] at createStream at KafkaConsumer.java:64 of time 1492420200000 ms
17/04/17 14:40:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420180000 ms)
17/04/17 14:40:00 INFO InputInfoTracker: remove old batch metadata: 1492420180000 ms
17/04/17 14:40:10 INFO JobScheduler: Added jobs for time 1492420210000 ms
17/04/17 14:40:10 INFO JobScheduler: Starting job streaming job 1492420210000 ms.0 from job set of time 1492420210000 ms
-------------------------------------------
Time: 1492420210000 ms
-------------------------------------------

17/04/17 14:40:10 INFO JobScheduler: Finished job streaming job 1492420210000 ms.0 from job set of time 1492420210000 ms
17/04/17 14:40:10 INFO MapPartitionsRDD: Removing RDD 958 from persistence list
17/04/17 14:40:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420210000 ms (execution: 0.000 s)
17/04/17 14:40:10 INFO MapPartitionsRDD: Removing RDD 957 from persistence list
17/04/17 14:40:10 INFO BlockManager: Removing RDD 958
17/04/17 14:40:10 INFO BlockManager: Removing RDD 957
17/04/17 14:40:10 INFO BlockRDD: Removing RDD 956 from persistence list
17/04/17 14:40:10 INFO BlockManager: Removing RDD 956
17/04/17 14:40:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[956] at createStream at KafkaConsumer.java:64 of time 1492420210000 ms
17/04/17 14:40:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420190000 ms)
17/04/17 14:40:10 INFO InputInfoTracker: remove old batch metadata: 1492420190000 ms
17/04/17 14:40:20 INFO JobScheduler: Added jobs for time 1492420220000 ms
17/04/17 14:40:20 INFO JobScheduler: Starting job streaming job 1492420220000 ms.0 from job set of time 1492420220000 ms
-------------------------------------------
Time: 1492420220000 ms
-------------------------------------------

17/04/17 14:40:20 INFO JobScheduler: Finished job streaming job 1492420220000 ms.0 from job set of time 1492420220000 ms
17/04/17 14:40:20 INFO MapPartitionsRDD: Removing RDD 961 from persistence list
17/04/17 14:40:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420220000 ms (execution: 0.001 s)
17/04/17 14:40:20 INFO MapPartitionsRDD: Removing RDD 960 from persistence list
17/04/17 14:40:20 INFO BlockManager: Removing RDD 961
17/04/17 14:40:20 INFO BlockManager: Removing RDD 960
17/04/17 14:40:20 INFO BlockRDD: Removing RDD 959 from persistence list
17/04/17 14:40:20 INFO BlockManager: Removing RDD 959
17/04/17 14:40:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[959] at createStream at KafkaConsumer.java:64 of time 1492420220000 ms
17/04/17 14:40:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420200000 ms)
17/04/17 14:40:20 INFO InputInfoTracker: remove old batch metadata: 1492420200000 ms
17/04/17 14:40:30 INFO JobScheduler: Starting job streaming job 1492420230000 ms.0 from job set of time 1492420230000 ms
-------------------------------------------
Time: 1492420230000 ms
-------------------------------------------

17/04/17 14:40:30 INFO JobScheduler: Added jobs for time 1492420230000 ms
17/04/17 14:40:30 INFO JobScheduler: Finished job streaming job 1492420230000 ms.0 from job set of time 1492420230000 ms
17/04/17 14:40:30 INFO MapPartitionsRDD: Removing RDD 964 from persistence list
17/04/17 14:40:30 INFO JobScheduler: Total delay: 0.005 s for time 1492420230000 ms (execution: 0.000 s)
17/04/17 14:40:30 INFO BlockManager: Removing RDD 964
17/04/17 14:40:30 INFO MapPartitionsRDD: Removing RDD 963 from persistence list
17/04/17 14:40:30 INFO BlockManager: Removing RDD 963
17/04/17 14:40:30 INFO BlockRDD: Removing RDD 962 from persistence list
17/04/17 14:40:30 INFO BlockManager: Removing RDD 962
17/04/17 14:40:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[962] at createStream at KafkaConsumer.java:64 of time 1492420230000 ms
17/04/17 14:40:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420210000 ms)
17/04/17 14:40:30 INFO InputInfoTracker: remove old batch metadata: 1492420210000 ms
17/04/17 14:40:40 INFO JobScheduler: Added jobs for time 1492420240000 ms
17/04/17 14:40:40 INFO JobScheduler: Starting job streaming job 1492420240000 ms.0 from job set of time 1492420240000 ms
-------------------------------------------
Time: 1492420240000 ms
-------------------------------------------

17/04/17 14:40:40 INFO JobScheduler: Finished job streaming job 1492420240000 ms.0 from job set of time 1492420240000 ms
17/04/17 14:40:40 INFO MapPartitionsRDD: Removing RDD 967 from persistence list
17/04/17 14:40:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420240000 ms (execution: 0.000 s)
17/04/17 14:40:40 INFO BlockManager: Removing RDD 967
17/04/17 14:40:40 INFO MapPartitionsRDD: Removing RDD 966 from persistence list
17/04/17 14:40:40 INFO BlockManager: Removing RDD 966
17/04/17 14:40:40 INFO BlockRDD: Removing RDD 965 from persistence list
17/04/17 14:40:40 INFO BlockManager: Removing RDD 965
17/04/17 14:40:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[965] at createStream at KafkaConsumer.java:64 of time 1492420240000 ms
17/04/17 14:40:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420220000 ms)
17/04/17 14:40:40 INFO InputInfoTracker: remove old batch metadata: 1492420220000 ms
17/04/17 14:40:50 INFO JobScheduler: Added jobs for time 1492420250000 ms
17/04/17 14:40:50 INFO JobScheduler: Starting job streaming job 1492420250000 ms.0 from job set of time 1492420250000 ms
-------------------------------------------
Time: 1492420250000 ms
-------------------------------------------

17/04/17 14:40:50 INFO JobScheduler: Finished job streaming job 1492420250000 ms.0 from job set of time 1492420250000 ms
17/04/17 14:40:50 INFO MapPartitionsRDD: Removing RDD 970 from persistence list
17/04/17 14:40:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420250000 ms (execution: 0.000 s)
17/04/17 14:40:50 INFO BlockManager: Removing RDD 970
17/04/17 14:40:50 INFO MapPartitionsRDD: Removing RDD 969 from persistence list
17/04/17 14:40:50 INFO BlockManager: Removing RDD 969
17/04/17 14:40:50 INFO BlockRDD: Removing RDD 968 from persistence list
17/04/17 14:40:50 INFO BlockManager: Removing RDD 968
17/04/17 14:40:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[968] at createStream at KafkaConsumer.java:64 of time 1492420250000 ms
17/04/17 14:40:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420230000 ms)
17/04/17 14:40:50 INFO InputInfoTracker: remove old batch metadata: 1492420230000 ms
17/04/17 14:41:00 INFO JobScheduler: Added jobs for time 1492420260000 ms
17/04/17 14:41:00 INFO JobScheduler: Starting job streaming job 1492420260000 ms.0 from job set of time 1492420260000 ms
-------------------------------------------
Time: 1492420260000 ms
-------------------------------------------

17/04/17 14:41:00 INFO JobScheduler: Finished job streaming job 1492420260000 ms.0 from job set of time 1492420260000 ms
17/04/17 14:41:00 INFO MapPartitionsRDD: Removing RDD 973 from persistence list
17/04/17 14:41:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420260000 ms (execution: 0.001 s)
17/04/17 14:41:00 INFO BlockManager: Removing RDD 973
17/04/17 14:41:00 INFO MapPartitionsRDD: Removing RDD 972 from persistence list
17/04/17 14:41:00 INFO BlockManager: Removing RDD 972
17/04/17 14:41:00 INFO BlockRDD: Removing RDD 971 from persistence list
17/04/17 14:41:00 INFO BlockManager: Removing RDD 971
17/04/17 14:41:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[971] at createStream at KafkaConsumer.java:64 of time 1492420260000 ms
17/04/17 14:41:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420240000 ms)
17/04/17 14:41:00 INFO InputInfoTracker: remove old batch metadata: 1492420240000 ms
17/04/17 14:41:10 INFO JobScheduler: Added jobs for time 1492420270000 ms
17/04/17 14:41:10 INFO JobScheduler: Starting job streaming job 1492420270000 ms.0 from job set of time 1492420270000 ms
-------------------------------------------
Time: 1492420270000 ms
-------------------------------------------

17/04/17 14:41:10 INFO JobScheduler: Finished job streaming job 1492420270000 ms.0 from job set of time 1492420270000 ms
17/04/17 14:41:10 INFO MapPartitionsRDD: Removing RDD 976 from persistence list
17/04/17 14:41:10 INFO JobScheduler: Total delay: 0.005 s for time 1492420270000 ms (execution: 0.001 s)
17/04/17 14:41:10 INFO BlockManager: Removing RDD 976
17/04/17 14:41:10 INFO MapPartitionsRDD: Removing RDD 975 from persistence list
17/04/17 14:41:10 INFO BlockManager: Removing RDD 975
17/04/17 14:41:10 INFO BlockRDD: Removing RDD 974 from persistence list
17/04/17 14:41:10 INFO BlockManager: Removing RDD 974
17/04/17 14:41:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[974] at createStream at KafkaConsumer.java:64 of time 1492420270000 ms
17/04/17 14:41:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420250000 ms)
17/04/17 14:41:10 INFO InputInfoTracker: remove old batch metadata: 1492420250000 ms
17/04/17 14:41:20 INFO JobScheduler: Added jobs for time 1492420280000 ms
17/04/17 14:41:20 INFO JobScheduler: Starting job streaming job 1492420280000 ms.0 from job set of time 1492420280000 ms
-------------------------------------------
Time: 1492420280000 ms
-------------------------------------------

17/04/17 14:41:20 INFO JobScheduler: Finished job streaming job 1492420280000 ms.0 from job set of time 1492420280000 ms
17/04/17 14:41:20 INFO MapPartitionsRDD: Removing RDD 979 from persistence list
17/04/17 14:41:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420280000 ms (execution: 0.000 s)
17/04/17 14:41:20 INFO BlockManager: Removing RDD 979
17/04/17 14:41:20 INFO MapPartitionsRDD: Removing RDD 978 from persistence list
17/04/17 14:41:20 INFO BlockManager: Removing RDD 978
17/04/17 14:41:20 INFO BlockRDD: Removing RDD 977 from persistence list
17/04/17 14:41:20 INFO BlockManager: Removing RDD 977
17/04/17 14:41:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[977] at createStream at KafkaConsumer.java:64 of time 1492420280000 ms
17/04/17 14:41:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420260000 ms)
17/04/17 14:41:20 INFO InputInfoTracker: remove old batch metadata: 1492420260000 ms
17/04/17 14:41:30 INFO JobScheduler: Added jobs for time 1492420290000 ms
17/04/17 14:41:30 INFO JobScheduler: Starting job streaming job 1492420290000 ms.0 from job set of time 1492420290000 ms
-------------------------------------------
Time: 1492420290000 ms
-------------------------------------------

17/04/17 14:41:30 INFO JobScheduler: Finished job streaming job 1492420290000 ms.0 from job set of time 1492420290000 ms
17/04/17 14:41:30 INFO MapPartitionsRDD: Removing RDD 982 from persistence list
17/04/17 14:41:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420290000 ms (execution: 0.000 s)
17/04/17 14:41:30 INFO BlockManager: Removing RDD 982
17/04/17 14:41:30 INFO MapPartitionsRDD: Removing RDD 981 from persistence list
17/04/17 14:41:30 INFO BlockManager: Removing RDD 981
17/04/17 14:41:30 INFO BlockRDD: Removing RDD 980 from persistence list
17/04/17 14:41:30 INFO BlockManager: Removing RDD 980
17/04/17 14:41:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[980] at createStream at KafkaConsumer.java:64 of time 1492420290000 ms
17/04/17 14:41:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420270000 ms)
17/04/17 14:41:30 INFO InputInfoTracker: remove old batch metadata: 1492420270000 ms
17/04/17 14:41:40 INFO JobScheduler: Added jobs for time 1492420300000 ms
17/04/17 14:41:40 INFO JobScheduler: Starting job streaming job 1492420300000 ms.0 from job set of time 1492420300000 ms
-------------------------------------------
Time: 1492420300000 ms
-------------------------------------------

17/04/17 14:41:40 INFO JobScheduler: Finished job streaming job 1492420300000 ms.0 from job set of time 1492420300000 ms
17/04/17 14:41:40 INFO MapPartitionsRDD: Removing RDD 985 from persistence list
17/04/17 14:41:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420300000 ms (execution: 0.000 s)
17/04/17 14:41:40 INFO BlockManager: Removing RDD 985
17/04/17 14:41:40 INFO MapPartitionsRDD: Removing RDD 984 from persistence list
17/04/17 14:41:40 INFO BlockManager: Removing RDD 984
17/04/17 14:41:40 INFO BlockRDD: Removing RDD 983 from persistence list
17/04/17 14:41:40 INFO BlockManager: Removing RDD 983
17/04/17 14:41:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[983] at createStream at KafkaConsumer.java:64 of time 1492420300000 ms
17/04/17 14:41:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420280000 ms)
17/04/17 14:41:40 INFO InputInfoTracker: remove old batch metadata: 1492420280000 ms
17/04/17 14:41:50 INFO JobScheduler: Added jobs for time 1492420310000 ms
17/04/17 14:41:50 INFO JobScheduler: Starting job streaming job 1492420310000 ms.0 from job set of time 1492420310000 ms
-------------------------------------------
Time: 1492420310000 ms
-------------------------------------------

17/04/17 14:41:50 INFO JobScheduler: Finished job streaming job 1492420310000 ms.0 from job set of time 1492420310000 ms
17/04/17 14:41:50 INFO MapPartitionsRDD: Removing RDD 988 from persistence list
17/04/17 14:41:50 INFO JobScheduler: Total delay: 0.005 s for time 1492420310000 ms (execution: 0.001 s)
17/04/17 14:41:50 INFO BlockManager: Removing RDD 988
17/04/17 14:41:50 INFO MapPartitionsRDD: Removing RDD 987 from persistence list
17/04/17 14:41:50 INFO BlockManager: Removing RDD 987
17/04/17 14:41:50 INFO BlockRDD: Removing RDD 986 from persistence list
17/04/17 14:41:50 INFO BlockManager: Removing RDD 986
17/04/17 14:41:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[986] at createStream at KafkaConsumer.java:64 of time 1492420310000 ms
17/04/17 14:41:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420290000 ms)
17/04/17 14:41:50 INFO InputInfoTracker: remove old batch metadata: 1492420290000 ms
17/04/17 14:42:00 INFO JobScheduler: Added jobs for time 1492420320000 ms
-------------------------------------------
Time: 1492420320000 ms
-------------------------------------------

17/04/17 14:42:00 INFO JobScheduler: Starting job streaming job 1492420320000 ms.0 from job set of time 1492420320000 ms
17/04/17 14:42:00 INFO JobScheduler: Finished job streaming job 1492420320000 ms.0 from job set of time 1492420320000 ms
17/04/17 14:42:00 INFO MapPartitionsRDD: Removing RDD 991 from persistence list
17/04/17 14:42:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420320000 ms (execution: 0.000 s)
17/04/17 14:42:00 INFO BlockManager: Removing RDD 991
17/04/17 14:42:00 INFO MapPartitionsRDD: Removing RDD 990 from persistence list
17/04/17 14:42:00 INFO BlockManager: Removing RDD 990
17/04/17 14:42:00 INFO BlockRDD: Removing RDD 989 from persistence list
17/04/17 14:42:00 INFO BlockManager: Removing RDD 989
17/04/17 14:42:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[989] at createStream at KafkaConsumer.java:64 of time 1492420320000 ms
17/04/17 14:42:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420300000 ms)
17/04/17 14:42:00 INFO InputInfoTracker: remove old batch metadata: 1492420300000 ms
17/04/17 14:42:10 INFO JobScheduler: Added jobs for time 1492420330000 ms
-------------------------------------------
Time: 1492420330000 ms
-------------------------------------------

17/04/17 14:42:10 INFO JobScheduler: Starting job streaming job 1492420330000 ms.0 from job set of time 1492420330000 ms
17/04/17 14:42:10 INFO JobScheduler: Finished job streaming job 1492420330000 ms.0 from job set of time 1492420330000 ms
17/04/17 14:42:10 INFO MapPartitionsRDD: Removing RDD 994 from persistence list
17/04/17 14:42:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420330000 ms (execution: 0.000 s)
17/04/17 14:42:10 INFO BlockManager: Removing RDD 994
17/04/17 14:42:10 INFO MapPartitionsRDD: Removing RDD 993 from persistence list
17/04/17 14:42:10 INFO BlockManager: Removing RDD 993
17/04/17 14:42:10 INFO BlockRDD: Removing RDD 992 from persistence list
17/04/17 14:42:10 INFO BlockManager: Removing RDD 992
17/04/17 14:42:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[992] at createStream at KafkaConsumer.java:64 of time 1492420330000 ms
17/04/17 14:42:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420310000 ms)
17/04/17 14:42:10 INFO InputInfoTracker: remove old batch metadata: 1492420310000 ms
17/04/17 14:42:20 INFO JobScheduler: Added jobs for time 1492420340000 ms
17/04/17 14:42:20 INFO JobScheduler: Starting job streaming job 1492420340000 ms.0 from job set of time 1492420340000 ms
-------------------------------------------
Time: 1492420340000 ms
-------------------------------------------

17/04/17 14:42:20 INFO JobScheduler: Finished job streaming job 1492420340000 ms.0 from job set of time 1492420340000 ms
17/04/17 14:42:20 INFO MapPartitionsRDD: Removing RDD 997 from persistence list
17/04/17 14:42:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420340000 ms (execution: 0.000 s)
17/04/17 14:42:20 INFO BlockManager: Removing RDD 997
17/04/17 14:42:20 INFO MapPartitionsRDD: Removing RDD 996 from persistence list
17/04/17 14:42:20 INFO BlockManager: Removing RDD 996
17/04/17 14:42:20 INFO BlockRDD: Removing RDD 995 from persistence list
17/04/17 14:42:20 INFO BlockManager: Removing RDD 995
17/04/17 14:42:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[995] at createStream at KafkaConsumer.java:64 of time 1492420340000 ms
17/04/17 14:42:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420320000 ms)
17/04/17 14:42:20 INFO InputInfoTracker: remove old batch metadata: 1492420320000 ms
17/04/17 14:42:30 INFO JobScheduler: Added jobs for time 1492420350000 ms
17/04/17 14:42:30 INFO JobScheduler: Starting job streaming job 1492420350000 ms.0 from job set of time 1492420350000 ms
-------------------------------------------
Time: 1492420350000 ms
-------------------------------------------

17/04/17 14:42:30 INFO JobScheduler: Finished job streaming job 1492420350000 ms.0 from job set of time 1492420350000 ms
17/04/17 14:42:30 INFO MapPartitionsRDD: Removing RDD 1000 from persistence list
17/04/17 14:42:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420350000 ms (execution: 0.000 s)
17/04/17 14:42:30 INFO BlockManager: Removing RDD 1000
17/04/17 14:42:30 INFO MapPartitionsRDD: Removing RDD 999 from persistence list
17/04/17 14:42:30 INFO BlockManager: Removing RDD 999
17/04/17 14:42:30 INFO BlockRDD: Removing RDD 998 from persistence list
17/04/17 14:42:30 INFO BlockManager: Removing RDD 998
17/04/17 14:42:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[998] at createStream at KafkaConsumer.java:64 of time 1492420350000 ms
17/04/17 14:42:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420330000 ms)
17/04/17 14:42:30 INFO InputInfoTracker: remove old batch metadata: 1492420330000 ms
17/04/17 14:42:40 INFO JobScheduler: Added jobs for time 1492420360000 ms
-------------------------------------------
Time: 1492420360000 ms
-------------------------------------------

17/04/17 14:42:40 INFO JobScheduler: Starting job streaming job 1492420360000 ms.0 from job set of time 1492420360000 ms
17/04/17 14:42:40 INFO JobScheduler: Finished job streaming job 1492420360000 ms.0 from job set of time 1492420360000 ms
17/04/17 14:42:40 INFO MapPartitionsRDD: Removing RDD 1003 from persistence list
17/04/17 14:42:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420360000 ms (execution: 0.000 s)
17/04/17 14:42:40 INFO BlockManager: Removing RDD 1003
17/04/17 14:42:40 INFO MapPartitionsRDD: Removing RDD 1002 from persistence list
17/04/17 14:42:40 INFO BlockRDD: Removing RDD 1001 from persistence list
17/04/17 14:42:40 INFO BlockManager: Removing RDD 1002
17/04/17 14:42:40 INFO BlockManager: Removing RDD 1001
17/04/17 14:42:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1001] at createStream at KafkaConsumer.java:64 of time 1492420360000 ms
17/04/17 14:42:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420340000 ms)
17/04/17 14:42:40 INFO InputInfoTracker: remove old batch metadata: 1492420340000 ms
17/04/17 14:42:50 INFO JobScheduler: Added jobs for time 1492420370000 ms
17/04/17 14:42:50 INFO JobScheduler: Starting job streaming job 1492420370000 ms.0 from job set of time 1492420370000 ms
-------------------------------------------
Time: 1492420370000 ms
-------------------------------------------

17/04/17 14:42:50 INFO JobScheduler: Finished job streaming job 1492420370000 ms.0 from job set of time 1492420370000 ms
17/04/17 14:42:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420370000 ms (execution: 0.000 s)
17/04/17 14:42:50 INFO MapPartitionsRDD: Removing RDD 1006 from persistence list
17/04/17 14:42:50 INFO BlockManager: Removing RDD 1006
17/04/17 14:42:50 INFO MapPartitionsRDD: Removing RDD 1005 from persistence list
17/04/17 14:42:50 INFO BlockManager: Removing RDD 1005
17/04/17 14:42:50 INFO BlockRDD: Removing RDD 1004 from persistence list
17/04/17 14:42:50 INFO BlockManager: Removing RDD 1004
17/04/17 14:42:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1004] at createStream at KafkaConsumer.java:64 of time 1492420370000 ms
17/04/17 14:42:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420350000 ms)
17/04/17 14:42:50 INFO InputInfoTracker: remove old batch metadata: 1492420350000 ms
17/04/17 14:43:00 INFO JobScheduler: Added jobs for time 1492420380000 ms
17/04/17 14:43:00 INFO JobScheduler: Starting job streaming job 1492420380000 ms.0 from job set of time 1492420380000 ms
-------------------------------------------
Time: 1492420380000 ms
-------------------------------------------

17/04/17 14:43:00 INFO JobScheduler: Finished job streaming job 1492420380000 ms.0 from job set of time 1492420380000 ms
17/04/17 14:43:00 INFO MapPartitionsRDD: Removing RDD 1009 from persistence list
17/04/17 14:43:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420380000 ms (execution: 0.000 s)
17/04/17 14:43:00 INFO BlockManager: Removing RDD 1009
17/04/17 14:43:00 INFO MapPartitionsRDD: Removing RDD 1008 from persistence list
17/04/17 14:43:00 INFO BlockManager: Removing RDD 1008
17/04/17 14:43:00 INFO BlockRDD: Removing RDD 1007 from persistence list
17/04/17 14:43:00 INFO BlockManager: Removing RDD 1007
17/04/17 14:43:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1007] at createStream at KafkaConsumer.java:64 of time 1492420380000 ms
17/04/17 14:43:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420360000 ms)
17/04/17 14:43:00 INFO InputInfoTracker: remove old batch metadata: 1492420360000 ms
17/04/17 14:43:10 INFO JobScheduler: Added jobs for time 1492420390000 ms
-------------------------------------------
Time: 1492420390000 ms
-------------------------------------------

17/04/17 14:43:10 INFO JobScheduler: Starting job streaming job 1492420390000 ms.0 from job set of time 1492420390000 ms
17/04/17 14:43:10 INFO JobScheduler: Finished job streaming job 1492420390000 ms.0 from job set of time 1492420390000 ms
17/04/17 14:43:10 INFO MapPartitionsRDD: Removing RDD 1012 from persistence list
17/04/17 14:43:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420390000 ms (execution: 0.000 s)
17/04/17 14:43:10 INFO MapPartitionsRDD: Removing RDD 1011 from persistence list
17/04/17 14:43:10 INFO BlockManager: Removing RDD 1012
17/04/17 14:43:10 INFO BlockRDD: Removing RDD 1010 from persistence list
17/04/17 14:43:10 INFO BlockManager: Removing RDD 1011
17/04/17 14:43:10 INFO BlockManager: Removing RDD 1010
17/04/17 14:43:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1010] at createStream at KafkaConsumer.java:64 of time 1492420390000 ms
17/04/17 14:43:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420370000 ms)
17/04/17 14:43:10 INFO InputInfoTracker: remove old batch metadata: 1492420370000 ms
17/04/17 14:43:20 INFO JobScheduler: Added jobs for time 1492420400000 ms
17/04/17 14:43:20 INFO JobScheduler: Starting job streaming job 1492420400000 ms.0 from job set of time 1492420400000 ms
-------------------------------------------
Time: 1492420400000 ms
-------------------------------------------

17/04/17 14:43:20 INFO JobScheduler: Finished job streaming job 1492420400000 ms.0 from job set of time 1492420400000 ms
17/04/17 14:43:20 INFO MapPartitionsRDD: Removing RDD 1015 from persistence list
17/04/17 14:43:20 INFO JobScheduler: Total delay: 0.006 s for time 1492420400000 ms (execution: 0.001 s)
17/04/17 14:43:20 INFO BlockManager: Removing RDD 1015
17/04/17 14:43:20 INFO MapPartitionsRDD: Removing RDD 1014 from persistence list
17/04/17 14:43:20 INFO BlockManager: Removing RDD 1014
17/04/17 14:43:20 INFO BlockRDD: Removing RDD 1013 from persistence list
17/04/17 14:43:20 INFO BlockManager: Removing RDD 1013
17/04/17 14:43:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1013] at createStream at KafkaConsumer.java:64 of time 1492420400000 ms
17/04/17 14:43:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420380000 ms)
17/04/17 14:43:20 INFO InputInfoTracker: remove old batch metadata: 1492420380000 ms
17/04/17 14:43:30 INFO JobScheduler: Added jobs for time 1492420410000 ms
17/04/17 14:43:30 INFO JobScheduler: Starting job streaming job 1492420410000 ms.0 from job set of time 1492420410000 ms
-------------------------------------------
Time: 1492420410000 ms
-------------------------------------------

17/04/17 14:43:30 INFO JobScheduler: Finished job streaming job 1492420410000 ms.0 from job set of time 1492420410000 ms
17/04/17 14:43:30 INFO MapPartitionsRDD: Removing RDD 1018 from persistence list
17/04/17 14:43:30 INFO JobScheduler: Total delay: 0.005 s for time 1492420410000 ms (execution: 0.001 s)
17/04/17 14:43:30 INFO BlockManager: Removing RDD 1018
17/04/17 14:43:30 INFO MapPartitionsRDD: Removing RDD 1017 from persistence list
17/04/17 14:43:30 INFO BlockManager: Removing RDD 1017
17/04/17 14:43:30 INFO BlockRDD: Removing RDD 1016 from persistence list
17/04/17 14:43:30 INFO BlockManager: Removing RDD 1016
17/04/17 14:43:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1016] at createStream at KafkaConsumer.java:64 of time 1492420410000 ms
17/04/17 14:43:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420390000 ms)
17/04/17 14:43:30 INFO InputInfoTracker: remove old batch metadata: 1492420390000 ms
17/04/17 14:43:40 INFO JobScheduler: Added jobs for time 1492420420000 ms
17/04/17 14:43:40 INFO JobScheduler: Starting job streaming job 1492420420000 ms.0 from job set of time 1492420420000 ms
-------------------------------------------
Time: 1492420420000 ms
-------------------------------------------

17/04/17 14:43:40 INFO JobScheduler: Finished job streaming job 1492420420000 ms.0 from job set of time 1492420420000 ms
17/04/17 14:43:40 INFO MapPartitionsRDD: Removing RDD 1021 from persistence list
17/04/17 14:43:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420420000 ms (execution: 0.001 s)
17/04/17 14:43:40 INFO MapPartitionsRDD: Removing RDD 1020 from persistence list
17/04/17 14:43:40 INFO BlockManager: Removing RDD 1021
17/04/17 14:43:40 INFO BlockManager: Removing RDD 1020
17/04/17 14:43:40 INFO BlockRDD: Removing RDD 1019 from persistence list
17/04/17 14:43:40 INFO BlockManager: Removing RDD 1019
17/04/17 14:43:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1019] at createStream at KafkaConsumer.java:64 of time 1492420420000 ms
17/04/17 14:43:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420400000 ms)
17/04/17 14:43:40 INFO InputInfoTracker: remove old batch metadata: 1492420400000 ms
17/04/17 14:43:50 INFO JobScheduler: Added jobs for time 1492420430000 ms
17/04/17 14:43:50 INFO JobScheduler: Starting job streaming job 1492420430000 ms.0 from job set of time 1492420430000 ms
-------------------------------------------
Time: 1492420430000 ms
-------------------------------------------

17/04/17 14:43:50 INFO JobScheduler: Finished job streaming job 1492420430000 ms.0 from job set of time 1492420430000 ms
17/04/17 14:43:50 INFO MapPartitionsRDD: Removing RDD 1024 from persistence list
17/04/17 14:43:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420430000 ms (execution: 0.001 s)
17/04/17 14:43:50 INFO BlockManager: Removing RDD 1024
17/04/17 14:43:50 INFO MapPartitionsRDD: Removing RDD 1023 from persistence list
17/04/17 14:43:50 INFO BlockManager: Removing RDD 1023
17/04/17 14:43:50 INFO BlockRDD: Removing RDD 1022 from persistence list
17/04/17 14:43:50 INFO BlockManager: Removing RDD 1022
17/04/17 14:43:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1022] at createStream at KafkaConsumer.java:64 of time 1492420430000 ms
17/04/17 14:43:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420410000 ms)
17/04/17 14:43:50 INFO InputInfoTracker: remove old batch metadata: 1492420410000 ms
17/04/17 14:44:00 INFO JobScheduler: Added jobs for time 1492420440000 ms
17/04/17 14:44:00 INFO JobScheduler: Starting job streaming job 1492420440000 ms.0 from job set of time 1492420440000 ms
-------------------------------------------
Time: 1492420440000 ms
-------------------------------------------

17/04/17 14:44:00 INFO JobScheduler: Finished job streaming job 1492420440000 ms.0 from job set of time 1492420440000 ms
17/04/17 14:44:00 INFO MapPartitionsRDD: Removing RDD 1027 from persistence list
17/04/17 14:44:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420440000 ms (execution: 0.001 s)
17/04/17 14:44:00 INFO BlockManager: Removing RDD 1027
17/04/17 14:44:00 INFO MapPartitionsRDD: Removing RDD 1026 from persistence list
17/04/17 14:44:00 INFO BlockManager: Removing RDD 1026
17/04/17 14:44:00 INFO BlockRDD: Removing RDD 1025 from persistence list
17/04/17 14:44:00 INFO BlockManager: Removing RDD 1025
17/04/17 14:44:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1025] at createStream at KafkaConsumer.java:64 of time 1492420440000 ms
17/04/17 14:44:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420420000 ms)
17/04/17 14:44:00 INFO InputInfoTracker: remove old batch metadata: 1492420420000 ms
17/04/17 14:44:10 INFO JobScheduler: Added jobs for time 1492420450000 ms
17/04/17 14:44:10 INFO JobScheduler: Starting job streaming job 1492420450000 ms.0 from job set of time 1492420450000 ms
-------------------------------------------
Time: 1492420450000 ms
-------------------------------------------

17/04/17 14:44:10 INFO JobScheduler: Finished job streaming job 1492420450000 ms.0 from job set of time 1492420450000 ms
17/04/17 14:44:10 INFO MapPartitionsRDD: Removing RDD 1030 from persistence list
17/04/17 14:44:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420450000 ms (execution: 0.000 s)
17/04/17 14:44:10 INFO BlockManager: Removing RDD 1030
17/04/17 14:44:10 INFO MapPartitionsRDD: Removing RDD 1029 from persistence list
17/04/17 14:44:10 INFO BlockManager: Removing RDD 1029
17/04/17 14:44:10 INFO BlockRDD: Removing RDD 1028 from persistence list
17/04/17 14:44:10 INFO BlockManager: Removing RDD 1028
17/04/17 14:44:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1028] at createStream at KafkaConsumer.java:64 of time 1492420450000 ms
17/04/17 14:44:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420430000 ms)
17/04/17 14:44:10 INFO InputInfoTracker: remove old batch metadata: 1492420430000 ms
17/04/17 14:44:20 INFO JobScheduler: Starting job streaming job 1492420460000 ms.0 from job set of time 1492420460000 ms
-------------------------------------------
Time: 1492420460000 ms
-------------------------------------------

17/04/17 14:44:20 INFO JobScheduler: Added jobs for time 1492420460000 ms
17/04/17 14:44:20 INFO JobScheduler: Finished job streaming job 1492420460000 ms.0 from job set of time 1492420460000 ms
17/04/17 14:44:20 INFO MapPartitionsRDD: Removing RDD 1033 from persistence list
17/04/17 14:44:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420460000 ms (execution: 0.001 s)
17/04/17 14:44:20 INFO BlockManager: Removing RDD 1033
17/04/17 14:44:20 INFO MapPartitionsRDD: Removing RDD 1032 from persistence list
17/04/17 14:44:20 INFO BlockManager: Removing RDD 1032
17/04/17 14:44:20 INFO BlockRDD: Removing RDD 1031 from persistence list
17/04/17 14:44:20 INFO BlockManager: Removing RDD 1031
17/04/17 14:44:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1031] at createStream at KafkaConsumer.java:64 of time 1492420460000 ms
17/04/17 14:44:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420440000 ms)
17/04/17 14:44:20 INFO InputInfoTracker: remove old batch metadata: 1492420440000 ms
17/04/17 14:44:30 INFO JobScheduler: Added jobs for time 1492420470000 ms
17/04/17 14:44:30 INFO JobScheduler: Starting job streaming job 1492420470000 ms.0 from job set of time 1492420470000 ms
-------------------------------------------
Time: 1492420470000 ms
-------------------------------------------

17/04/17 14:44:30 INFO JobScheduler: Finished job streaming job 1492420470000 ms.0 from job set of time 1492420470000 ms
17/04/17 14:44:30 INFO MapPartitionsRDD: Removing RDD 1036 from persistence list
17/04/17 14:44:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420470000 ms (execution: 0.000 s)
17/04/17 14:44:30 INFO BlockManager: Removing RDD 1036
17/04/17 14:44:30 INFO MapPartitionsRDD: Removing RDD 1035 from persistence list
17/04/17 14:44:30 INFO BlockManager: Removing RDD 1035
17/04/17 14:44:30 INFO BlockRDD: Removing RDD 1034 from persistence list
17/04/17 14:44:30 INFO BlockManager: Removing RDD 1034
17/04/17 14:44:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1034] at createStream at KafkaConsumer.java:64 of time 1492420470000 ms
17/04/17 14:44:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420450000 ms)
17/04/17 14:44:30 INFO InputInfoTracker: remove old batch metadata: 1492420450000 ms
17/04/17 14:44:40 INFO JobScheduler: Added jobs for time 1492420480000 ms
17/04/17 14:44:40 INFO JobScheduler: Starting job streaming job 1492420480000 ms.0 from job set of time 1492420480000 ms
-------------------------------------------
Time: 1492420480000 ms
-------------------------------------------

17/04/17 14:44:40 INFO JobScheduler: Finished job streaming job 1492420480000 ms.0 from job set of time 1492420480000 ms
17/04/17 14:44:40 INFO MapPartitionsRDD: Removing RDD 1039 from persistence list
17/04/17 14:44:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420480000 ms (execution: 0.000 s)
17/04/17 14:44:40 INFO BlockManager: Removing RDD 1039
17/04/17 14:44:40 INFO MapPartitionsRDD: Removing RDD 1038 from persistence list
17/04/17 14:44:40 INFO BlockManager: Removing RDD 1038
17/04/17 14:44:40 INFO BlockRDD: Removing RDD 1037 from persistence list
17/04/17 14:44:40 INFO BlockManager: Removing RDD 1037
17/04/17 14:44:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1037] at createStream at KafkaConsumer.java:64 of time 1492420480000 ms
17/04/17 14:44:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420460000 ms)
17/04/17 14:44:40 INFO InputInfoTracker: remove old batch metadata: 1492420460000 ms
17/04/17 14:44:50 INFO JobScheduler: Added jobs for time 1492420490000 ms
17/04/17 14:44:50 INFO JobScheduler: Starting job streaming job 1492420490000 ms.0 from job set of time 1492420490000 ms
-------------------------------------------
Time: 1492420490000 ms
-------------------------------------------

17/04/17 14:44:50 INFO JobScheduler: Finished job streaming job 1492420490000 ms.0 from job set of time 1492420490000 ms
17/04/17 14:44:50 INFO MapPartitionsRDD: Removing RDD 1042 from persistence list
17/04/17 14:44:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420490000 ms (execution: 0.000 s)
17/04/17 14:44:50 INFO BlockManager: Removing RDD 1042
17/04/17 14:44:50 INFO MapPartitionsRDD: Removing RDD 1041 from persistence list
17/04/17 14:44:50 INFO BlockManager: Removing RDD 1041
17/04/17 14:44:50 INFO BlockRDD: Removing RDD 1040 from persistence list
17/04/17 14:44:50 INFO BlockManager: Removing RDD 1040
17/04/17 14:44:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1040] at createStream at KafkaConsumer.java:64 of time 1492420490000 ms
17/04/17 14:44:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420470000 ms)
17/04/17 14:44:50 INFO InputInfoTracker: remove old batch metadata: 1492420470000 ms
17/04/17 14:45:00 INFO JobScheduler: Added jobs for time 1492420500000 ms
-------------------------------------------
Time: 1492420500000 ms
-------------------------------------------

17/04/17 14:45:00 INFO JobScheduler: Starting job streaming job 1492420500000 ms.0 from job set of time 1492420500000 ms
17/04/17 14:45:00 INFO JobScheduler: Finished job streaming job 1492420500000 ms.0 from job set of time 1492420500000 ms
17/04/17 14:45:00 INFO MapPartitionsRDD: Removing RDD 1045 from persistence list
17/04/17 14:45:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420500000 ms (execution: 0.000 s)
17/04/17 14:45:00 INFO BlockManager: Removing RDD 1045
17/04/17 14:45:00 INFO MapPartitionsRDD: Removing RDD 1044 from persistence list
17/04/17 14:45:00 INFO BlockManager: Removing RDD 1044
17/04/17 14:45:00 INFO BlockRDD: Removing RDD 1043 from persistence list
17/04/17 14:45:00 INFO BlockManager: Removing RDD 1043
17/04/17 14:45:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1043] at createStream at KafkaConsumer.java:64 of time 1492420500000 ms
17/04/17 14:45:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420480000 ms)
17/04/17 14:45:00 INFO InputInfoTracker: remove old batch metadata: 1492420480000 ms
17/04/17 14:45:10 INFO JobScheduler: Added jobs for time 1492420510000 ms
-------------------------------------------
Time: 1492420510000 ms
-------------------------------------------

17/04/17 14:45:10 INFO JobScheduler: Starting job streaming job 1492420510000 ms.0 from job set of time 1492420510000 ms
17/04/17 14:45:10 INFO JobScheduler: Finished job streaming job 1492420510000 ms.0 from job set of time 1492420510000 ms
17/04/17 14:45:10 INFO MapPartitionsRDD: Removing RDD 1048 from persistence list
17/04/17 14:45:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420510000 ms (execution: 0.000 s)
17/04/17 14:45:10 INFO BlockManager: Removing RDD 1048
17/04/17 14:45:10 INFO MapPartitionsRDD: Removing RDD 1047 from persistence list
17/04/17 14:45:10 INFO BlockManager: Removing RDD 1047
17/04/17 14:45:10 INFO BlockRDD: Removing RDD 1046 from persistence list
17/04/17 14:45:10 INFO BlockManager: Removing RDD 1046
17/04/17 14:45:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1046] at createStream at KafkaConsumer.java:64 of time 1492420510000 ms
17/04/17 14:45:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420490000 ms)
17/04/17 14:45:10 INFO InputInfoTracker: remove old batch metadata: 1492420490000 ms
17/04/17 14:45:20 INFO JobScheduler: Added jobs for time 1492420520000 ms
17/04/17 14:45:20 INFO JobScheduler: Starting job streaming job 1492420520000 ms.0 from job set of time 1492420520000 ms
-------------------------------------------
Time: 1492420520000 ms
-------------------------------------------

17/04/17 14:45:20 INFO JobScheduler: Finished job streaming job 1492420520000 ms.0 from job set of time 1492420520000 ms
17/04/17 14:45:20 INFO MapPartitionsRDD: Removing RDD 1051 from persistence list
17/04/17 14:45:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420520000 ms (execution: 0.001 s)
17/04/17 14:45:20 INFO BlockManager: Removing RDD 1051
17/04/17 14:45:20 INFO MapPartitionsRDD: Removing RDD 1050 from persistence list
17/04/17 14:45:20 INFO BlockManager: Removing RDD 1050
17/04/17 14:45:20 INFO BlockRDD: Removing RDD 1049 from persistence list
17/04/17 14:45:20 INFO BlockManager: Removing RDD 1049
17/04/17 14:45:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1049] at createStream at KafkaConsumer.java:64 of time 1492420520000 ms
17/04/17 14:45:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420500000 ms)
17/04/17 14:45:20 INFO InputInfoTracker: remove old batch metadata: 1492420500000 ms
17/04/17 14:45:30 INFO JobScheduler: Added jobs for time 1492420530000 ms
-------------------------------------------
Time: 1492420530000 ms
-------------------------------------------

17/04/17 14:45:30 INFO JobScheduler: Starting job streaming job 1492420530000 ms.0 from job set of time 1492420530000 ms
17/04/17 14:45:30 INFO JobScheduler: Finished job streaming job 1492420530000 ms.0 from job set of time 1492420530000 ms
17/04/17 14:45:30 INFO MapPartitionsRDD: Removing RDD 1054 from persistence list
17/04/17 14:45:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420530000 ms (execution: 0.000 s)
17/04/17 14:45:30 INFO BlockManager: Removing RDD 1054
17/04/17 14:45:30 INFO MapPartitionsRDD: Removing RDD 1053 from persistence list
17/04/17 14:45:30 INFO BlockManager: Removing RDD 1053
17/04/17 14:45:30 INFO BlockRDD: Removing RDD 1052 from persistence list
17/04/17 14:45:30 INFO BlockManager: Removing RDD 1052
17/04/17 14:45:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1052] at createStream at KafkaConsumer.java:64 of time 1492420530000 ms
17/04/17 14:45:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420510000 ms)
17/04/17 14:45:30 INFO InputInfoTracker: remove old batch metadata: 1492420510000 ms
17/04/17 14:45:40 INFO JobScheduler: Added jobs for time 1492420540000 ms
17/04/17 14:45:40 INFO JobScheduler: Starting job streaming job 1492420540000 ms.0 from job set of time 1492420540000 ms
-------------------------------------------
Time: 1492420540000 ms
-------------------------------------------

17/04/17 14:45:40 INFO JobScheduler: Finished job streaming job 1492420540000 ms.0 from job set of time 1492420540000 ms
17/04/17 14:45:40 INFO MapPartitionsRDD: Removing RDD 1057 from persistence list
17/04/17 14:45:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420540000 ms (execution: 0.001 s)
17/04/17 14:45:40 INFO MapPartitionsRDD: Removing RDD 1056 from persistence list
17/04/17 14:45:40 INFO BlockManager: Removing RDD 1057
17/04/17 14:45:40 INFO BlockManager: Removing RDD 1056
17/04/17 14:45:40 INFO BlockRDD: Removing RDD 1055 from persistence list
17/04/17 14:45:40 INFO BlockManager: Removing RDD 1055
17/04/17 14:45:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1055] at createStream at KafkaConsumer.java:64 of time 1492420540000 ms
17/04/17 14:45:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420520000 ms)
17/04/17 14:45:40 INFO InputInfoTracker: remove old batch metadata: 1492420520000 ms
17/04/17 14:45:50 INFO JobScheduler: Added jobs for time 1492420550000 ms
17/04/17 14:45:50 INFO JobScheduler: Starting job streaming job 1492420550000 ms.0 from job set of time 1492420550000 ms
-------------------------------------------
Time: 1492420550000 ms
-------------------------------------------

17/04/17 14:45:50 INFO JobScheduler: Finished job streaming job 1492420550000 ms.0 from job set of time 1492420550000 ms
17/04/17 14:45:50 INFO MapPartitionsRDD: Removing RDD 1060 from persistence list
17/04/17 14:45:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420550000 ms (execution: 0.000 s)
17/04/17 14:45:50 INFO MapPartitionsRDD: Removing RDD 1059 from persistence list
17/04/17 14:45:50 INFO BlockManager: Removing RDD 1060
17/04/17 14:45:50 INFO BlockManager: Removing RDD 1059
17/04/17 14:45:50 INFO BlockRDD: Removing RDD 1058 from persistence list
17/04/17 14:45:50 INFO BlockManager: Removing RDD 1058
17/04/17 14:45:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1058] at createStream at KafkaConsumer.java:64 of time 1492420550000 ms
17/04/17 14:45:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420530000 ms)
17/04/17 14:45:50 INFO InputInfoTracker: remove old batch metadata: 1492420530000 ms
17/04/17 14:46:00 INFO JobScheduler: Added jobs for time 1492420560000 ms
17/04/17 14:46:00 INFO JobScheduler: Starting job streaming job 1492420560000 ms.0 from job set of time 1492420560000 ms
-------------------------------------------
Time: 1492420560000 ms
-------------------------------------------

17/04/17 14:46:00 INFO JobScheduler: Finished job streaming job 1492420560000 ms.0 from job set of time 1492420560000 ms
17/04/17 14:46:00 INFO MapPartitionsRDD: Removing RDD 1063 from persistence list
17/04/17 14:46:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420560000 ms (execution: 0.001 s)
17/04/17 14:46:00 INFO BlockManager: Removing RDD 1063
17/04/17 14:46:00 INFO MapPartitionsRDD: Removing RDD 1062 from persistence list
17/04/17 14:46:00 INFO BlockManager: Removing RDD 1062
17/04/17 14:46:00 INFO BlockRDD: Removing RDD 1061 from persistence list
17/04/17 14:46:00 INFO BlockManager: Removing RDD 1061
17/04/17 14:46:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1061] at createStream at KafkaConsumer.java:64 of time 1492420560000 ms
17/04/17 14:46:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420540000 ms)
17/04/17 14:46:00 INFO InputInfoTracker: remove old batch metadata: 1492420540000 ms
17/04/17 14:46:10 INFO JobScheduler: Added jobs for time 1492420570000 ms
17/04/17 14:46:10 INFO JobScheduler: Starting job streaming job 1492420570000 ms.0 from job set of time 1492420570000 ms
-------------------------------------------
Time: 1492420570000 ms
-------------------------------------------

17/04/17 14:46:10 INFO JobScheduler: Finished job streaming job 1492420570000 ms.0 from job set of time 1492420570000 ms
17/04/17 14:46:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420570000 ms (execution: 0.000 s)
17/04/17 14:46:10 INFO MapPartitionsRDD: Removing RDD 1066 from persistence list
17/04/17 14:46:10 INFO BlockManager: Removing RDD 1066
17/04/17 14:46:10 INFO MapPartitionsRDD: Removing RDD 1065 from persistence list
17/04/17 14:46:10 INFO BlockManager: Removing RDD 1065
17/04/17 14:46:10 INFO BlockRDD: Removing RDD 1064 from persistence list
17/04/17 14:46:10 INFO BlockManager: Removing RDD 1064
17/04/17 14:46:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1064] at createStream at KafkaConsumer.java:64 of time 1492420570000 ms
17/04/17 14:46:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420550000 ms)
17/04/17 14:46:10 INFO InputInfoTracker: remove old batch metadata: 1492420550000 ms
17/04/17 14:46:20 INFO JobScheduler: Added jobs for time 1492420580000 ms
17/04/17 14:46:20 INFO JobScheduler: Starting job streaming job 1492420580000 ms.0 from job set of time 1492420580000 ms
-------------------------------------------
Time: 1492420580000 ms
-------------------------------------------

17/04/17 14:46:20 INFO JobScheduler: Finished job streaming job 1492420580000 ms.0 from job set of time 1492420580000 ms
17/04/17 14:46:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420580000 ms (execution: 0.000 s)
17/04/17 14:46:20 INFO MapPartitionsRDD: Removing RDD 1069 from persistence list
17/04/17 14:46:20 INFO BlockManager: Removing RDD 1069
17/04/17 14:46:20 INFO MapPartitionsRDD: Removing RDD 1068 from persistence list
17/04/17 14:46:20 INFO BlockManager: Removing RDD 1068
17/04/17 14:46:20 INFO BlockRDD: Removing RDD 1067 from persistence list
17/04/17 14:46:20 INFO BlockManager: Removing RDD 1067
17/04/17 14:46:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1067] at createStream at KafkaConsumer.java:64 of time 1492420580000 ms
17/04/17 14:46:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420560000 ms)
17/04/17 14:46:20 INFO InputInfoTracker: remove old batch metadata: 1492420560000 ms
17/04/17 14:46:30 INFO JobScheduler: Added jobs for time 1492420590000 ms
17/04/17 14:46:30 INFO JobScheduler: Starting job streaming job 1492420590000 ms.0 from job set of time 1492420590000 ms
-------------------------------------------
Time: 1492420590000 ms
-------------------------------------------

17/04/17 14:46:30 INFO JobScheduler: Finished job streaming job 1492420590000 ms.0 from job set of time 1492420590000 ms
17/04/17 14:46:30 INFO MapPartitionsRDD: Removing RDD 1072 from persistence list
17/04/17 14:46:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420590000 ms (execution: 0.001 s)
17/04/17 14:46:30 INFO BlockManager: Removing RDD 1072
17/04/17 14:46:30 INFO MapPartitionsRDD: Removing RDD 1071 from persistence list
17/04/17 14:46:30 INFO BlockManager: Removing RDD 1071
17/04/17 14:46:30 INFO BlockRDD: Removing RDD 1070 from persistence list
17/04/17 14:46:30 INFO BlockManager: Removing RDD 1070
17/04/17 14:46:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1070] at createStream at KafkaConsumer.java:64 of time 1492420590000 ms
17/04/17 14:46:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420570000 ms)
17/04/17 14:46:30 INFO InputInfoTracker: remove old batch metadata: 1492420570000 ms
17/04/17 14:46:40 INFO JobScheduler: Added jobs for time 1492420600000 ms
17/04/17 14:46:40 INFO JobScheduler: Starting job streaming job 1492420600000 ms.0 from job set of time 1492420600000 ms
-------------------------------------------
Time: 1492420600000 ms
-------------------------------------------

17/04/17 14:46:40 INFO JobScheduler: Finished job streaming job 1492420600000 ms.0 from job set of time 1492420600000 ms
17/04/17 14:46:40 INFO MapPartitionsRDD: Removing RDD 1075 from persistence list
17/04/17 14:46:40 INFO JobScheduler: Total delay: 0.012 s for time 1492420600000 ms (execution: 0.001 s)
17/04/17 14:46:40 INFO BlockManager: Removing RDD 1075
17/04/17 14:46:40 INFO MapPartitionsRDD: Removing RDD 1074 from persistence list
17/04/17 14:46:40 INFO BlockManager: Removing RDD 1074
17/04/17 14:46:40 INFO BlockRDD: Removing RDD 1073 from persistence list
17/04/17 14:46:40 INFO BlockManager: Removing RDD 1073
17/04/17 14:46:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1073] at createStream at KafkaConsumer.java:64 of time 1492420600000 ms
17/04/17 14:46:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420580000 ms)
17/04/17 14:46:40 INFO InputInfoTracker: remove old batch metadata: 1492420580000 ms
17/04/17 14:46:50 INFO JobScheduler: Added jobs for time 1492420610000 ms
17/04/17 14:46:50 INFO JobScheduler: Starting job streaming job 1492420610000 ms.0 from job set of time 1492420610000 ms
-------------------------------------------
Time: 1492420610000 ms
-------------------------------------------

17/04/17 14:46:50 INFO JobScheduler: Finished job streaming job 1492420610000 ms.0 from job set of time 1492420610000 ms
17/04/17 14:46:50 INFO MapPartitionsRDD: Removing RDD 1078 from persistence list
17/04/17 14:46:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420610000 ms (execution: 0.000 s)
17/04/17 14:46:50 INFO BlockManager: Removing RDD 1078
17/04/17 14:46:50 INFO MapPartitionsRDD: Removing RDD 1077 from persistence list
17/04/17 14:46:50 INFO BlockManager: Removing RDD 1077
17/04/17 14:46:50 INFO BlockRDD: Removing RDD 1076 from persistence list
17/04/17 14:46:50 INFO BlockManager: Removing RDD 1076
17/04/17 14:46:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1076] at createStream at KafkaConsumer.java:64 of time 1492420610000 ms
17/04/17 14:46:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420590000 ms)
17/04/17 14:46:50 INFO InputInfoTracker: remove old batch metadata: 1492420590000 ms
17/04/17 14:47:00 INFO JobScheduler: Added jobs for time 1492420620000 ms
17/04/17 14:47:00 INFO JobScheduler: Starting job streaming job 1492420620000 ms.0 from job set of time 1492420620000 ms
-------------------------------------------
Time: 1492420620000 ms
-------------------------------------------

17/04/17 14:47:00 INFO JobScheduler: Finished job streaming job 1492420620000 ms.0 from job set of time 1492420620000 ms
17/04/17 14:47:00 INFO MapPartitionsRDD: Removing RDD 1081 from persistence list
17/04/17 14:47:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420620000 ms (execution: 0.000 s)
17/04/17 14:47:00 INFO BlockManager: Removing RDD 1081
17/04/17 14:47:00 INFO MapPartitionsRDD: Removing RDD 1080 from persistence list
17/04/17 14:47:00 INFO BlockManager: Removing RDD 1080
17/04/17 14:47:00 INFO BlockRDD: Removing RDD 1079 from persistence list
17/04/17 14:47:00 INFO BlockManager: Removing RDD 1079
17/04/17 14:47:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1079] at createStream at KafkaConsumer.java:64 of time 1492420620000 ms
17/04/17 14:47:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420600000 ms)
17/04/17 14:47:00 INFO InputInfoTracker: remove old batch metadata: 1492420600000 ms
17/04/17 14:47:10 INFO JobScheduler: Added jobs for time 1492420630000 ms
17/04/17 14:47:10 INFO JobScheduler: Starting job streaming job 1492420630000 ms.0 from job set of time 1492420630000 ms
-------------------------------------------
Time: 1492420630000 ms
-------------------------------------------

17/04/17 14:47:10 INFO JobScheduler: Finished job streaming job 1492420630000 ms.0 from job set of time 1492420630000 ms
17/04/17 14:47:10 INFO MapPartitionsRDD: Removing RDD 1084 from persistence list
17/04/17 14:47:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420630000 ms (execution: 0.000 s)
17/04/17 14:47:10 INFO BlockManager: Removing RDD 1084
17/04/17 14:47:10 INFO MapPartitionsRDD: Removing RDD 1083 from persistence list
17/04/17 14:47:10 INFO BlockManager: Removing RDD 1083
17/04/17 14:47:10 INFO BlockRDD: Removing RDD 1082 from persistence list
17/04/17 14:47:10 INFO BlockManager: Removing RDD 1082
17/04/17 14:47:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1082] at createStream at KafkaConsumer.java:64 of time 1492420630000 ms
17/04/17 14:47:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420610000 ms)
17/04/17 14:47:10 INFO InputInfoTracker: remove old batch metadata: 1492420610000 ms
17/04/17 14:47:20 INFO JobScheduler: Added jobs for time 1492420640000 ms
17/04/17 14:47:20 INFO JobScheduler: Starting job streaming job 1492420640000 ms.0 from job set of time 1492420640000 ms
-------------------------------------------
Time: 1492420640000 ms
-------------------------------------------

17/04/17 14:47:20 INFO JobScheduler: Finished job streaming job 1492420640000 ms.0 from job set of time 1492420640000 ms
17/04/17 14:47:20 INFO MapPartitionsRDD: Removing RDD 1087 from persistence list
17/04/17 14:47:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420640000 ms (execution: 0.000 s)
17/04/17 14:47:20 INFO MapPartitionsRDD: Removing RDD 1086 from persistence list
17/04/17 14:47:20 INFO BlockManager: Removing RDD 1087
17/04/17 14:47:20 INFO BlockManager: Removing RDD 1086
17/04/17 14:47:20 INFO BlockRDD: Removing RDD 1085 from persistence list
17/04/17 14:47:20 INFO BlockManager: Removing RDD 1085
17/04/17 14:47:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1085] at createStream at KafkaConsumer.java:64 of time 1492420640000 ms
17/04/17 14:47:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420620000 ms)
17/04/17 14:47:20 INFO InputInfoTracker: remove old batch metadata: 1492420620000 ms
17/04/17 14:47:30 INFO JobScheduler: Added jobs for time 1492420650000 ms
17/04/17 14:47:30 INFO JobScheduler: Starting job streaming job 1492420650000 ms.0 from job set of time 1492420650000 ms
-------------------------------------------
Time: 1492420650000 ms
-------------------------------------------

17/04/17 14:47:30 INFO JobScheduler: Finished job streaming job 1492420650000 ms.0 from job set of time 1492420650000 ms
17/04/17 14:47:30 INFO MapPartitionsRDD: Removing RDD 1090 from persistence list
17/04/17 14:47:30 INFO JobScheduler: Total delay: 0.003 s for time 1492420650000 ms (execution: 0.000 s)
17/04/17 14:47:30 INFO BlockManager: Removing RDD 1090
17/04/17 14:47:30 INFO MapPartitionsRDD: Removing RDD 1089 from persistence list
17/04/17 14:47:30 INFO BlockManager: Removing RDD 1089
17/04/17 14:47:30 INFO BlockRDD: Removing RDD 1088 from persistence list
17/04/17 14:47:30 INFO BlockManager: Removing RDD 1088
17/04/17 14:47:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1088] at createStream at KafkaConsumer.java:64 of time 1492420650000 ms
17/04/17 14:47:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420630000 ms)
17/04/17 14:47:30 INFO InputInfoTracker: remove old batch metadata: 1492420630000 ms
17/04/17 14:47:40 INFO JobScheduler: Added jobs for time 1492420660000 ms
17/04/17 14:47:40 INFO JobScheduler: Starting job streaming job 1492420660000 ms.0 from job set of time 1492420660000 ms
-------------------------------------------
Time: 1492420660000 ms
-------------------------------------------

17/04/17 14:47:40 INFO JobScheduler: Finished job streaming job 1492420660000 ms.0 from job set of time 1492420660000 ms
17/04/17 14:47:40 INFO MapPartitionsRDD: Removing RDD 1093 from persistence list
17/04/17 14:47:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420660000 ms (execution: 0.000 s)
17/04/17 14:47:40 INFO BlockManager: Removing RDD 1093
17/04/17 14:47:40 INFO MapPartitionsRDD: Removing RDD 1092 from persistence list
17/04/17 14:47:40 INFO BlockManager: Removing RDD 1092
17/04/17 14:47:40 INFO BlockRDD: Removing RDD 1091 from persistence list
17/04/17 14:47:40 INFO BlockManager: Removing RDD 1091
17/04/17 14:47:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1091] at createStream at KafkaConsumer.java:64 of time 1492420660000 ms
17/04/17 14:47:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420640000 ms)
17/04/17 14:47:40 INFO InputInfoTracker: remove old batch metadata: 1492420640000 ms
17/04/17 14:47:50 INFO JobScheduler: Added jobs for time 1492420670000 ms
17/04/17 14:47:50 INFO JobScheduler: Starting job streaming job 1492420670000 ms.0 from job set of time 1492420670000 ms
-------------------------------------------
Time: 1492420670000 ms
-------------------------------------------

17/04/17 14:47:50 INFO JobScheduler: Finished job streaming job 1492420670000 ms.0 from job set of time 1492420670000 ms
17/04/17 14:47:50 INFO MapPartitionsRDD: Removing RDD 1096 from persistence list
17/04/17 14:47:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420670000 ms (execution: 0.000 s)
17/04/17 14:47:50 INFO BlockManager: Removing RDD 1096
17/04/17 14:47:50 INFO MapPartitionsRDD: Removing RDD 1095 from persistence list
17/04/17 14:47:50 INFO BlockManager: Removing RDD 1095
17/04/17 14:47:50 INFO BlockRDD: Removing RDD 1094 from persistence list
17/04/17 14:47:50 INFO BlockManager: Removing RDD 1094
17/04/17 14:47:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1094] at createStream at KafkaConsumer.java:64 of time 1492420670000 ms
17/04/17 14:47:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420650000 ms)
17/04/17 14:47:50 INFO InputInfoTracker: remove old batch metadata: 1492420650000 ms
17/04/17 14:48:00 INFO JobScheduler: Added jobs for time 1492420680000 ms
17/04/17 14:48:00 INFO JobScheduler: Starting job streaming job 1492420680000 ms.0 from job set of time 1492420680000 ms
-------------------------------------------
Time: 1492420680000 ms
-------------------------------------------

17/04/17 14:48:00 INFO JobScheduler: Finished job streaming job 1492420680000 ms.0 from job set of time 1492420680000 ms
17/04/17 14:48:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420680000 ms (execution: 0.000 s)
17/04/17 14:48:00 INFO MapPartitionsRDD: Removing RDD 1099 from persistence list
17/04/17 14:48:00 INFO BlockManager: Removing RDD 1099
17/04/17 14:48:00 INFO MapPartitionsRDD: Removing RDD 1098 from persistence list
17/04/17 14:48:00 INFO BlockManager: Removing RDD 1098
17/04/17 14:48:00 INFO BlockRDD: Removing RDD 1097 from persistence list
17/04/17 14:48:00 INFO BlockManager: Removing RDD 1097
17/04/17 14:48:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1097] at createStream at KafkaConsumer.java:64 of time 1492420680000 ms
17/04/17 14:48:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420660000 ms)
17/04/17 14:48:00 INFO InputInfoTracker: remove old batch metadata: 1492420660000 ms
17/04/17 14:48:10 INFO JobScheduler: Added jobs for time 1492420690000 ms
17/04/17 14:48:10 INFO JobScheduler: Starting job streaming job 1492420690000 ms.0 from job set of time 1492420690000 ms
-------------------------------------------
Time: 1492420690000 ms
-------------------------------------------

17/04/17 14:48:10 INFO JobScheduler: Finished job streaming job 1492420690000 ms.0 from job set of time 1492420690000 ms
17/04/17 14:48:10 INFO MapPartitionsRDD: Removing RDD 1102 from persistence list
17/04/17 14:48:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420690000 ms (execution: 0.000 s)
17/04/17 14:48:10 INFO BlockManager: Removing RDD 1102
17/04/17 14:48:10 INFO MapPartitionsRDD: Removing RDD 1101 from persistence list
17/04/17 14:48:10 INFO BlockManager: Removing RDD 1101
17/04/17 14:48:10 INFO BlockRDD: Removing RDD 1100 from persistence list
17/04/17 14:48:10 INFO BlockManager: Removing RDD 1100
17/04/17 14:48:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1100] at createStream at KafkaConsumer.java:64 of time 1492420690000 ms
17/04/17 14:48:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420670000 ms)
17/04/17 14:48:10 INFO InputInfoTracker: remove old batch metadata: 1492420670000 ms
17/04/17 14:48:20 INFO JobScheduler: Added jobs for time 1492420700000 ms
17/04/17 14:48:20 INFO JobScheduler: Starting job streaming job 1492420700000 ms.0 from job set of time 1492420700000 ms
-------------------------------------------
Time: 1492420700000 ms
-------------------------------------------

17/04/17 14:48:20 INFO JobScheduler: Finished job streaming job 1492420700000 ms.0 from job set of time 1492420700000 ms
17/04/17 14:48:20 INFO MapPartitionsRDD: Removing RDD 1105 from persistence list
17/04/17 14:48:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420700000 ms (execution: 0.000 s)
17/04/17 14:48:20 INFO BlockManager: Removing RDD 1105
17/04/17 14:48:20 INFO MapPartitionsRDD: Removing RDD 1104 from persistence list
17/04/17 14:48:20 INFO BlockRDD: Removing RDD 1103 from persistence list
17/04/17 14:48:20 INFO BlockManager: Removing RDD 1104
17/04/17 14:48:20 INFO BlockManager: Removing RDD 1103
17/04/17 14:48:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1103] at createStream at KafkaConsumer.java:64 of time 1492420700000 ms
17/04/17 14:48:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420680000 ms)
17/04/17 14:48:20 INFO InputInfoTracker: remove old batch metadata: 1492420680000 ms
17/04/17 14:48:30 INFO JobScheduler: Added jobs for time 1492420710000 ms
17/04/17 14:48:30 INFO JobScheduler: Starting job streaming job 1492420710000 ms.0 from job set of time 1492420710000 ms
-------------------------------------------
Time: 1492420710000 ms
-------------------------------------------

17/04/17 14:48:30 INFO JobScheduler: Finished job streaming job 1492420710000 ms.0 from job set of time 1492420710000 ms
17/04/17 14:48:30 INFO MapPartitionsRDD: Removing RDD 1108 from persistence list
17/04/17 14:48:30 INFO JobScheduler: Total delay: 0.005 s for time 1492420710000 ms (execution: 0.001 s)
17/04/17 14:48:30 INFO MapPartitionsRDD: Removing RDD 1107 from persistence list
17/04/17 14:48:30 INFO BlockManager: Removing RDD 1108
17/04/17 14:48:30 INFO BlockRDD: Removing RDD 1106 from persistence list
17/04/17 14:48:30 INFO BlockManager: Removing RDD 1107
17/04/17 14:48:30 INFO BlockManager: Removing RDD 1106
17/04/17 14:48:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1106] at createStream at KafkaConsumer.java:64 of time 1492420710000 ms
17/04/17 14:48:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420690000 ms)
17/04/17 14:48:30 INFO InputInfoTracker: remove old batch metadata: 1492420690000 ms
17/04/17 14:48:40 INFO JobScheduler: Added jobs for time 1492420720000 ms
17/04/17 14:48:40 INFO JobScheduler: Starting job streaming job 1492420720000 ms.0 from job set of time 1492420720000 ms
-------------------------------------------
Time: 1492420720000 ms
-------------------------------------------

17/04/17 14:48:40 INFO JobScheduler: Finished job streaming job 1492420720000 ms.0 from job set of time 1492420720000 ms
17/04/17 14:48:40 INFO MapPartitionsRDD: Removing RDD 1111 from persistence list
17/04/17 14:48:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420720000 ms (execution: 0.000 s)
17/04/17 14:48:40 INFO BlockManager: Removing RDD 1111
17/04/17 14:48:40 INFO MapPartitionsRDD: Removing RDD 1110 from persistence list
17/04/17 14:48:40 INFO BlockManager: Removing RDD 1110
17/04/17 14:48:40 INFO BlockRDD: Removing RDD 1109 from persistence list
17/04/17 14:48:40 INFO BlockManager: Removing RDD 1109
17/04/17 14:48:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1109] at createStream at KafkaConsumer.java:64 of time 1492420720000 ms
17/04/17 14:48:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420700000 ms)
17/04/17 14:48:40 INFO InputInfoTracker: remove old batch metadata: 1492420700000 ms
17/04/17 14:48:50 INFO JobScheduler: Added jobs for time 1492420730000 ms
17/04/17 14:48:50 INFO JobScheduler: Starting job streaming job 1492420730000 ms.0 from job set of time 1492420730000 ms
-------------------------------------------
Time: 1492420730000 ms
-------------------------------------------

17/04/17 14:48:50 INFO JobScheduler: Finished job streaming job 1492420730000 ms.0 from job set of time 1492420730000 ms
17/04/17 14:48:50 INFO MapPartitionsRDD: Removing RDD 1114 from persistence list
17/04/17 14:48:50 INFO JobScheduler: Total delay: 0.005 s for time 1492420730000 ms (execution: 0.001 s)
17/04/17 14:48:50 INFO BlockManager: Removing RDD 1114
17/04/17 14:48:50 INFO MapPartitionsRDD: Removing RDD 1113 from persistence list
17/04/17 14:48:50 INFO BlockManager: Removing RDD 1113
17/04/17 14:48:50 INFO BlockRDD: Removing RDD 1112 from persistence list
17/04/17 14:48:50 INFO BlockManager: Removing RDD 1112
17/04/17 14:48:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1112] at createStream at KafkaConsumer.java:64 of time 1492420730000 ms
17/04/17 14:48:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420710000 ms)
17/04/17 14:48:50 INFO InputInfoTracker: remove old batch metadata: 1492420710000 ms
17/04/17 14:49:00 INFO JobScheduler: Added jobs for time 1492420740000 ms
17/04/17 14:49:00 INFO JobScheduler: Starting job streaming job 1492420740000 ms.0 from job set of time 1492420740000 ms
-------------------------------------------
Time: 1492420740000 ms
-------------------------------------------

17/04/17 14:49:00 INFO JobScheduler: Finished job streaming job 1492420740000 ms.0 from job set of time 1492420740000 ms
17/04/17 14:49:00 INFO MapPartitionsRDD: Removing RDD 1117 from persistence list
17/04/17 14:49:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420740000 ms (execution: 0.001 s)
17/04/17 14:49:00 INFO BlockManager: Removing RDD 1117
17/04/17 14:49:00 INFO MapPartitionsRDD: Removing RDD 1116 from persistence list
17/04/17 14:49:00 INFO BlockManager: Removing RDD 1116
17/04/17 14:49:00 INFO BlockRDD: Removing RDD 1115 from persistence list
17/04/17 14:49:00 INFO BlockManager: Removing RDD 1115
17/04/17 14:49:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1115] at createStream at KafkaConsumer.java:64 of time 1492420740000 ms
17/04/17 14:49:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420720000 ms)
17/04/17 14:49:00 INFO InputInfoTracker: remove old batch metadata: 1492420720000 ms
17/04/17 14:49:10 INFO JobScheduler: Added jobs for time 1492420750000 ms
17/04/17 14:49:10 INFO JobScheduler: Starting job streaming job 1492420750000 ms.0 from job set of time 1492420750000 ms
-------------------------------------------
Time: 1492420750000 ms
-------------------------------------------

17/04/17 14:49:10 INFO JobScheduler: Finished job streaming job 1492420750000 ms.0 from job set of time 1492420750000 ms
17/04/17 14:49:10 INFO MapPartitionsRDD: Removing RDD 1120 from persistence list
17/04/17 14:49:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420750000 ms (execution: 0.000 s)
17/04/17 14:49:10 INFO BlockManager: Removing RDD 1120
17/04/17 14:49:10 INFO MapPartitionsRDD: Removing RDD 1119 from persistence list
17/04/17 14:49:10 INFO BlockManager: Removing RDD 1119
17/04/17 14:49:10 INFO BlockRDD: Removing RDD 1118 from persistence list
17/04/17 14:49:10 INFO BlockManager: Removing RDD 1118
17/04/17 14:49:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1118] at createStream at KafkaConsumer.java:64 of time 1492420750000 ms
17/04/17 14:49:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420730000 ms)
17/04/17 14:49:10 INFO InputInfoTracker: remove old batch metadata: 1492420730000 ms
17/04/17 14:49:20 INFO JobScheduler: Added jobs for time 1492420760000 ms
17/04/17 14:49:20 INFO JobScheduler: Starting job streaming job 1492420760000 ms.0 from job set of time 1492420760000 ms
-------------------------------------------
Time: 1492420760000 ms
-------------------------------------------

17/04/17 14:49:20 INFO JobScheduler: Finished job streaming job 1492420760000 ms.0 from job set of time 1492420760000 ms
17/04/17 14:49:20 INFO MapPartitionsRDD: Removing RDD 1123 from persistence list
17/04/17 14:49:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420760000 ms (execution: 0.000 s)
17/04/17 14:49:20 INFO BlockManager: Removing RDD 1123
17/04/17 14:49:20 INFO MapPartitionsRDD: Removing RDD 1122 from persistence list
17/04/17 14:49:20 INFO BlockManager: Removing RDD 1122
17/04/17 14:49:20 INFO BlockRDD: Removing RDD 1121 from persistence list
17/04/17 14:49:20 INFO BlockManager: Removing RDD 1121
17/04/17 14:49:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1121] at createStream at KafkaConsumer.java:64 of time 1492420760000 ms
17/04/17 14:49:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420740000 ms)
17/04/17 14:49:20 INFO InputInfoTracker: remove old batch metadata: 1492420740000 ms
17/04/17 14:49:30 INFO JobScheduler: Added jobs for time 1492420770000 ms
17/04/17 14:49:30 INFO JobScheduler: Starting job streaming job 1492420770000 ms.0 from job set of time 1492420770000 ms
-------------------------------------------
Time: 1492420770000 ms
-------------------------------------------

17/04/17 14:49:30 INFO JobScheduler: Finished job streaming job 1492420770000 ms.0 from job set of time 1492420770000 ms
17/04/17 14:49:30 INFO MapPartitionsRDD: Removing RDD 1126 from persistence list
17/04/17 14:49:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420770000 ms (execution: 0.000 s)
17/04/17 14:49:30 INFO BlockManager: Removing RDD 1126
17/04/17 14:49:30 INFO MapPartitionsRDD: Removing RDD 1125 from persistence list
17/04/17 14:49:30 INFO BlockManager: Removing RDD 1125
17/04/17 14:49:30 INFO BlockRDD: Removing RDD 1124 from persistence list
17/04/17 14:49:30 INFO BlockManager: Removing RDD 1124
17/04/17 14:49:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1124] at createStream at KafkaConsumer.java:64 of time 1492420770000 ms
17/04/17 14:49:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420750000 ms)
17/04/17 14:49:30 INFO InputInfoTracker: remove old batch metadata: 1492420750000 ms
17/04/17 14:49:40 INFO JobScheduler: Added jobs for time 1492420780000 ms
17/04/17 14:49:40 INFO JobScheduler: Starting job streaming job 1492420780000 ms.0 from job set of time 1492420780000 ms
-------------------------------------------
Time: 1492420780000 ms
-------------------------------------------

17/04/17 14:49:40 INFO JobScheduler: Finished job streaming job 1492420780000 ms.0 from job set of time 1492420780000 ms
17/04/17 14:49:40 INFO MapPartitionsRDD: Removing RDD 1129 from persistence list
17/04/17 14:49:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420780000 ms (execution: 0.001 s)
17/04/17 14:49:40 INFO BlockManager: Removing RDD 1129
17/04/17 14:49:40 INFO MapPartitionsRDD: Removing RDD 1128 from persistence list
17/04/17 14:49:40 INFO BlockManager: Removing RDD 1128
17/04/17 14:49:40 INFO BlockRDD: Removing RDD 1127 from persistence list
17/04/17 14:49:40 INFO BlockManager: Removing RDD 1127
17/04/17 14:49:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1127] at createStream at KafkaConsumer.java:64 of time 1492420780000 ms
17/04/17 14:49:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420760000 ms)
17/04/17 14:49:40 INFO InputInfoTracker: remove old batch metadata: 1492420760000 ms
17/04/17 14:49:50 INFO JobScheduler: Added jobs for time 1492420790000 ms
-------------------------------------------
Time: 1492420790000 ms
-------------------------------------------

17/04/17 14:49:50 INFO JobScheduler: Starting job streaming job 1492420790000 ms.0 from job set of time 1492420790000 ms
17/04/17 14:49:50 INFO JobScheduler: Finished job streaming job 1492420790000 ms.0 from job set of time 1492420790000 ms
17/04/17 14:49:50 INFO MapPartitionsRDD: Removing RDD 1132 from persistence list
17/04/17 14:49:50 INFO JobScheduler: Total delay: 0.003 s for time 1492420790000 ms (execution: 0.000 s)
17/04/17 14:49:50 INFO BlockManager: Removing RDD 1132
17/04/17 14:49:50 INFO MapPartitionsRDD: Removing RDD 1131 from persistence list
17/04/17 14:49:50 INFO BlockManager: Removing RDD 1131
17/04/17 14:49:50 INFO BlockRDD: Removing RDD 1130 from persistence list
17/04/17 14:49:50 INFO BlockManager: Removing RDD 1130
17/04/17 14:49:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1130] at createStream at KafkaConsumer.java:64 of time 1492420790000 ms
17/04/17 14:49:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420770000 ms)
17/04/17 14:49:50 INFO InputInfoTracker: remove old batch metadata: 1492420770000 ms
17/04/17 14:50:00 INFO JobScheduler: Added jobs for time 1492420800000 ms
17/04/17 14:50:00 INFO JobScheduler: Starting job streaming job 1492420800000 ms.0 from job set of time 1492420800000 ms
-------------------------------------------
Time: 1492420800000 ms
-------------------------------------------

17/04/17 14:50:00 INFO JobScheduler: Finished job streaming job 1492420800000 ms.0 from job set of time 1492420800000 ms
17/04/17 14:50:00 INFO MapPartitionsRDD: Removing RDD 1135 from persistence list
17/04/17 14:50:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420800000 ms (execution: 0.000 s)
17/04/17 14:50:00 INFO BlockManager: Removing RDD 1135
17/04/17 14:50:00 INFO MapPartitionsRDD: Removing RDD 1134 from persistence list
17/04/17 14:50:00 INFO BlockManager: Removing RDD 1134
17/04/17 14:50:00 INFO BlockRDD: Removing RDD 1133 from persistence list
17/04/17 14:50:00 INFO BlockManager: Removing RDD 1133
17/04/17 14:50:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1133] at createStream at KafkaConsumer.java:64 of time 1492420800000 ms
17/04/17 14:50:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420780000 ms)
17/04/17 14:50:00 INFO InputInfoTracker: remove old batch metadata: 1492420780000 ms
17/04/17 14:50:10 INFO JobScheduler: Added jobs for time 1492420810000 ms
17/04/17 14:50:10 INFO JobScheduler: Starting job streaming job 1492420810000 ms.0 from job set of time 1492420810000 ms
-------------------------------------------
Time: 1492420810000 ms
-------------------------------------------

17/04/17 14:50:10 INFO JobScheduler: Finished job streaming job 1492420810000 ms.0 from job set of time 1492420810000 ms
17/04/17 14:50:10 INFO MapPartitionsRDD: Removing RDD 1138 from persistence list
17/04/17 14:50:10 INFO JobScheduler: Total delay: 0.003 s for time 1492420810000 ms (execution: 0.000 s)
17/04/17 14:50:10 INFO BlockManager: Removing RDD 1138
17/04/17 14:50:10 INFO MapPartitionsRDD: Removing RDD 1137 from persistence list
17/04/17 14:50:10 INFO BlockManager: Removing RDD 1137
17/04/17 14:50:10 INFO BlockRDD: Removing RDD 1136 from persistence list
17/04/17 14:50:10 INFO BlockManager: Removing RDD 1136
17/04/17 14:50:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1136] at createStream at KafkaConsumer.java:64 of time 1492420810000 ms
17/04/17 14:50:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420790000 ms)
17/04/17 14:50:10 INFO InputInfoTracker: remove old batch metadata: 1492420790000 ms
17/04/17 14:50:20 INFO JobScheduler: Added jobs for time 1492420820000 ms
17/04/17 14:50:20 INFO JobScheduler: Starting job streaming job 1492420820000 ms.0 from job set of time 1492420820000 ms
-------------------------------------------
Time: 1492420820000 ms
-------------------------------------------

17/04/17 14:50:20 INFO JobScheduler: Finished job streaming job 1492420820000 ms.0 from job set of time 1492420820000 ms
17/04/17 14:50:20 INFO MapPartitionsRDD: Removing RDD 1141 from persistence list
17/04/17 14:50:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420820000 ms (execution: 0.000 s)
17/04/17 14:50:20 INFO BlockManager: Removing RDD 1141
17/04/17 14:50:20 INFO MapPartitionsRDD: Removing RDD 1140 from persistence list
17/04/17 14:50:20 INFO BlockManager: Removing RDD 1140
17/04/17 14:50:20 INFO BlockRDD: Removing RDD 1139 from persistence list
17/04/17 14:50:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1139] at createStream at KafkaConsumer.java:64 of time 1492420820000 ms
17/04/17 14:50:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420800000 ms)
17/04/17 14:50:20 INFO InputInfoTracker: remove old batch metadata: 1492420800000 ms
17/04/17 14:50:20 INFO BlockManager: Removing RDD 1139
17/04/17 14:50:30 INFO JobScheduler: Added jobs for time 1492420830000 ms
17/04/17 14:50:30 INFO JobScheduler: Starting job streaming job 1492420830000 ms.0 from job set of time 1492420830000 ms
-------------------------------------------
Time: 1492420830000 ms
-------------------------------------------

17/04/17 14:50:30 INFO JobScheduler: Finished job streaming job 1492420830000 ms.0 from job set of time 1492420830000 ms
17/04/17 14:50:30 INFO MapPartitionsRDD: Removing RDD 1144 from persistence list
17/04/17 14:50:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420830000 ms (execution: 0.001 s)
17/04/17 14:50:30 INFO BlockManager: Removing RDD 1144
17/04/17 14:50:30 INFO MapPartitionsRDD: Removing RDD 1143 from persistence list
17/04/17 14:50:30 INFO BlockManager: Removing RDD 1143
17/04/17 14:50:30 INFO BlockRDD: Removing RDD 1142 from persistence list
17/04/17 14:50:30 INFO BlockManager: Removing RDD 1142
17/04/17 14:50:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1142] at createStream at KafkaConsumer.java:64 of time 1492420830000 ms
17/04/17 14:50:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420810000 ms)
17/04/17 14:50:30 INFO InputInfoTracker: remove old batch metadata: 1492420810000 ms
17/04/17 14:50:40 INFO JobScheduler: Added jobs for time 1492420840000 ms
17/04/17 14:50:40 INFO JobScheduler: Starting job streaming job 1492420840000 ms.0 from job set of time 1492420840000 ms
-------------------------------------------
Time: 1492420840000 ms
-------------------------------------------

17/04/17 14:50:40 INFO JobScheduler: Finished job streaming job 1492420840000 ms.0 from job set of time 1492420840000 ms
17/04/17 14:50:40 INFO MapPartitionsRDD: Removing RDD 1147 from persistence list
17/04/17 14:50:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420840000 ms (execution: 0.001 s)
17/04/17 14:50:40 INFO BlockManager: Removing RDD 1147
17/04/17 14:50:40 INFO MapPartitionsRDD: Removing RDD 1146 from persistence list
17/04/17 14:50:40 INFO BlockRDD: Removing RDD 1145 from persistence list
17/04/17 14:50:40 INFO BlockManager: Removing RDD 1146
17/04/17 14:50:40 INFO BlockManager: Removing RDD 1145
17/04/17 14:50:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1145] at createStream at KafkaConsumer.java:64 of time 1492420840000 ms
17/04/17 14:50:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420820000 ms)
17/04/17 14:50:40 INFO InputInfoTracker: remove old batch metadata: 1492420820000 ms
17/04/17 14:50:50 INFO JobScheduler: Added jobs for time 1492420850000 ms
17/04/17 14:50:50 INFO JobScheduler: Starting job streaming job 1492420850000 ms.0 from job set of time 1492420850000 ms
-------------------------------------------
Time: 1492420850000 ms
-------------------------------------------

17/04/17 14:50:50 INFO JobScheduler: Finished job streaming job 1492420850000 ms.0 from job set of time 1492420850000 ms
17/04/17 14:50:50 INFO MapPartitionsRDD: Removing RDD 1150 from persistence list
17/04/17 14:50:50 INFO JobScheduler: Total delay: 0.006 s for time 1492420850000 ms (execution: 0.001 s)
17/04/17 14:50:50 INFO BlockManager: Removing RDD 1150
17/04/17 14:50:50 INFO MapPartitionsRDD: Removing RDD 1149 from persistence list
17/04/17 14:50:50 INFO BlockManager: Removing RDD 1149
17/04/17 14:50:50 INFO BlockRDD: Removing RDD 1148 from persistence list
17/04/17 14:50:50 INFO BlockManager: Removing RDD 1148
17/04/17 14:50:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1148] at createStream at KafkaConsumer.java:64 of time 1492420850000 ms
17/04/17 14:50:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420830000 ms)
17/04/17 14:50:50 INFO InputInfoTracker: remove old batch metadata: 1492420830000 ms
17/04/17 14:51:00 INFO JobScheduler: Added jobs for time 1492420860000 ms
17/04/17 14:51:00 INFO JobScheduler: Starting job streaming job 1492420860000 ms.0 from job set of time 1492420860000 ms
-------------------------------------------
Time: 1492420860000 ms
-------------------------------------------

17/04/17 14:51:00 INFO JobScheduler: Finished job streaming job 1492420860000 ms.0 from job set of time 1492420860000 ms
17/04/17 14:51:00 INFO MapPartitionsRDD: Removing RDD 1153 from persistence list
17/04/17 14:51:00 INFO JobScheduler: Total delay: 0.006 s for time 1492420860000 ms (execution: 0.001 s)
17/04/17 14:51:00 INFO BlockManager: Removing RDD 1153
17/04/17 14:51:00 INFO MapPartitionsRDD: Removing RDD 1152 from persistence list
17/04/17 14:51:00 INFO BlockManager: Removing RDD 1152
17/04/17 14:51:00 INFO BlockRDD: Removing RDD 1151 from persistence list
17/04/17 14:51:00 INFO BlockManager: Removing RDD 1151
17/04/17 14:51:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1151] at createStream at KafkaConsumer.java:64 of time 1492420860000 ms
17/04/17 14:51:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420840000 ms)
17/04/17 14:51:00 INFO InputInfoTracker: remove old batch metadata: 1492420840000 ms
17/04/17 14:51:10 INFO JobScheduler: Added jobs for time 1492420870000 ms
17/04/17 14:51:10 INFO JobScheduler: Starting job streaming job 1492420870000 ms.0 from job set of time 1492420870000 ms
-------------------------------------------
Time: 1492420870000 ms
-------------------------------------------

17/04/17 14:51:10 INFO JobScheduler: Finished job streaming job 1492420870000 ms.0 from job set of time 1492420870000 ms
17/04/17 14:51:10 INFO MapPartitionsRDD: Removing RDD 1156 from persistence list
17/04/17 14:51:10 INFO JobScheduler: Total delay: 0.005 s for time 1492420870000 ms (execution: 0.001 s)
17/04/17 14:51:10 INFO BlockManager: Removing RDD 1156
17/04/17 14:51:10 INFO MapPartitionsRDD: Removing RDD 1155 from persistence list
17/04/17 14:51:10 INFO BlockManager: Removing RDD 1155
17/04/17 14:51:10 INFO BlockRDD: Removing RDD 1154 from persistence list
17/04/17 14:51:10 INFO BlockManager: Removing RDD 1154
17/04/17 14:51:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1154] at createStream at KafkaConsumer.java:64 of time 1492420870000 ms
17/04/17 14:51:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420850000 ms)
17/04/17 14:51:10 INFO InputInfoTracker: remove old batch metadata: 1492420850000 ms
17/04/17 14:51:20 INFO JobScheduler: Added jobs for time 1492420880000 ms
17/04/17 14:51:20 INFO JobScheduler: Starting job streaming job 1492420880000 ms.0 from job set of time 1492420880000 ms
-------------------------------------------
Time: 1492420880000 ms
-------------------------------------------

17/04/17 14:51:20 INFO JobScheduler: Finished job streaming job 1492420880000 ms.0 from job set of time 1492420880000 ms
17/04/17 14:51:20 INFO MapPartitionsRDD: Removing RDD 1159 from persistence list
17/04/17 14:51:20 INFO JobScheduler: Total delay: 0.004 s for time 1492420880000 ms (execution: 0.000 s)
17/04/17 14:51:20 INFO BlockManager: Removing RDD 1159
17/04/17 14:51:20 INFO MapPartitionsRDD: Removing RDD 1158 from persistence list
17/04/17 14:51:20 INFO BlockManager: Removing RDD 1158
17/04/17 14:51:20 INFO BlockRDD: Removing RDD 1157 from persistence list
17/04/17 14:51:20 INFO BlockManager: Removing RDD 1157
17/04/17 14:51:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1157] at createStream at KafkaConsumer.java:64 of time 1492420880000 ms
17/04/17 14:51:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420860000 ms)
17/04/17 14:51:20 INFO InputInfoTracker: remove old batch metadata: 1492420860000 ms
17/04/17 14:51:30 INFO JobScheduler: Added jobs for time 1492420890000 ms
17/04/17 14:51:30 INFO JobScheduler: Starting job streaming job 1492420890000 ms.0 from job set of time 1492420890000 ms
-------------------------------------------
Time: 1492420890000 ms
-------------------------------------------

17/04/17 14:51:30 INFO JobScheduler: Finished job streaming job 1492420890000 ms.0 from job set of time 1492420890000 ms
17/04/17 14:51:30 INFO MapPartitionsRDD: Removing RDD 1162 from persistence list
17/04/17 14:51:30 INFO JobScheduler: Total delay: 0.004 s for time 1492420890000 ms (execution: 0.000 s)
17/04/17 14:51:30 INFO BlockManager: Removing RDD 1162
17/04/17 14:51:30 INFO MapPartitionsRDD: Removing RDD 1161 from persistence list
17/04/17 14:51:30 INFO BlockManager: Removing RDD 1161
17/04/17 14:51:30 INFO BlockRDD: Removing RDD 1160 from persistence list
17/04/17 14:51:30 INFO BlockManager: Removing RDD 1160
17/04/17 14:51:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1160] at createStream at KafkaConsumer.java:64 of time 1492420890000 ms
17/04/17 14:51:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420870000 ms)
17/04/17 14:51:30 INFO InputInfoTracker: remove old batch metadata: 1492420870000 ms
17/04/17 14:51:40 INFO JobScheduler: Added jobs for time 1492420900000 ms
17/04/17 14:51:40 INFO JobScheduler: Starting job streaming job 1492420900000 ms.0 from job set of time 1492420900000 ms
-------------------------------------------
Time: 1492420900000 ms
-------------------------------------------

17/04/17 14:51:40 INFO JobScheduler: Finished job streaming job 1492420900000 ms.0 from job set of time 1492420900000 ms
17/04/17 14:51:40 INFO MapPartitionsRDD: Removing RDD 1165 from persistence list
17/04/17 14:51:40 INFO JobScheduler: Total delay: 0.005 s for time 1492420900000 ms (execution: 0.001 s)
17/04/17 14:51:40 INFO BlockManager: Removing RDD 1165
17/04/17 14:51:40 INFO MapPartitionsRDD: Removing RDD 1164 from persistence list
17/04/17 14:51:40 INFO BlockManager: Removing RDD 1164
17/04/17 14:51:40 INFO BlockRDD: Removing RDD 1163 from persistence list
17/04/17 14:51:40 INFO BlockManager: Removing RDD 1163
17/04/17 14:51:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1163] at createStream at KafkaConsumer.java:64 of time 1492420900000 ms
17/04/17 14:51:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420880000 ms)
17/04/17 14:51:40 INFO InputInfoTracker: remove old batch metadata: 1492420880000 ms
17/04/17 14:51:50 INFO JobScheduler: Added jobs for time 1492420910000 ms
-------------------------------------------
Time: 1492420910000 ms
-------------------------------------------

17/04/17 14:51:50 INFO JobScheduler: Starting job streaming job 1492420910000 ms.0 from job set of time 1492420910000 ms
17/04/17 14:51:50 INFO JobScheduler: Finished job streaming job 1492420910000 ms.0 from job set of time 1492420910000 ms
17/04/17 14:51:50 INFO JobScheduler: Total delay: 0.004 s for time 1492420910000 ms (execution: 0.000 s)
17/04/17 14:51:50 INFO MapPartitionsRDD: Removing RDD 1168 from persistence list
17/04/17 14:51:50 INFO MapPartitionsRDD: Removing RDD 1167 from persistence list
17/04/17 14:51:50 INFO BlockRDD: Removing RDD 1166 from persistence list
17/04/17 14:51:50 INFO BlockManager: Removing RDD 1168
17/04/17 14:51:50 INFO BlockManager: Removing RDD 1167
17/04/17 14:51:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1166] at createStream at KafkaConsumer.java:64 of time 1492420910000 ms
17/04/17 14:51:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420890000 ms)
17/04/17 14:51:50 INFO InputInfoTracker: remove old batch metadata: 1492420890000 ms
17/04/17 14:51:50 INFO BlockManager: Removing RDD 1166
17/04/17 14:52:00 INFO JobScheduler: Added jobs for time 1492420920000 ms
-------------------------------------------
Time: 1492420920000 ms
-------------------------------------------

17/04/17 14:52:00 INFO JobScheduler: Starting job streaming job 1492420920000 ms.0 from job set of time 1492420920000 ms
17/04/17 14:52:00 INFO JobScheduler: Finished job streaming job 1492420920000 ms.0 from job set of time 1492420920000 ms
17/04/17 14:52:00 INFO JobScheduler: Total delay: 0.004 s for time 1492420920000 ms (execution: 0.000 s)
17/04/17 14:52:00 INFO MapPartitionsRDD: Removing RDD 1171 from persistence list
17/04/17 14:52:00 INFO BlockManager: Removing RDD 1171
17/04/17 14:52:00 INFO MapPartitionsRDD: Removing RDD 1170 from persistence list
17/04/17 14:52:00 INFO BlockManager: Removing RDD 1170
17/04/17 14:52:00 INFO BlockRDD: Removing RDD 1169 from persistence list
17/04/17 14:52:00 INFO BlockManager: Removing RDD 1169
17/04/17 14:52:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1169] at createStream at KafkaConsumer.java:64 of time 1492420920000 ms
17/04/17 14:52:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420900000 ms)
17/04/17 14:52:00 INFO InputInfoTracker: remove old batch metadata: 1492420900000 ms
17/04/17 14:52:10 INFO JobScheduler: Added jobs for time 1492420930000 ms
17/04/17 14:52:10 INFO JobScheduler: Starting job streaming job 1492420930000 ms.0 from job set of time 1492420930000 ms
-------------------------------------------
Time: 1492420930000 ms
-------------------------------------------

17/04/17 14:52:10 INFO JobScheduler: Finished job streaming job 1492420930000 ms.0 from job set of time 1492420930000 ms
17/04/17 14:52:10 INFO MapPartitionsRDD: Removing RDD 1174 from persistence list
17/04/17 14:52:10 INFO JobScheduler: Total delay: 0.005 s for time 1492420930000 ms (execution: 0.001 s)
17/04/17 14:52:10 INFO BlockManager: Removing RDD 1174
17/04/17 14:52:10 INFO MapPartitionsRDD: Removing RDD 1173 from persistence list
17/04/17 14:52:10 INFO BlockManager: Removing RDD 1173
17/04/17 14:52:10 INFO BlockRDD: Removing RDD 1172 from persistence list
17/04/17 14:52:10 INFO BlockManager: Removing RDD 1172
17/04/17 14:52:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1172] at createStream at KafkaConsumer.java:64 of time 1492420930000 ms
17/04/17 14:52:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420910000 ms)
17/04/17 14:52:10 INFO InputInfoTracker: remove old batch metadata: 1492420910000 ms
17/04/17 14:52:20 INFO JobScheduler: Added jobs for time 1492420940000 ms
17/04/17 14:52:20 INFO JobScheduler: Starting job streaming job 1492420940000 ms.0 from job set of time 1492420940000 ms
-------------------------------------------
Time: 1492420940000 ms
-------------------------------------------

17/04/17 14:52:20 INFO JobScheduler: Finished job streaming job 1492420940000 ms.0 from job set of time 1492420940000 ms
17/04/17 14:52:20 INFO MapPartitionsRDD: Removing RDD 1177 from persistence list
17/04/17 14:52:20 INFO JobScheduler: Total delay: 0.005 s for time 1492420940000 ms (execution: 0.000 s)
17/04/17 14:52:20 INFO BlockManager: Removing RDD 1177
17/04/17 14:52:20 INFO MapPartitionsRDD: Removing RDD 1176 from persistence list
17/04/17 14:52:20 INFO BlockManager: Removing RDD 1176
17/04/17 14:52:20 INFO BlockRDD: Removing RDD 1175 from persistence list
17/04/17 14:52:20 INFO BlockManager: Removing RDD 1175
17/04/17 14:52:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1175] at createStream at KafkaConsumer.java:64 of time 1492420940000 ms
17/04/17 14:52:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420920000 ms)
17/04/17 14:52:20 INFO InputInfoTracker: remove old batch metadata: 1492420920000 ms
17/04/17 14:52:30 INFO JobScheduler: Added jobs for time 1492420950000 ms
17/04/17 14:52:30 INFO JobScheduler: Starting job streaming job 1492420950000 ms.0 from job set of time 1492420950000 ms
-------------------------------------------
Time: 1492420950000 ms
-------------------------------------------

17/04/17 14:52:30 INFO JobScheduler: Finished job streaming job 1492420950000 ms.0 from job set of time 1492420950000 ms
17/04/17 14:52:30 INFO MapPartitionsRDD: Removing RDD 1180 from persistence list
17/04/17 14:52:30 INFO JobScheduler: Total delay: 0.003 s for time 1492420950000 ms (execution: 0.000 s)
17/04/17 14:52:30 INFO BlockManager: Removing RDD 1180
17/04/17 14:52:30 INFO MapPartitionsRDD: Removing RDD 1179 from persistence list
17/04/17 14:52:30 INFO BlockManager: Removing RDD 1179
17/04/17 14:52:30 INFO BlockRDD: Removing RDD 1178 from persistence list
17/04/17 14:52:30 INFO BlockManager: Removing RDD 1178
17/04/17 14:52:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1178] at createStream at KafkaConsumer.java:64 of time 1492420950000 ms
17/04/17 14:52:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420930000 ms)
17/04/17 14:52:30 INFO InputInfoTracker: remove old batch metadata: 1492420930000 ms
17/04/17 14:52:40 INFO JobScheduler: Added jobs for time 1492420960000 ms
17/04/17 14:52:40 INFO JobScheduler: Starting job streaming job 1492420960000 ms.0 from job set of time 1492420960000 ms
-------------------------------------------
Time: 1492420960000 ms
-------------------------------------------

17/04/17 14:52:40 INFO JobScheduler: Finished job streaming job 1492420960000 ms.0 from job set of time 1492420960000 ms
17/04/17 14:52:40 INFO MapPartitionsRDD: Removing RDD 1183 from persistence list
17/04/17 14:52:40 INFO JobScheduler: Total delay: 0.004 s for time 1492420960000 ms (execution: 0.001 s)
17/04/17 14:52:40 INFO BlockManager: Removing RDD 1183
17/04/17 14:52:40 INFO MapPartitionsRDD: Removing RDD 1182 from persistence list
17/04/17 14:52:40 INFO BlockManager: Removing RDD 1182
17/04/17 14:52:40 INFO BlockRDD: Removing RDD 1181 from persistence list
17/04/17 14:52:40 INFO BlockManager: Removing RDD 1181
17/04/17 14:52:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1181] at createStream at KafkaConsumer.java:64 of time 1492420960000 ms
17/04/17 14:52:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420940000 ms)
17/04/17 14:52:40 INFO InputInfoTracker: remove old batch metadata: 1492420940000 ms
17/04/17 14:52:50 INFO JobScheduler: Added jobs for time 1492420970000 ms
17/04/17 14:52:50 INFO JobScheduler: Starting job streaming job 1492420970000 ms.0 from job set of time 1492420970000 ms
-------------------------------------------
Time: 1492420970000 ms
-------------------------------------------

17/04/17 14:52:50 INFO JobScheduler: Finished job streaming job 1492420970000 ms.0 from job set of time 1492420970000 ms
17/04/17 14:52:50 INFO MapPartitionsRDD: Removing RDD 1186 from persistence list
17/04/17 14:52:50 INFO JobScheduler: Total delay: 0.006 s for time 1492420970000 ms (execution: 0.001 s)
17/04/17 14:52:50 INFO BlockManager: Removing RDD 1186
17/04/17 14:52:50 INFO MapPartitionsRDD: Removing RDD 1185 from persistence list
17/04/17 14:52:50 INFO BlockManager: Removing RDD 1185
17/04/17 14:52:50 INFO BlockRDD: Removing RDD 1184 from persistence list
17/04/17 14:52:50 INFO BlockManager: Removing RDD 1184
17/04/17 14:52:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1184] at createStream at KafkaConsumer.java:64 of time 1492420970000 ms
17/04/17 14:52:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420950000 ms)
17/04/17 14:52:50 INFO InputInfoTracker: remove old batch metadata: 1492420950000 ms
17/04/17 14:53:00 INFO JobScheduler: Added jobs for time 1492420980000 ms
17/04/17 14:53:00 INFO JobScheduler: Starting job streaming job 1492420980000 ms.0 from job set of time 1492420980000 ms
-------------------------------------------
Time: 1492420980000 ms
-------------------------------------------

17/04/17 14:53:00 INFO JobScheduler: Finished job streaming job 1492420980000 ms.0 from job set of time 1492420980000 ms
17/04/17 14:53:00 INFO MapPartitionsRDD: Removing RDD 1189 from persistence list
17/04/17 14:53:00 INFO JobScheduler: Total delay: 0.005 s for time 1492420980000 ms (execution: 0.001 s)
17/04/17 14:53:00 INFO BlockManager: Removing RDD 1189
17/04/17 14:53:00 INFO MapPartitionsRDD: Removing RDD 1188 from persistence list
17/04/17 14:53:00 INFO BlockManager: Removing RDD 1188
17/04/17 14:53:00 INFO BlockRDD: Removing RDD 1187 from persistence list
17/04/17 14:53:00 INFO BlockManager: Removing RDD 1187
17/04/17 14:53:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1187] at createStream at KafkaConsumer.java:64 of time 1492420980000 ms
17/04/17 14:53:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420960000 ms)
17/04/17 14:53:00 INFO InputInfoTracker: remove old batch metadata: 1492420960000 ms
17/04/17 14:53:10 INFO JobScheduler: Added jobs for time 1492420990000 ms
-------------------------------------------
Time: 1492420990000 ms
-------------------------------------------

17/04/17 14:53:10 INFO JobScheduler: Starting job streaming job 1492420990000 ms.0 from job set of time 1492420990000 ms
17/04/17 14:53:10 INFO JobScheduler: Finished job streaming job 1492420990000 ms.0 from job set of time 1492420990000 ms
17/04/17 14:53:10 INFO MapPartitionsRDD: Removing RDD 1192 from persistence list
17/04/17 14:53:10 INFO JobScheduler: Total delay: 0.004 s for time 1492420990000 ms (execution: 0.000 s)
17/04/17 14:53:10 INFO BlockManager: Removing RDD 1192
17/04/17 14:53:10 INFO MapPartitionsRDD: Removing RDD 1191 from persistence list
17/04/17 14:53:10 INFO BlockManager: Removing RDD 1191
17/04/17 14:53:10 INFO BlockRDD: Removing RDD 1190 from persistence list
17/04/17 14:53:10 INFO BlockManager: Removing RDD 1190
17/04/17 14:53:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1190] at createStream at KafkaConsumer.java:64 of time 1492420990000 ms
17/04/17 14:53:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420970000 ms)
17/04/17 14:53:10 INFO InputInfoTracker: remove old batch metadata: 1492420970000 ms
17/04/17 14:53:20 INFO JobScheduler: Added jobs for time 1492421000000 ms
17/04/17 14:53:20 INFO JobScheduler: Starting job streaming job 1492421000000 ms.0 from job set of time 1492421000000 ms
-------------------------------------------
Time: 1492421000000 ms
-------------------------------------------

17/04/17 14:53:20 INFO JobScheduler: Finished job streaming job 1492421000000 ms.0 from job set of time 1492421000000 ms
17/04/17 14:53:20 INFO MapPartitionsRDD: Removing RDD 1195 from persistence list
17/04/17 14:53:20 INFO JobScheduler: Total delay: 0.005 s for time 1492421000000 ms (execution: 0.001 s)
17/04/17 14:53:20 INFO BlockManager: Removing RDD 1195
17/04/17 14:53:20 INFO MapPartitionsRDD: Removing RDD 1194 from persistence list
17/04/17 14:53:20 INFO BlockRDD: Removing RDD 1193 from persistence list
17/04/17 14:53:20 INFO BlockManager: Removing RDD 1194
17/04/17 14:53:20 INFO BlockManager: Removing RDD 1193
17/04/17 14:53:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1193] at createStream at KafkaConsumer.java:64 of time 1492421000000 ms
17/04/17 14:53:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420980000 ms)
17/04/17 14:53:20 INFO InputInfoTracker: remove old batch metadata: 1492420980000 ms
17/04/17 14:53:30 INFO JobScheduler: Added jobs for time 1492421010000 ms
17/04/17 14:53:30 INFO JobScheduler: Starting job streaming job 1492421010000 ms.0 from job set of time 1492421010000 ms
-------------------------------------------
Time: 1492421010000 ms
-------------------------------------------

17/04/17 14:53:30 INFO JobScheduler: Finished job streaming job 1492421010000 ms.0 from job set of time 1492421010000 ms
17/04/17 14:53:30 INFO MapPartitionsRDD: Removing RDD 1198 from persistence list
17/04/17 14:53:30 INFO JobScheduler: Total delay: 0.005 s for time 1492421010000 ms (execution: 0.001 s)
17/04/17 14:53:30 INFO BlockManager: Removing RDD 1198
17/04/17 14:53:30 INFO MapPartitionsRDD: Removing RDD 1197 from persistence list
17/04/17 14:53:30 INFO BlockManager: Removing RDD 1197
17/04/17 14:53:30 INFO BlockRDD: Removing RDD 1196 from persistence list
17/04/17 14:53:30 INFO BlockManager: Removing RDD 1196
17/04/17 14:53:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1196] at createStream at KafkaConsumer.java:64 of time 1492421010000 ms
17/04/17 14:53:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492420990000 ms)
17/04/17 14:53:30 INFO InputInfoTracker: remove old batch metadata: 1492420990000 ms
17/04/17 14:53:40 INFO JobScheduler: Added jobs for time 1492421020000 ms
17/04/17 14:53:40 INFO JobScheduler: Starting job streaming job 1492421020000 ms.0 from job set of time 1492421020000 ms
-------------------------------------------
Time: 1492421020000 ms
-------------------------------------------

17/04/17 14:53:40 INFO JobScheduler: Finished job streaming job 1492421020000 ms.0 from job set of time 1492421020000 ms
17/04/17 14:53:40 INFO MapPartitionsRDD: Removing RDD 1201 from persistence list
17/04/17 14:53:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421020000 ms (execution: 0.000 s)
17/04/17 14:53:40 INFO MapPartitionsRDD: Removing RDD 1200 from persistence list
17/04/17 14:53:40 INFO BlockManager: Removing RDD 1201
17/04/17 14:53:40 INFO BlockManager: Removing RDD 1200
17/04/17 14:53:40 INFO BlockRDD: Removing RDD 1199 from persistence list
17/04/17 14:53:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1199] at createStream at KafkaConsumer.java:64 of time 1492421020000 ms
17/04/17 14:53:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421000000 ms)
17/04/17 14:53:40 INFO InputInfoTracker: remove old batch metadata: 1492421000000 ms
17/04/17 14:53:40 INFO BlockManager: Removing RDD 1199
17/04/17 14:53:50 INFO JobScheduler: Added jobs for time 1492421030000 ms
17/04/17 14:53:50 INFO JobScheduler: Starting job streaming job 1492421030000 ms.0 from job set of time 1492421030000 ms
-------------------------------------------
Time: 1492421030000 ms
-------------------------------------------

17/04/17 14:53:50 INFO JobScheduler: Finished job streaming job 1492421030000 ms.0 from job set of time 1492421030000 ms
17/04/17 14:53:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421030000 ms (execution: 0.000 s)
17/04/17 14:53:50 INFO MapPartitionsRDD: Removing RDD 1204 from persistence list
17/04/17 14:53:50 INFO BlockManager: Removing RDD 1204
17/04/17 14:53:50 INFO MapPartitionsRDD: Removing RDD 1203 from persistence list
17/04/17 14:53:50 INFO BlockManager: Removing RDD 1203
17/04/17 14:53:50 INFO BlockRDD: Removing RDD 1202 from persistence list
17/04/17 14:53:50 INFO BlockManager: Removing RDD 1202
17/04/17 14:53:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1202] at createStream at KafkaConsumer.java:64 of time 1492421030000 ms
17/04/17 14:53:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421010000 ms)
17/04/17 14:53:50 INFO InputInfoTracker: remove old batch metadata: 1492421010000 ms
17/04/17 14:54:00 INFO JobScheduler: Added jobs for time 1492421040000 ms
17/04/17 14:54:00 INFO JobScheduler: Starting job streaming job 1492421040000 ms.0 from job set of time 1492421040000 ms
-------------------------------------------
Time: 1492421040000 ms
-------------------------------------------

17/04/17 14:54:00 INFO JobScheduler: Finished job streaming job 1492421040000 ms.0 from job set of time 1492421040000 ms
17/04/17 14:54:00 INFO MapPartitionsRDD: Removing RDD 1207 from persistence list
17/04/17 14:54:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421040000 ms (execution: 0.000 s)
17/04/17 14:54:00 INFO MapPartitionsRDD: Removing RDD 1206 from persistence list
17/04/17 14:54:00 INFO BlockManager: Removing RDD 1207
17/04/17 14:54:00 INFO BlockManager: Removing RDD 1206
17/04/17 14:54:00 INFO BlockRDD: Removing RDD 1205 from persistence list
17/04/17 14:54:00 INFO BlockManager: Removing RDD 1205
17/04/17 14:54:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1205] at createStream at KafkaConsumer.java:64 of time 1492421040000 ms
17/04/17 14:54:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421020000 ms)
17/04/17 14:54:00 INFO InputInfoTracker: remove old batch metadata: 1492421020000 ms
17/04/17 14:54:10 INFO JobScheduler: Added jobs for time 1492421050000 ms
17/04/17 14:54:10 INFO JobScheduler: Starting job streaming job 1492421050000 ms.0 from job set of time 1492421050000 ms
-------------------------------------------
Time: 1492421050000 ms
-------------------------------------------

17/04/17 14:54:10 INFO JobScheduler: Finished job streaming job 1492421050000 ms.0 from job set of time 1492421050000 ms
17/04/17 14:54:10 INFO MapPartitionsRDD: Removing RDD 1210 from persistence list
17/04/17 14:54:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421050000 ms (execution: 0.000 s)
17/04/17 14:54:10 INFO BlockManager: Removing RDD 1210
17/04/17 14:54:10 INFO MapPartitionsRDD: Removing RDD 1209 from persistence list
17/04/17 14:54:10 INFO BlockManager: Removing RDD 1209
17/04/17 14:54:10 INFO BlockRDD: Removing RDD 1208 from persistence list
17/04/17 14:54:10 INFO BlockManager: Removing RDD 1208
17/04/17 14:54:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1208] at createStream at KafkaConsumer.java:64 of time 1492421050000 ms
17/04/17 14:54:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421030000 ms)
17/04/17 14:54:10 INFO InputInfoTracker: remove old batch metadata: 1492421030000 ms
17/04/17 14:54:20 INFO JobScheduler: Added jobs for time 1492421060000 ms
-------------------------------------------
Time: 1492421060000 ms
-------------------------------------------

17/04/17 14:54:20 INFO JobScheduler: Starting job streaming job 1492421060000 ms.0 from job set of time 1492421060000 ms
17/04/17 14:54:20 INFO JobScheduler: Finished job streaming job 1492421060000 ms.0 from job set of time 1492421060000 ms
17/04/17 14:54:20 INFO MapPartitionsRDD: Removing RDD 1213 from persistence list
17/04/17 14:54:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421060000 ms (execution: 0.000 s)
17/04/17 14:54:20 INFO BlockManager: Removing RDD 1213
17/04/17 14:54:20 INFO MapPartitionsRDD: Removing RDD 1212 from persistence list
17/04/17 14:54:20 INFO BlockManager: Removing RDD 1212
17/04/17 14:54:20 INFO BlockRDD: Removing RDD 1211 from persistence list
17/04/17 14:54:20 INFO BlockManager: Removing RDD 1211
17/04/17 14:54:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1211] at createStream at KafkaConsumer.java:64 of time 1492421060000 ms
17/04/17 14:54:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421040000 ms)
17/04/17 14:54:20 INFO InputInfoTracker: remove old batch metadata: 1492421040000 ms
17/04/17 14:54:30 INFO JobScheduler: Added jobs for time 1492421070000 ms
17/04/17 14:54:30 INFO JobScheduler: Starting job streaming job 1492421070000 ms.0 from job set of time 1492421070000 ms
-------------------------------------------
Time: 1492421070000 ms
-------------------------------------------

17/04/17 14:54:30 INFO JobScheduler: Finished job streaming job 1492421070000 ms.0 from job set of time 1492421070000 ms
17/04/17 14:54:30 INFO MapPartitionsRDD: Removing RDD 1216 from persistence list
17/04/17 14:54:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421070000 ms (execution: 0.001 s)
17/04/17 14:54:30 INFO BlockManager: Removing RDD 1216
17/04/17 14:54:30 INFO MapPartitionsRDD: Removing RDD 1215 from persistence list
17/04/17 14:54:30 INFO BlockManager: Removing RDD 1215
17/04/17 14:54:30 INFO BlockRDD: Removing RDD 1214 from persistence list
17/04/17 14:54:30 INFO BlockManager: Removing RDD 1214
17/04/17 14:54:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1214] at createStream at KafkaConsumer.java:64 of time 1492421070000 ms
17/04/17 14:54:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421050000 ms)
17/04/17 14:54:30 INFO InputInfoTracker: remove old batch metadata: 1492421050000 ms
17/04/17 14:54:40 INFO JobScheduler: Added jobs for time 1492421080000 ms
17/04/17 14:54:40 INFO JobScheduler: Starting job streaming job 1492421080000 ms.0 from job set of time 1492421080000 ms
-------------------------------------------
Time: 1492421080000 ms
-------------------------------------------

17/04/17 14:54:40 INFO JobScheduler: Finished job streaming job 1492421080000 ms.0 from job set of time 1492421080000 ms
17/04/17 14:54:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421080000 ms (execution: 0.000 s)
17/04/17 14:54:40 INFO MapPartitionsRDD: Removing RDD 1219 from persistence list
17/04/17 14:54:40 INFO BlockManager: Removing RDD 1219
17/04/17 14:54:40 INFO MapPartitionsRDD: Removing RDD 1218 from persistence list
17/04/17 14:54:40 INFO BlockManager: Removing RDD 1218
17/04/17 14:54:40 INFO BlockRDD: Removing RDD 1217 from persistence list
17/04/17 14:54:40 INFO BlockManager: Removing RDD 1217
17/04/17 14:54:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1217] at createStream at KafkaConsumer.java:64 of time 1492421080000 ms
17/04/17 14:54:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421060000 ms)
17/04/17 14:54:40 INFO InputInfoTracker: remove old batch metadata: 1492421060000 ms
17/04/17 14:54:50 INFO JobScheduler: Added jobs for time 1492421090000 ms
17/04/17 14:54:50 INFO JobScheduler: Starting job streaming job 1492421090000 ms.0 from job set of time 1492421090000 ms
-------------------------------------------
Time: 1492421090000 ms
-------------------------------------------

17/04/17 14:54:50 INFO JobScheduler: Finished job streaming job 1492421090000 ms.0 from job set of time 1492421090000 ms
17/04/17 14:54:50 INFO MapPartitionsRDD: Removing RDD 1222 from persistence list
17/04/17 14:54:50 INFO JobScheduler: Total delay: 0.007 s for time 1492421090000 ms (execution: 0.000 s)
17/04/17 14:54:50 INFO MapPartitionsRDD: Removing RDD 1221 from persistence list
17/04/17 14:54:50 INFO BlockManager: Removing RDD 1222
17/04/17 14:54:50 INFO BlockManager: Removing RDD 1221
17/04/17 14:54:50 INFO BlockRDD: Removing RDD 1220 from persistence list
17/04/17 14:54:50 INFO BlockManager: Removing RDD 1220
17/04/17 14:54:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1220] at createStream at KafkaConsumer.java:64 of time 1492421090000 ms
17/04/17 14:54:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421070000 ms)
17/04/17 14:54:50 INFO InputInfoTracker: remove old batch metadata: 1492421070000 ms
17/04/17 14:55:00 INFO JobScheduler: Added jobs for time 1492421100000 ms
17/04/17 14:55:00 INFO JobScheduler: Starting job streaming job 1492421100000 ms.0 from job set of time 1492421100000 ms
-------------------------------------------
Time: 1492421100000 ms
-------------------------------------------

17/04/17 14:55:00 INFO JobScheduler: Finished job streaming job 1492421100000 ms.0 from job set of time 1492421100000 ms
17/04/17 14:55:00 INFO MapPartitionsRDD: Removing RDD 1225 from persistence list
17/04/17 14:55:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421100000 ms (execution: 0.000 s)
17/04/17 14:55:00 INFO MapPartitionsRDD: Removing RDD 1224 from persistence list
17/04/17 14:55:00 INFO BlockManager: Removing RDD 1225
17/04/17 14:55:00 INFO BlockManager: Removing RDD 1224
17/04/17 14:55:00 INFO BlockRDD: Removing RDD 1223 from persistence list
17/04/17 14:55:00 INFO BlockManager: Removing RDD 1223
17/04/17 14:55:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1223] at createStream at KafkaConsumer.java:64 of time 1492421100000 ms
17/04/17 14:55:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421080000 ms)
17/04/17 14:55:00 INFO InputInfoTracker: remove old batch metadata: 1492421080000 ms
17/04/17 14:55:10 INFO JobScheduler: Added jobs for time 1492421110000 ms
17/04/17 14:55:10 INFO JobScheduler: Starting job streaming job 1492421110000 ms.0 from job set of time 1492421110000 ms
-------------------------------------------
Time: 1492421110000 ms
-------------------------------------------

17/04/17 14:55:10 INFO JobScheduler: Finished job streaming job 1492421110000 ms.0 from job set of time 1492421110000 ms
17/04/17 14:55:10 INFO MapPartitionsRDD: Removing RDD 1228 from persistence list
17/04/17 14:55:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421110000 ms (execution: 0.000 s)
17/04/17 14:55:10 INFO BlockManager: Removing RDD 1228
17/04/17 14:55:10 INFO MapPartitionsRDD: Removing RDD 1227 from persistence list
17/04/17 14:55:10 INFO BlockManager: Removing RDD 1227
17/04/17 14:55:10 INFO BlockRDD: Removing RDD 1226 from persistence list
17/04/17 14:55:10 INFO BlockManager: Removing RDD 1226
17/04/17 14:55:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1226] at createStream at KafkaConsumer.java:64 of time 1492421110000 ms
17/04/17 14:55:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421090000 ms)
17/04/17 14:55:10 INFO InputInfoTracker: remove old batch metadata: 1492421090000 ms
17/04/17 14:55:20 INFO JobScheduler: Added jobs for time 1492421120000 ms
17/04/17 14:55:20 INFO JobScheduler: Starting job streaming job 1492421120000 ms.0 from job set of time 1492421120000 ms
-------------------------------------------
Time: 1492421120000 ms
-------------------------------------------

17/04/17 14:55:20 INFO JobScheduler: Finished job streaming job 1492421120000 ms.0 from job set of time 1492421120000 ms
17/04/17 14:55:20 INFO MapPartitionsRDD: Removing RDD 1231 from persistence list
17/04/17 14:55:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421120000 ms (execution: 0.000 s)
17/04/17 14:55:20 INFO BlockManager: Removing RDD 1231
17/04/17 14:55:20 INFO MapPartitionsRDD: Removing RDD 1230 from persistence list
17/04/17 14:55:20 INFO BlockManager: Removing RDD 1230
17/04/17 14:55:20 INFO BlockRDD: Removing RDD 1229 from persistence list
17/04/17 14:55:20 INFO BlockManager: Removing RDD 1229
17/04/17 14:55:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1229] at createStream at KafkaConsumer.java:64 of time 1492421120000 ms
17/04/17 14:55:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421100000 ms)
17/04/17 14:55:20 INFO InputInfoTracker: remove old batch metadata: 1492421100000 ms
17/04/17 14:55:30 INFO JobScheduler: Added jobs for time 1492421130000 ms
17/04/17 14:55:30 INFO JobScheduler: Starting job streaming job 1492421130000 ms.0 from job set of time 1492421130000 ms
-------------------------------------------
Time: 1492421130000 ms
-------------------------------------------

17/04/17 14:55:30 INFO JobScheduler: Finished job streaming job 1492421130000 ms.0 from job set of time 1492421130000 ms
17/04/17 14:55:30 INFO MapPartitionsRDD: Removing RDD 1234 from persistence list
17/04/17 14:55:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421130000 ms (execution: 0.000 s)
17/04/17 14:55:30 INFO BlockManager: Removing RDD 1234
17/04/17 14:55:30 INFO MapPartitionsRDD: Removing RDD 1233 from persistence list
17/04/17 14:55:30 INFO BlockManager: Removing RDD 1233
17/04/17 14:55:30 INFO BlockRDD: Removing RDD 1232 from persistence list
17/04/17 14:55:30 INFO BlockManager: Removing RDD 1232
17/04/17 14:55:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1232] at createStream at KafkaConsumer.java:64 of time 1492421130000 ms
17/04/17 14:55:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421110000 ms)
17/04/17 14:55:30 INFO InputInfoTracker: remove old batch metadata: 1492421110000 ms
17/04/17 14:55:40 INFO JobScheduler: Added jobs for time 1492421140000 ms
17/04/17 14:55:40 INFO JobScheduler: Starting job streaming job 1492421140000 ms.0 from job set of time 1492421140000 ms
-------------------------------------------
Time: 1492421140000 ms
-------------------------------------------

17/04/17 14:55:40 INFO JobScheduler: Finished job streaming job 1492421140000 ms.0 from job set of time 1492421140000 ms
17/04/17 14:55:40 INFO MapPartitionsRDD: Removing RDD 1237 from persistence list
17/04/17 14:55:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421140000 ms (execution: 0.001 s)
17/04/17 14:55:40 INFO BlockManager: Removing RDD 1237
17/04/17 14:55:40 INFO MapPartitionsRDD: Removing RDD 1236 from persistence list
17/04/17 14:55:40 INFO BlockManager: Removing RDD 1236
17/04/17 14:55:40 INFO BlockRDD: Removing RDD 1235 from persistence list
17/04/17 14:55:40 INFO BlockManager: Removing RDD 1235
17/04/17 14:55:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1235] at createStream at KafkaConsumer.java:64 of time 1492421140000 ms
17/04/17 14:55:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421120000 ms)
17/04/17 14:55:40 INFO InputInfoTracker: remove old batch metadata: 1492421120000 ms
17/04/17 14:55:50 INFO JobScheduler: Added jobs for time 1492421150000 ms
17/04/17 14:55:50 INFO JobScheduler: Starting job streaming job 1492421150000 ms.0 from job set of time 1492421150000 ms
-------------------------------------------
Time: 1492421150000 ms
-------------------------------------------

17/04/17 14:55:50 INFO JobScheduler: Finished job streaming job 1492421150000 ms.0 from job set of time 1492421150000 ms
17/04/17 14:55:50 INFO MapPartitionsRDD: Removing RDD 1240 from persistence list
17/04/17 14:55:50 INFO JobScheduler: Total delay: 0.005 s for time 1492421150000 ms (execution: 0.000 s)
17/04/17 14:55:50 INFO BlockManager: Removing RDD 1240
17/04/17 14:55:50 INFO MapPartitionsRDD: Removing RDD 1239 from persistence list
17/04/17 14:55:50 INFO BlockManager: Removing RDD 1239
17/04/17 14:55:50 INFO BlockRDD: Removing RDD 1238 from persistence list
17/04/17 14:55:50 INFO BlockManager: Removing RDD 1238
17/04/17 14:55:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1238] at createStream at KafkaConsumer.java:64 of time 1492421150000 ms
17/04/17 14:55:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421130000 ms)
17/04/17 14:55:50 INFO InputInfoTracker: remove old batch metadata: 1492421130000 ms
17/04/17 14:56:00 INFO JobScheduler: Added jobs for time 1492421160000 ms
17/04/17 14:56:00 INFO JobScheduler: Starting job streaming job 1492421160000 ms.0 from job set of time 1492421160000 ms
-------------------------------------------
Time: 1492421160000 ms
-------------------------------------------

17/04/17 14:56:00 INFO JobScheduler: Finished job streaming job 1492421160000 ms.0 from job set of time 1492421160000 ms
17/04/17 14:56:00 INFO MapPartitionsRDD: Removing RDD 1243 from persistence list
17/04/17 14:56:00 INFO JobScheduler: Total delay: 0.005 s for time 1492421160000 ms (execution: 0.001 s)
17/04/17 14:56:00 INFO BlockManager: Removing RDD 1243
17/04/17 14:56:00 INFO MapPartitionsRDD: Removing RDD 1242 from persistence list
17/04/17 14:56:00 INFO BlockManager: Removing RDD 1242
17/04/17 14:56:00 INFO BlockRDD: Removing RDD 1241 from persistence list
17/04/17 14:56:00 INFO BlockManager: Removing RDD 1241
17/04/17 14:56:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1241] at createStream at KafkaConsumer.java:64 of time 1492421160000 ms
17/04/17 14:56:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421140000 ms)
17/04/17 14:56:00 INFO InputInfoTracker: remove old batch metadata: 1492421140000 ms
17/04/17 14:56:10 INFO JobScheduler: Added jobs for time 1492421170000 ms
17/04/17 14:56:10 INFO JobScheduler: Starting job streaming job 1492421170000 ms.0 from job set of time 1492421170000 ms
-------------------------------------------
Time: 1492421170000 ms
-------------------------------------------

17/04/17 14:56:10 INFO JobScheduler: Finished job streaming job 1492421170000 ms.0 from job set of time 1492421170000 ms
17/04/17 14:56:10 INFO MapPartitionsRDD: Removing RDD 1246 from persistence list
17/04/17 14:56:10 INFO JobScheduler: Total delay: 0.005 s for time 1492421170000 ms (execution: 0.001 s)
17/04/17 14:56:10 INFO BlockManager: Removing RDD 1246
17/04/17 14:56:10 INFO MapPartitionsRDD: Removing RDD 1245 from persistence list
17/04/17 14:56:10 INFO BlockManager: Removing RDD 1245
17/04/17 14:56:10 INFO BlockRDD: Removing RDD 1244 from persistence list
17/04/17 14:56:10 INFO BlockManager: Removing RDD 1244
17/04/17 14:56:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1244] at createStream at KafkaConsumer.java:64 of time 1492421170000 ms
17/04/17 14:56:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421150000 ms)
17/04/17 14:56:10 INFO InputInfoTracker: remove old batch metadata: 1492421150000 ms
17/04/17 14:56:20 INFO JobScheduler: Added jobs for time 1492421180000 ms
17/04/17 14:56:20 INFO JobScheduler: Starting job streaming job 1492421180000 ms.0 from job set of time 1492421180000 ms
-------------------------------------------
Time: 1492421180000 ms
-------------------------------------------

17/04/17 14:56:20 INFO JobScheduler: Finished job streaming job 1492421180000 ms.0 from job set of time 1492421180000 ms
17/04/17 14:56:20 INFO MapPartitionsRDD: Removing RDD 1249 from persistence list
17/04/17 14:56:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421180000 ms (execution: 0.000 s)
17/04/17 14:56:20 INFO BlockManager: Removing RDD 1249
17/04/17 14:56:20 INFO MapPartitionsRDD: Removing RDD 1248 from persistence list
17/04/17 14:56:20 INFO BlockManager: Removing RDD 1248
17/04/17 14:56:20 INFO BlockRDD: Removing RDD 1247 from persistence list
17/04/17 14:56:20 INFO BlockManager: Removing RDD 1247
17/04/17 14:56:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1247] at createStream at KafkaConsumer.java:64 of time 1492421180000 ms
17/04/17 14:56:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421160000 ms)
17/04/17 14:56:20 INFO InputInfoTracker: remove old batch metadata: 1492421160000 ms
17/04/17 14:56:30 INFO JobScheduler: Added jobs for time 1492421190000 ms
17/04/17 14:56:30 INFO JobScheduler: Starting job streaming job 1492421190000 ms.0 from job set of time 1492421190000 ms
-------------------------------------------
Time: 1492421190000 ms
-------------------------------------------

17/04/17 14:56:30 INFO JobScheduler: Finished job streaming job 1492421190000 ms.0 from job set of time 1492421190000 ms
17/04/17 14:56:30 INFO MapPartitionsRDD: Removing RDD 1252 from persistence list
17/04/17 14:56:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421190000 ms (execution: 0.000 s)
17/04/17 14:56:30 INFO BlockManager: Removing RDD 1252
17/04/17 14:56:30 INFO MapPartitionsRDD: Removing RDD 1251 from persistence list
17/04/17 14:56:30 INFO BlockManager: Removing RDD 1251
17/04/17 14:56:30 INFO BlockRDD: Removing RDD 1250 from persistence list
17/04/17 14:56:30 INFO BlockManager: Removing RDD 1250
17/04/17 14:56:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1250] at createStream at KafkaConsumer.java:64 of time 1492421190000 ms
17/04/17 14:56:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421170000 ms)
17/04/17 14:56:30 INFO InputInfoTracker: remove old batch metadata: 1492421170000 ms
17/04/17 14:56:40 INFO JobScheduler: Added jobs for time 1492421200000 ms
17/04/17 14:56:40 INFO JobScheduler: Starting job streaming job 1492421200000 ms.0 from job set of time 1492421200000 ms
-------------------------------------------
Time: 1492421200000 ms
-------------------------------------------

17/04/17 14:56:40 INFO JobScheduler: Finished job streaming job 1492421200000 ms.0 from job set of time 1492421200000 ms
17/04/17 14:56:40 INFO MapPartitionsRDD: Removing RDD 1255 from persistence list
17/04/17 14:56:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421200000 ms (execution: 0.000 s)
17/04/17 14:56:40 INFO BlockManager: Removing RDD 1255
17/04/17 14:56:40 INFO MapPartitionsRDD: Removing RDD 1254 from persistence list
17/04/17 14:56:40 INFO BlockManager: Removing RDD 1254
17/04/17 14:56:40 INFO BlockRDD: Removing RDD 1253 from persistence list
17/04/17 14:56:40 INFO BlockManager: Removing RDD 1253
17/04/17 14:56:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1253] at createStream at KafkaConsumer.java:64 of time 1492421200000 ms
17/04/17 14:56:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421180000 ms)
17/04/17 14:56:40 INFO InputInfoTracker: remove old batch metadata: 1492421180000 ms
17/04/17 14:56:50 INFO JobScheduler: Added jobs for time 1492421210000 ms
17/04/17 14:56:50 INFO JobScheduler: Starting job streaming job 1492421210000 ms.0 from job set of time 1492421210000 ms
-------------------------------------------
Time: 1492421210000 ms
-------------------------------------------

17/04/17 14:56:50 INFO JobScheduler: Finished job streaming job 1492421210000 ms.0 from job set of time 1492421210000 ms
17/04/17 14:56:50 INFO MapPartitionsRDD: Removing RDD 1258 from persistence list
17/04/17 14:56:50 INFO JobScheduler: Total delay: 0.006 s for time 1492421210000 ms (execution: 0.001 s)
17/04/17 14:56:50 INFO BlockManager: Removing RDD 1258
17/04/17 14:56:50 INFO MapPartitionsRDD: Removing RDD 1257 from persistence list
17/04/17 14:56:50 INFO BlockManager: Removing RDD 1257
17/04/17 14:56:50 INFO BlockRDD: Removing RDD 1256 from persistence list
17/04/17 14:56:50 INFO BlockManager: Removing RDD 1256
17/04/17 14:56:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1256] at createStream at KafkaConsumer.java:64 of time 1492421210000 ms
17/04/17 14:56:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421190000 ms)
17/04/17 14:56:50 INFO InputInfoTracker: remove old batch metadata: 1492421190000 ms
17/04/17 14:57:00 INFO JobScheduler: Added jobs for time 1492421220000 ms
17/04/17 14:57:00 INFO JobScheduler: Starting job streaming job 1492421220000 ms.0 from job set of time 1492421220000 ms
-------------------------------------------
Time: 1492421220000 ms
-------------------------------------------

17/04/17 14:57:00 INFO JobScheduler: Finished job streaming job 1492421220000 ms.0 from job set of time 1492421220000 ms
17/04/17 14:57:00 INFO MapPartitionsRDD: Removing RDD 1261 from persistence list
17/04/17 14:57:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421220000 ms (execution: 0.000 s)
17/04/17 14:57:00 INFO BlockManager: Removing RDD 1261
17/04/17 14:57:00 INFO MapPartitionsRDD: Removing RDD 1260 from persistence list
17/04/17 14:57:00 INFO BlockManager: Removing RDD 1260
17/04/17 14:57:00 INFO BlockRDD: Removing RDD 1259 from persistence list
17/04/17 14:57:00 INFO BlockManager: Removing RDD 1259
17/04/17 14:57:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1259] at createStream at KafkaConsumer.java:64 of time 1492421220000 ms
17/04/17 14:57:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421200000 ms)
17/04/17 14:57:00 INFO InputInfoTracker: remove old batch metadata: 1492421200000 ms
17/04/17 14:57:10 INFO JobScheduler: Added jobs for time 1492421230000 ms
17/04/17 14:57:10 INFO JobScheduler: Starting job streaming job 1492421230000 ms.0 from job set of time 1492421230000 ms
-------------------------------------------
Time: 1492421230000 ms
-------------------------------------------

17/04/17 14:57:10 INFO JobScheduler: Finished job streaming job 1492421230000 ms.0 from job set of time 1492421230000 ms
17/04/17 14:57:10 INFO MapPartitionsRDD: Removing RDD 1264 from persistence list
17/04/17 14:57:10 INFO JobScheduler: Total delay: 0.002 s for time 1492421230000 ms (execution: 0.000 s)
17/04/17 14:57:10 INFO BlockManager: Removing RDD 1264
17/04/17 14:57:10 INFO MapPartitionsRDD: Removing RDD 1263 from persistence list
17/04/17 14:57:10 INFO BlockManager: Removing RDD 1263
17/04/17 14:57:10 INFO BlockRDD: Removing RDD 1262 from persistence list
17/04/17 14:57:10 INFO BlockManager: Removing RDD 1262
17/04/17 14:57:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1262] at createStream at KafkaConsumer.java:64 of time 1492421230000 ms
17/04/17 14:57:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421210000 ms)
17/04/17 14:57:10 INFO InputInfoTracker: remove old batch metadata: 1492421210000 ms
17/04/17 14:57:20 INFO JobScheduler: Added jobs for time 1492421240000 ms
17/04/17 14:57:20 INFO JobScheduler: Starting job streaming job 1492421240000 ms.0 from job set of time 1492421240000 ms
-------------------------------------------
Time: 1492421240000 ms
-------------------------------------------

17/04/17 14:57:20 INFO JobScheduler: Finished job streaming job 1492421240000 ms.0 from job set of time 1492421240000 ms
17/04/17 14:57:20 INFO MapPartitionsRDD: Removing RDD 1267 from persistence list
17/04/17 14:57:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421240000 ms (execution: 0.000 s)
17/04/17 14:57:20 INFO MapPartitionsRDD: Removing RDD 1266 from persistence list
17/04/17 14:57:20 INFO BlockManager: Removing RDD 1267
17/04/17 14:57:20 INFO BlockManager: Removing RDD 1266
17/04/17 14:57:20 INFO BlockRDD: Removing RDD 1265 from persistence list
17/04/17 14:57:20 INFO BlockManager: Removing RDD 1265
17/04/17 14:57:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1265] at createStream at KafkaConsumer.java:64 of time 1492421240000 ms
17/04/17 14:57:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421220000 ms)
17/04/17 14:57:20 INFO InputInfoTracker: remove old batch metadata: 1492421220000 ms
17/04/17 14:57:30 INFO JobScheduler: Added jobs for time 1492421250000 ms
17/04/17 14:57:30 INFO JobScheduler: Starting job streaming job 1492421250000 ms.0 from job set of time 1492421250000 ms
-------------------------------------------
Time: 1492421250000 ms
-------------------------------------------

17/04/17 14:57:30 INFO JobScheduler: Finished job streaming job 1492421250000 ms.0 from job set of time 1492421250000 ms
17/04/17 14:57:30 INFO MapPartitionsRDD: Removing RDD 1270 from persistence list
17/04/17 14:57:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421250000 ms (execution: 0.000 s)
17/04/17 14:57:30 INFO MapPartitionsRDD: Removing RDD 1269 from persistence list
17/04/17 14:57:30 INFO BlockManager: Removing RDD 1270
17/04/17 14:57:30 INFO BlockManager: Removing RDD 1269
17/04/17 14:57:30 INFO BlockRDD: Removing RDD 1268 from persistence list
17/04/17 14:57:30 INFO BlockManager: Removing RDD 1268
17/04/17 14:57:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1268] at createStream at KafkaConsumer.java:64 of time 1492421250000 ms
17/04/17 14:57:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421230000 ms)
17/04/17 14:57:30 INFO InputInfoTracker: remove old batch metadata: 1492421230000 ms
17/04/17 14:57:40 INFO JobScheduler: Added jobs for time 1492421260000 ms
17/04/17 14:57:40 INFO JobScheduler: Starting job streaming job 1492421260000 ms.0 from job set of time 1492421260000 ms
-------------------------------------------
Time: 1492421260000 ms
-------------------------------------------

17/04/17 14:57:40 INFO JobScheduler: Finished job streaming job 1492421260000 ms.0 from job set of time 1492421260000 ms
17/04/17 14:57:40 INFO MapPartitionsRDD: Removing RDD 1273 from persistence list
17/04/17 14:57:40 INFO JobScheduler: Total delay: 0.005 s for time 1492421260000 ms (execution: 0.001 s)
17/04/17 14:57:40 INFO MapPartitionsRDD: Removing RDD 1272 from persistence list
17/04/17 14:57:40 INFO BlockManager: Removing RDD 1273
17/04/17 14:57:40 INFO BlockManager: Removing RDD 1272
17/04/17 14:57:40 INFO BlockRDD: Removing RDD 1271 from persistence list
17/04/17 14:57:40 INFO BlockManager: Removing RDD 1271
17/04/17 14:57:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1271] at createStream at KafkaConsumer.java:64 of time 1492421260000 ms
17/04/17 14:57:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421240000 ms)
17/04/17 14:57:40 INFO InputInfoTracker: remove old batch metadata: 1492421240000 ms
17/04/17 14:57:50 INFO JobScheduler: Added jobs for time 1492421270000 ms
17/04/17 14:57:50 INFO JobScheduler: Starting job streaming job 1492421270000 ms.0 from job set of time 1492421270000 ms
-------------------------------------------
Time: 1492421270000 ms
-------------------------------------------

17/04/17 14:57:50 INFO JobScheduler: Finished job streaming job 1492421270000 ms.0 from job set of time 1492421270000 ms
17/04/17 14:57:50 INFO MapPartitionsRDD: Removing RDD 1276 from persistence list
17/04/17 14:57:50 INFO JobScheduler: Total delay: 0.005 s for time 1492421270000 ms (execution: 0.001 s)
17/04/17 14:57:50 INFO BlockManager: Removing RDD 1276
17/04/17 14:57:50 INFO MapPartitionsRDD: Removing RDD 1275 from persistence list
17/04/17 14:57:50 INFO BlockManager: Removing RDD 1275
17/04/17 14:57:50 INFO BlockRDD: Removing RDD 1274 from persistence list
17/04/17 14:57:50 INFO BlockManager: Removing RDD 1274
17/04/17 14:57:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1274] at createStream at KafkaConsumer.java:64 of time 1492421270000 ms
17/04/17 14:57:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421250000 ms)
17/04/17 14:57:50 INFO InputInfoTracker: remove old batch metadata: 1492421250000 ms
17/04/17 14:58:00 INFO JobScheduler: Added jobs for time 1492421280000 ms
-------------------------------------------
Time: 1492421280000 ms
-------------------------------------------

17/04/17 14:58:00 INFO JobScheduler: Starting job streaming job 1492421280000 ms.0 from job set of time 1492421280000 ms
17/04/17 14:58:00 INFO JobScheduler: Finished job streaming job 1492421280000 ms.0 from job set of time 1492421280000 ms
17/04/17 14:58:00 INFO MapPartitionsRDD: Removing RDD 1279 from persistence list
17/04/17 14:58:00 INFO JobScheduler: Total delay: 0.005 s for time 1492421280000 ms (execution: 0.000 s)
17/04/17 14:58:00 INFO BlockManager: Removing RDD 1279
17/04/17 14:58:00 INFO MapPartitionsRDD: Removing RDD 1278 from persistence list
17/04/17 14:58:00 INFO BlockRDD: Removing RDD 1277 from persistence list
17/04/17 14:58:00 INFO BlockManager: Removing RDD 1278
17/04/17 14:58:00 INFO BlockManager: Removing RDD 1277
17/04/17 14:58:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1277] at createStream at KafkaConsumer.java:64 of time 1492421280000 ms
17/04/17 14:58:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421260000 ms)
17/04/17 14:58:00 INFO InputInfoTracker: remove old batch metadata: 1492421260000 ms
17/04/17 14:58:10 INFO JobScheduler: Added jobs for time 1492421290000 ms
17/04/17 14:58:10 INFO JobScheduler: Starting job streaming job 1492421290000 ms.0 from job set of time 1492421290000 ms
-------------------------------------------
Time: 1492421290000 ms
-------------------------------------------

17/04/17 14:58:10 INFO JobScheduler: Finished job streaming job 1492421290000 ms.0 from job set of time 1492421290000 ms
17/04/17 14:58:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421290000 ms (execution: 0.000 s)
17/04/17 14:58:10 INFO MapPartitionsRDD: Removing RDD 1282 from persistence list
17/04/17 14:58:10 INFO BlockManager: Removing RDD 1282
17/04/17 14:58:10 INFO MapPartitionsRDD: Removing RDD 1281 from persistence list
17/04/17 14:58:10 INFO BlockManager: Removing RDD 1281
17/04/17 14:58:10 INFO BlockRDD: Removing RDD 1280 from persistence list
17/04/17 14:58:10 INFO BlockManager: Removing RDD 1280
17/04/17 14:58:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1280] at createStream at KafkaConsumer.java:64 of time 1492421290000 ms
17/04/17 14:58:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421270000 ms)
17/04/17 14:58:10 INFO InputInfoTracker: remove old batch metadata: 1492421270000 ms
17/04/17 14:58:20 INFO JobScheduler: Added jobs for time 1492421300000 ms
17/04/17 14:58:20 INFO JobScheduler: Starting job streaming job 1492421300000 ms.0 from job set of time 1492421300000 ms
-------------------------------------------
Time: 1492421300000 ms
-------------------------------------------

17/04/17 14:58:20 INFO JobScheduler: Finished job streaming job 1492421300000 ms.0 from job set of time 1492421300000 ms
17/04/17 14:58:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421300000 ms (execution: 0.000 s)
17/04/17 14:58:20 INFO MapPartitionsRDD: Removing RDD 1285 from persistence list
17/04/17 14:58:20 INFO BlockManager: Removing RDD 1285
17/04/17 14:58:20 INFO MapPartitionsRDD: Removing RDD 1284 from persistence list
17/04/17 14:58:20 INFO BlockManager: Removing RDD 1284
17/04/17 14:58:20 INFO BlockRDD: Removing RDD 1283 from persistence list
17/04/17 14:58:20 INFO BlockManager: Removing RDD 1283
17/04/17 14:58:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1283] at createStream at KafkaConsumer.java:64 of time 1492421300000 ms
17/04/17 14:58:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421280000 ms)
17/04/17 14:58:20 INFO InputInfoTracker: remove old batch metadata: 1492421280000 ms
17/04/17 14:58:30 INFO JobScheduler: Added jobs for time 1492421310000 ms
17/04/17 14:58:30 INFO JobScheduler: Starting job streaming job 1492421310000 ms.0 from job set of time 1492421310000 ms
-------------------------------------------
Time: 1492421310000 ms
-------------------------------------------

17/04/17 14:58:30 INFO JobScheduler: Finished job streaming job 1492421310000 ms.0 from job set of time 1492421310000 ms
17/04/17 14:58:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421310000 ms (execution: 0.000 s)
17/04/17 14:58:30 INFO MapPartitionsRDD: Removing RDD 1288 from persistence list
17/04/17 14:58:30 INFO BlockManager: Removing RDD 1288
17/04/17 14:58:30 INFO MapPartitionsRDD: Removing RDD 1287 from persistence list
17/04/17 14:58:30 INFO BlockManager: Removing RDD 1287
17/04/17 14:58:30 INFO BlockRDD: Removing RDD 1286 from persistence list
17/04/17 14:58:30 INFO BlockManager: Removing RDD 1286
17/04/17 14:58:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1286] at createStream at KafkaConsumer.java:64 of time 1492421310000 ms
17/04/17 14:58:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421290000 ms)
17/04/17 14:58:30 INFO InputInfoTracker: remove old batch metadata: 1492421290000 ms
17/04/17 14:58:40 INFO JobScheduler: Added jobs for time 1492421320000 ms
17/04/17 14:58:40 INFO JobScheduler: Starting job streaming job 1492421320000 ms.0 from job set of time 1492421320000 ms
-------------------------------------------
Time: 1492421320000 ms
-------------------------------------------

17/04/17 14:58:40 INFO JobScheduler: Finished job streaming job 1492421320000 ms.0 from job set of time 1492421320000 ms
17/04/17 14:58:40 INFO MapPartitionsRDD: Removing RDD 1291 from persistence list
17/04/17 14:58:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421320000 ms (execution: 0.000 s)
17/04/17 14:58:40 INFO BlockManager: Removing RDD 1291
17/04/17 14:58:40 INFO MapPartitionsRDD: Removing RDD 1290 from persistence list
17/04/17 14:58:40 INFO BlockManager: Removing RDD 1290
17/04/17 14:58:40 INFO BlockRDD: Removing RDD 1289 from persistence list
17/04/17 14:58:40 INFO BlockManager: Removing RDD 1289
17/04/17 14:58:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1289] at createStream at KafkaConsumer.java:64 of time 1492421320000 ms
17/04/17 14:58:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421300000 ms)
17/04/17 14:58:40 INFO InputInfoTracker: remove old batch metadata: 1492421300000 ms
17/04/17 14:58:50 INFO JobScheduler: Added jobs for time 1492421330000 ms
17/04/17 14:58:50 INFO JobScheduler: Starting job streaming job 1492421330000 ms.0 from job set of time 1492421330000 ms
-------------------------------------------
Time: 1492421330000 ms
-------------------------------------------

17/04/17 14:58:50 INFO JobScheduler: Finished job streaming job 1492421330000 ms.0 from job set of time 1492421330000 ms
17/04/17 14:58:50 INFO MapPartitionsRDD: Removing RDD 1294 from persistence list
17/04/17 14:58:50 INFO JobScheduler: Total delay: 0.005 s for time 1492421330000 ms (execution: 0.001 s)
17/04/17 14:58:50 INFO BlockManager: Removing RDD 1294
17/04/17 14:58:50 INFO MapPartitionsRDD: Removing RDD 1293 from persistence list
17/04/17 14:58:50 INFO BlockManager: Removing RDD 1293
17/04/17 14:58:50 INFO BlockRDD: Removing RDD 1292 from persistence list
17/04/17 14:58:50 INFO BlockManager: Removing RDD 1292
17/04/17 14:58:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1292] at createStream at KafkaConsumer.java:64 of time 1492421330000 ms
17/04/17 14:58:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421310000 ms)
17/04/17 14:58:50 INFO InputInfoTracker: remove old batch metadata: 1492421310000 ms
17/04/17 14:59:00 INFO JobScheduler: Added jobs for time 1492421340000 ms
17/04/17 14:59:00 INFO JobScheduler: Starting job streaming job 1492421340000 ms.0 from job set of time 1492421340000 ms
-------------------------------------------
Time: 1492421340000 ms
-------------------------------------------

17/04/17 14:59:00 INFO JobScheduler: Finished job streaming job 1492421340000 ms.0 from job set of time 1492421340000 ms
17/04/17 14:59:00 INFO MapPartitionsRDD: Removing RDD 1297 from persistence list
17/04/17 14:59:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421340000 ms (execution: 0.000 s)
17/04/17 14:59:00 INFO BlockManager: Removing RDD 1297
17/04/17 14:59:00 INFO MapPartitionsRDD: Removing RDD 1296 from persistence list
17/04/17 14:59:00 INFO BlockManager: Removing RDD 1296
17/04/17 14:59:00 INFO BlockRDD: Removing RDD 1295 from persistence list
17/04/17 14:59:00 INFO BlockManager: Removing RDD 1295
17/04/17 14:59:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1295] at createStream at KafkaConsumer.java:64 of time 1492421340000 ms
17/04/17 14:59:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421320000 ms)
17/04/17 14:59:00 INFO InputInfoTracker: remove old batch metadata: 1492421320000 ms
17/04/17 14:59:10 INFO JobScheduler: Added jobs for time 1492421350000 ms
-------------------------------------------
Time: 1492421350000 ms
-------------------------------------------

17/04/17 14:59:10 INFO JobScheduler: Starting job streaming job 1492421350000 ms.0 from job set of time 1492421350000 ms
17/04/17 14:59:10 INFO JobScheduler: Finished job streaming job 1492421350000 ms.0 from job set of time 1492421350000 ms
17/04/17 14:59:10 INFO MapPartitionsRDD: Removing RDD 1300 from persistence list
17/04/17 14:59:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421350000 ms (execution: 0.000 s)
17/04/17 14:59:10 INFO BlockManager: Removing RDD 1300
17/04/17 14:59:10 INFO MapPartitionsRDD: Removing RDD 1299 from persistence list
17/04/17 14:59:10 INFO BlockManager: Removing RDD 1299
17/04/17 14:59:10 INFO BlockRDD: Removing RDD 1298 from persistence list
17/04/17 14:59:10 INFO BlockManager: Removing RDD 1298
17/04/17 14:59:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1298] at createStream at KafkaConsumer.java:64 of time 1492421350000 ms
17/04/17 14:59:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421330000 ms)
17/04/17 14:59:10 INFO InputInfoTracker: remove old batch metadata: 1492421330000 ms
17/04/17 14:59:20 INFO JobScheduler: Added jobs for time 1492421360000 ms
17/04/17 14:59:20 INFO JobScheduler: Starting job streaming job 1492421360000 ms.0 from job set of time 1492421360000 ms
-------------------------------------------
Time: 1492421360000 ms
-------------------------------------------

17/04/17 14:59:20 INFO JobScheduler: Finished job streaming job 1492421360000 ms.0 from job set of time 1492421360000 ms
17/04/17 14:59:20 INFO MapPartitionsRDD: Removing RDD 1303 from persistence list
17/04/17 14:59:20 INFO JobScheduler: Total delay: 0.005 s for time 1492421360000 ms (execution: 0.000 s)
17/04/17 14:59:20 INFO BlockManager: Removing RDD 1303
17/04/17 14:59:20 INFO MapPartitionsRDD: Removing RDD 1302 from persistence list
17/04/17 14:59:20 INFO BlockManager: Removing RDD 1302
17/04/17 14:59:20 INFO BlockRDD: Removing RDD 1301 from persistence list
17/04/17 14:59:20 INFO BlockManager: Removing RDD 1301
17/04/17 14:59:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1301] at createStream at KafkaConsumer.java:64 of time 1492421360000 ms
17/04/17 14:59:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421340000 ms)
17/04/17 14:59:20 INFO InputInfoTracker: remove old batch metadata: 1492421340000 ms
17/04/17 14:59:30 INFO JobScheduler: Added jobs for time 1492421370000 ms
17/04/17 14:59:30 INFO JobScheduler: Starting job streaming job 1492421370000 ms.0 from job set of time 1492421370000 ms
-------------------------------------------
Time: 1492421370000 ms
-------------------------------------------

17/04/17 14:59:30 INFO JobScheduler: Finished job streaming job 1492421370000 ms.0 from job set of time 1492421370000 ms
17/04/17 14:59:30 INFO MapPartitionsRDD: Removing RDD 1306 from persistence list
17/04/17 14:59:30 INFO JobScheduler: Total delay: 0.005 s for time 1492421370000 ms (execution: 0.000 s)
17/04/17 14:59:30 INFO BlockManager: Removing RDD 1306
17/04/17 14:59:30 INFO MapPartitionsRDD: Removing RDD 1305 from persistence list
17/04/17 14:59:30 INFO BlockManager: Removing RDD 1305
17/04/17 14:59:30 INFO BlockRDD: Removing RDD 1304 from persistence list
17/04/17 14:59:30 INFO BlockManager: Removing RDD 1304
17/04/17 14:59:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1304] at createStream at KafkaConsumer.java:64 of time 1492421370000 ms
17/04/17 14:59:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421350000 ms)
17/04/17 14:59:30 INFO InputInfoTracker: remove old batch metadata: 1492421350000 ms
17/04/17 14:59:40 INFO JobScheduler: Added jobs for time 1492421380000 ms
17/04/17 14:59:40 INFO JobScheduler: Starting job streaming job 1492421380000 ms.0 from job set of time 1492421380000 ms
-------------------------------------------
Time: 1492421380000 ms
-------------------------------------------

17/04/17 14:59:40 INFO JobScheduler: Finished job streaming job 1492421380000 ms.0 from job set of time 1492421380000 ms
17/04/17 14:59:40 INFO MapPartitionsRDD: Removing RDD 1309 from persistence list
17/04/17 14:59:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421380000 ms (execution: 0.000 s)
17/04/17 14:59:40 INFO BlockManager: Removing RDD 1309
17/04/17 14:59:40 INFO MapPartitionsRDD: Removing RDD 1308 from persistence list
17/04/17 14:59:40 INFO BlockManager: Removing RDD 1308
17/04/17 14:59:40 INFO BlockRDD: Removing RDD 1307 from persistence list
17/04/17 14:59:40 INFO BlockManager: Removing RDD 1307
17/04/17 14:59:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1307] at createStream at KafkaConsumer.java:64 of time 1492421380000 ms
17/04/17 14:59:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421360000 ms)
17/04/17 14:59:40 INFO InputInfoTracker: remove old batch metadata: 1492421360000 ms
17/04/17 14:59:50 INFO JobScheduler: Added jobs for time 1492421390000 ms
17/04/17 14:59:50 INFO JobScheduler: Starting job streaming job 1492421390000 ms.0 from job set of time 1492421390000 ms
-------------------------------------------
Time: 1492421390000 ms
-------------------------------------------

17/04/17 14:59:50 INFO JobScheduler: Finished job streaming job 1492421390000 ms.0 from job set of time 1492421390000 ms
17/04/17 14:59:50 INFO MapPartitionsRDD: Removing RDD 1312 from persistence list
17/04/17 14:59:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421390000 ms (execution: 0.000 s)
17/04/17 14:59:50 INFO BlockManager: Removing RDD 1312
17/04/17 14:59:50 INFO MapPartitionsRDD: Removing RDD 1311 from persistence list
17/04/17 14:59:50 INFO BlockManager: Removing RDD 1311
17/04/17 14:59:50 INFO BlockRDD: Removing RDD 1310 from persistence list
17/04/17 14:59:50 INFO BlockManager: Removing RDD 1310
17/04/17 14:59:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1310] at createStream at KafkaConsumer.java:64 of time 1492421390000 ms
17/04/17 14:59:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421370000 ms)
17/04/17 14:59:50 INFO InputInfoTracker: remove old batch metadata: 1492421370000 ms
17/04/17 15:00:00 INFO JobScheduler: Added jobs for time 1492421400000 ms
17/04/17 15:00:00 INFO JobScheduler: Starting job streaming job 1492421400000 ms.0 from job set of time 1492421400000 ms
-------------------------------------------
Time: 1492421400000 ms
-------------------------------------------

17/04/17 15:00:00 INFO JobScheduler: Finished job streaming job 1492421400000 ms.0 from job set of time 1492421400000 ms
17/04/17 15:00:00 INFO MapPartitionsRDD: Removing RDD 1315 from persistence list
17/04/17 15:00:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421400000 ms (execution: 0.001 s)
17/04/17 15:00:00 INFO BlockManager: Removing RDD 1315
17/04/17 15:00:00 INFO MapPartitionsRDD: Removing RDD 1314 from persistence list
17/04/17 15:00:00 INFO BlockRDD: Removing RDD 1313 from persistence list
17/04/17 15:00:00 INFO BlockManager: Removing RDD 1314
17/04/17 15:00:00 INFO BlockManager: Removing RDD 1313
17/04/17 15:00:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1313] at createStream at KafkaConsumer.java:64 of time 1492421400000 ms
17/04/17 15:00:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421380000 ms)
17/04/17 15:00:00 INFO InputInfoTracker: remove old batch metadata: 1492421380000 ms
17/04/17 15:00:10 INFO JobScheduler: Added jobs for time 1492421410000 ms
17/04/17 15:00:10 INFO JobScheduler: Starting job streaming job 1492421410000 ms.0 from job set of time 1492421410000 ms
-------------------------------------------
Time: 1492421410000 ms
-------------------------------------------

17/04/17 15:00:10 INFO JobScheduler: Finished job streaming job 1492421410000 ms.0 from job set of time 1492421410000 ms
17/04/17 15:00:10 INFO MapPartitionsRDD: Removing RDD 1318 from persistence list
17/04/17 15:00:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421410000 ms (execution: 0.000 s)
17/04/17 15:00:10 INFO BlockManager: Removing RDD 1318
17/04/17 15:00:10 INFO MapPartitionsRDD: Removing RDD 1317 from persistence list
17/04/17 15:00:10 INFO BlockManager: Removing RDD 1317
17/04/17 15:00:10 INFO BlockRDD: Removing RDD 1316 from persistence list
17/04/17 15:00:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1316] at createStream at KafkaConsumer.java:64 of time 1492421410000 ms
17/04/17 15:00:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421390000 ms)
17/04/17 15:00:10 INFO InputInfoTracker: remove old batch metadata: 1492421390000 ms
17/04/17 15:00:10 INFO BlockManager: Removing RDD 1316
17/04/17 15:00:20 INFO JobScheduler: Added jobs for time 1492421420000 ms
17/04/17 15:00:20 INFO JobScheduler: Starting job streaming job 1492421420000 ms.0 from job set of time 1492421420000 ms
-------------------------------------------
Time: 1492421420000 ms
-------------------------------------------

17/04/17 15:00:20 INFO JobScheduler: Finished job streaming job 1492421420000 ms.0 from job set of time 1492421420000 ms
17/04/17 15:00:20 INFO MapPartitionsRDD: Removing RDD 1321 from persistence list
17/04/17 15:00:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421420000 ms (execution: 0.000 s)
17/04/17 15:00:20 INFO MapPartitionsRDD: Removing RDD 1320 from persistence list
17/04/17 15:00:20 INFO BlockManager: Removing RDD 1321
17/04/17 15:00:20 INFO BlockRDD: Removing RDD 1319 from persistence list
17/04/17 15:00:20 INFO BlockManager: Removing RDD 1320
17/04/17 15:00:20 INFO BlockManager: Removing RDD 1319
17/04/17 15:00:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1319] at createStream at KafkaConsumer.java:64 of time 1492421420000 ms
17/04/17 15:00:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421400000 ms)
17/04/17 15:00:20 INFO InputInfoTracker: remove old batch metadata: 1492421400000 ms
17/04/17 15:00:30 INFO JobScheduler: Added jobs for time 1492421430000 ms
17/04/17 15:00:30 INFO JobScheduler: Starting job streaming job 1492421430000 ms.0 from job set of time 1492421430000 ms
-------------------------------------------
Time: 1492421430000 ms
-------------------------------------------

17/04/17 15:00:30 INFO JobScheduler: Finished job streaming job 1492421430000 ms.0 from job set of time 1492421430000 ms
17/04/17 15:00:30 INFO MapPartitionsRDD: Removing RDD 1324 from persistence list
17/04/17 15:00:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421430000 ms (execution: 0.000 s)
17/04/17 15:00:30 INFO BlockManager: Removing RDD 1324
17/04/17 15:00:30 INFO MapPartitionsRDD: Removing RDD 1323 from persistence list
17/04/17 15:00:30 INFO BlockManager: Removing RDD 1323
17/04/17 15:00:30 INFO BlockRDD: Removing RDD 1322 from persistence list
17/04/17 15:00:30 INFO BlockManager: Removing RDD 1322
17/04/17 15:00:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1322] at createStream at KafkaConsumer.java:64 of time 1492421430000 ms
17/04/17 15:00:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421410000 ms)
17/04/17 15:00:30 INFO InputInfoTracker: remove old batch metadata: 1492421410000 ms
17/04/17 15:00:40 INFO JobScheduler: Added jobs for time 1492421440000 ms
17/04/17 15:00:40 INFO JobScheduler: Starting job streaming job 1492421440000 ms.0 from job set of time 1492421440000 ms
-------------------------------------------
Time: 1492421440000 ms
-------------------------------------------

17/04/17 15:00:40 INFO JobScheduler: Finished job streaming job 1492421440000 ms.0 from job set of time 1492421440000 ms
17/04/17 15:00:40 INFO MapPartitionsRDD: Removing RDD 1327 from persistence list
17/04/17 15:00:40 INFO JobScheduler: Total delay: 0.005 s for time 1492421440000 ms (execution: 0.001 s)
17/04/17 15:00:40 INFO MapPartitionsRDD: Removing RDD 1326 from persistence list
17/04/17 15:00:40 INFO BlockManager: Removing RDD 1327
17/04/17 15:00:40 INFO BlockRDD: Removing RDD 1325 from persistence list
17/04/17 15:00:40 INFO BlockManager: Removing RDD 1326
17/04/17 15:00:40 INFO BlockManager: Removing RDD 1325
17/04/17 15:00:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1325] at createStream at KafkaConsumer.java:64 of time 1492421440000 ms
17/04/17 15:00:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421420000 ms)
17/04/17 15:00:40 INFO InputInfoTracker: remove old batch metadata: 1492421420000 ms
17/04/17 15:00:50 INFO JobScheduler: Added jobs for time 1492421450000 ms
17/04/17 15:00:50 INFO JobScheduler: Starting job streaming job 1492421450000 ms.0 from job set of time 1492421450000 ms
-------------------------------------------
Time: 1492421450000 ms
-------------------------------------------

17/04/17 15:00:50 INFO JobScheduler: Finished job streaming job 1492421450000 ms.0 from job set of time 1492421450000 ms
17/04/17 15:00:50 INFO MapPartitionsRDD: Removing RDD 1330 from persistence list
17/04/17 15:00:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421450000 ms (execution: 0.000 s)
17/04/17 15:00:50 INFO BlockManager: Removing RDD 1330
17/04/17 15:00:50 INFO MapPartitionsRDD: Removing RDD 1329 from persistence list
17/04/17 15:00:50 INFO BlockManager: Removing RDD 1329
17/04/17 15:00:50 INFO BlockRDD: Removing RDD 1328 from persistence list
17/04/17 15:00:50 INFO BlockManager: Removing RDD 1328
17/04/17 15:00:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1328] at createStream at KafkaConsumer.java:64 of time 1492421450000 ms
17/04/17 15:00:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421430000 ms)
17/04/17 15:00:50 INFO InputInfoTracker: remove old batch metadata: 1492421430000 ms
17/04/17 15:01:00 INFO JobScheduler: Added jobs for time 1492421460000 ms
17/04/17 15:01:00 INFO JobScheduler: Starting job streaming job 1492421460000 ms.0 from job set of time 1492421460000 ms
-------------------------------------------
Time: 1492421460000 ms
-------------------------------------------

17/04/17 15:01:00 INFO JobScheduler: Finished job streaming job 1492421460000 ms.0 from job set of time 1492421460000 ms
17/04/17 15:01:00 INFO MapPartitionsRDD: Removing RDD 1333 from persistence list
17/04/17 15:01:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421460000 ms (execution: 0.000 s)
17/04/17 15:01:00 INFO BlockManager: Removing RDD 1333
17/04/17 15:01:00 INFO MapPartitionsRDD: Removing RDD 1332 from persistence list
17/04/17 15:01:00 INFO BlockManager: Removing RDD 1332
17/04/17 15:01:00 INFO BlockRDD: Removing RDD 1331 from persistence list
17/04/17 15:01:00 INFO BlockManager: Removing RDD 1331
17/04/17 15:01:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1331] at createStream at KafkaConsumer.java:64 of time 1492421460000 ms
17/04/17 15:01:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421440000 ms)
17/04/17 15:01:00 INFO InputInfoTracker: remove old batch metadata: 1492421440000 ms
17/04/17 15:01:10 INFO JobScheduler: Added jobs for time 1492421470000 ms
17/04/17 15:01:10 INFO JobScheduler: Starting job streaming job 1492421470000 ms.0 from job set of time 1492421470000 ms
-------------------------------------------
Time: 1492421470000 ms
-------------------------------------------

17/04/17 15:01:10 INFO JobScheduler: Finished job streaming job 1492421470000 ms.0 from job set of time 1492421470000 ms
17/04/17 15:01:10 INFO MapPartitionsRDD: Removing RDD 1336 from persistence list
17/04/17 15:01:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421470000 ms (execution: 0.000 s)
17/04/17 15:01:10 INFO BlockManager: Removing RDD 1336
17/04/17 15:01:10 INFO MapPartitionsRDD: Removing RDD 1335 from persistence list
17/04/17 15:01:10 INFO BlockManager: Removing RDD 1335
17/04/17 15:01:10 INFO BlockRDD: Removing RDD 1334 from persistence list
17/04/17 15:01:10 INFO BlockManager: Removing RDD 1334
17/04/17 15:01:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1334] at createStream at KafkaConsumer.java:64 of time 1492421470000 ms
17/04/17 15:01:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421450000 ms)
17/04/17 15:01:10 INFO InputInfoTracker: remove old batch metadata: 1492421450000 ms
17/04/17 15:01:20 INFO JobScheduler: Starting job streaming job 1492421480000 ms.0 from job set of time 1492421480000 ms
-------------------------------------------
Time: 1492421480000 ms
-------------------------------------------

17/04/17 15:01:20 INFO JobScheduler: Added jobs for time 1492421480000 ms
17/04/17 15:01:20 INFO JobScheduler: Finished job streaming job 1492421480000 ms.0 from job set of time 1492421480000 ms
17/04/17 15:01:20 INFO MapPartitionsRDD: Removing RDD 1339 from persistence list
17/04/17 15:01:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421480000 ms (execution: 0.000 s)
17/04/17 15:01:20 INFO BlockManager: Removing RDD 1339
17/04/17 15:01:20 INFO MapPartitionsRDD: Removing RDD 1338 from persistence list
17/04/17 15:01:20 INFO BlockManager: Removing RDD 1338
17/04/17 15:01:20 INFO BlockRDD: Removing RDD 1337 from persistence list
17/04/17 15:01:20 INFO BlockManager: Removing RDD 1337
17/04/17 15:01:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1337] at createStream at KafkaConsumer.java:64 of time 1492421480000 ms
17/04/17 15:01:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421460000 ms)
17/04/17 15:01:20 INFO InputInfoTracker: remove old batch metadata: 1492421460000 ms
17/04/17 15:01:30 INFO JobScheduler: Added jobs for time 1492421490000 ms
17/04/17 15:01:30 INFO JobScheduler: Starting job streaming job 1492421490000 ms.0 from job set of time 1492421490000 ms
-------------------------------------------
Time: 1492421490000 ms
-------------------------------------------

17/04/17 15:01:30 INFO JobScheduler: Finished job streaming job 1492421490000 ms.0 from job set of time 1492421490000 ms
17/04/17 15:01:30 INFO MapPartitionsRDD: Removing RDD 1342 from persistence list
17/04/17 15:01:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421490000 ms (execution: 0.000 s)
17/04/17 15:01:30 INFO BlockManager: Removing RDD 1342
17/04/17 15:01:30 INFO MapPartitionsRDD: Removing RDD 1341 from persistence list
17/04/17 15:01:30 INFO BlockManager: Removing RDD 1341
17/04/17 15:01:30 INFO BlockRDD: Removing RDD 1340 from persistence list
17/04/17 15:01:30 INFO BlockManager: Removing RDD 1340
17/04/17 15:01:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1340] at createStream at KafkaConsumer.java:64 of time 1492421490000 ms
17/04/17 15:01:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421470000 ms)
17/04/17 15:01:30 INFO InputInfoTracker: remove old batch metadata: 1492421470000 ms
17/04/17 15:01:40 INFO JobScheduler: Added jobs for time 1492421500000 ms
17/04/17 15:01:40 INFO JobScheduler: Starting job streaming job 1492421500000 ms.0 from job set of time 1492421500000 ms
-------------------------------------------
Time: 1492421500000 ms
-------------------------------------------

17/04/17 15:01:40 INFO JobScheduler: Finished job streaming job 1492421500000 ms.0 from job set of time 1492421500000 ms
17/04/17 15:01:40 INFO MapPartitionsRDD: Removing RDD 1345 from persistence list
17/04/17 15:01:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421500000 ms (execution: 0.000 s)
17/04/17 15:01:40 INFO BlockManager: Removing RDD 1345
17/04/17 15:01:40 INFO MapPartitionsRDD: Removing RDD 1344 from persistence list
17/04/17 15:01:40 INFO BlockManager: Removing RDD 1344
17/04/17 15:01:40 INFO BlockRDD: Removing RDD 1343 from persistence list
17/04/17 15:01:40 INFO BlockManager: Removing RDD 1343
17/04/17 15:01:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1343] at createStream at KafkaConsumer.java:64 of time 1492421500000 ms
17/04/17 15:01:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421480000 ms)
17/04/17 15:01:40 INFO InputInfoTracker: remove old batch metadata: 1492421480000 ms
17/04/17 15:01:50 INFO JobScheduler: Added jobs for time 1492421510000 ms
17/04/17 15:01:50 INFO JobScheduler: Starting job streaming job 1492421510000 ms.0 from job set of time 1492421510000 ms
-------------------------------------------
Time: 1492421510000 ms
-------------------------------------------

17/04/17 15:01:50 INFO JobScheduler: Finished job streaming job 1492421510000 ms.0 from job set of time 1492421510000 ms
17/04/17 15:01:50 INFO MapPartitionsRDD: Removing RDD 1348 from persistence list
17/04/17 15:01:50 INFO JobScheduler: Total delay: 0.005 s for time 1492421510000 ms (execution: 0.001 s)
17/04/17 15:01:50 INFO BlockManager: Removing RDD 1348
17/04/17 15:01:50 INFO MapPartitionsRDD: Removing RDD 1347 from persistence list
17/04/17 15:01:50 INFO BlockManager: Removing RDD 1347
17/04/17 15:01:50 INFO BlockRDD: Removing RDD 1346 from persistence list
17/04/17 15:01:50 INFO BlockManager: Removing RDD 1346
17/04/17 15:01:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1346] at createStream at KafkaConsumer.java:64 of time 1492421510000 ms
17/04/17 15:01:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421490000 ms)
17/04/17 15:01:50 INFO InputInfoTracker: remove old batch metadata: 1492421490000 ms
17/04/17 15:02:00 INFO JobScheduler: Added jobs for time 1492421520000 ms
17/04/17 15:02:00 INFO JobScheduler: Starting job streaming job 1492421520000 ms.0 from job set of time 1492421520000 ms
-------------------------------------------
Time: 1492421520000 ms
-------------------------------------------

17/04/17 15:02:00 INFO JobScheduler: Finished job streaming job 1492421520000 ms.0 from job set of time 1492421520000 ms
17/04/17 15:02:00 INFO MapPartitionsRDD: Removing RDD 1351 from persistence list
17/04/17 15:02:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421520000 ms (execution: 0.000 s)
17/04/17 15:02:00 INFO BlockManager: Removing RDD 1351
17/04/17 15:02:00 INFO MapPartitionsRDD: Removing RDD 1350 from persistence list
17/04/17 15:02:00 INFO BlockManager: Removing RDD 1350
17/04/17 15:02:00 INFO BlockRDD: Removing RDD 1349 from persistence list
17/04/17 15:02:00 INFO BlockManager: Removing RDD 1349
17/04/17 15:02:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1349] at createStream at KafkaConsumer.java:64 of time 1492421520000 ms
17/04/17 15:02:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421500000 ms)
17/04/17 15:02:00 INFO InputInfoTracker: remove old batch metadata: 1492421500000 ms
17/04/17 15:02:10 INFO JobScheduler: Added jobs for time 1492421530000 ms
17/04/17 15:02:10 INFO JobScheduler: Starting job streaming job 1492421530000 ms.0 from job set of time 1492421530000 ms
-------------------------------------------
Time: 1492421530000 ms
-------------------------------------------

17/04/17 15:02:10 INFO JobScheduler: Finished job streaming job 1492421530000 ms.0 from job set of time 1492421530000 ms
17/04/17 15:02:10 INFO MapPartitionsRDD: Removing RDD 1354 from persistence list
17/04/17 15:02:10 INFO JobScheduler: Total delay: 0.006 s for time 1492421530000 ms (execution: 0.001 s)
17/04/17 15:02:10 INFO BlockManager: Removing RDD 1354
17/04/17 15:02:10 INFO MapPartitionsRDD: Removing RDD 1353 from persistence list
17/04/17 15:02:10 INFO BlockManager: Removing RDD 1353
17/04/17 15:02:10 INFO BlockRDD: Removing RDD 1352 from persistence list
17/04/17 15:02:10 INFO BlockManager: Removing RDD 1352
17/04/17 15:02:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1352] at createStream at KafkaConsumer.java:64 of time 1492421530000 ms
17/04/17 15:02:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421510000 ms)
17/04/17 15:02:10 INFO InputInfoTracker: remove old batch metadata: 1492421510000 ms
17/04/17 15:02:20 INFO JobScheduler: Added jobs for time 1492421540000 ms
17/04/17 15:02:20 INFO JobScheduler: Starting job streaming job 1492421540000 ms.0 from job set of time 1492421540000 ms
-------------------------------------------
Time: 1492421540000 ms
-------------------------------------------

17/04/17 15:02:20 INFO JobScheduler: Finished job streaming job 1492421540000 ms.0 from job set of time 1492421540000 ms
17/04/17 15:02:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421540000 ms (execution: 0.000 s)
17/04/17 15:02:20 INFO MapPartitionsRDD: Removing RDD 1357 from persistence list
17/04/17 15:02:20 INFO BlockManager: Removing RDD 1357
17/04/17 15:02:20 INFO MapPartitionsRDD: Removing RDD 1356 from persistence list
17/04/17 15:02:20 INFO BlockManager: Removing RDD 1356
17/04/17 15:02:20 INFO BlockRDD: Removing RDD 1355 from persistence list
17/04/17 15:02:20 INFO BlockManager: Removing RDD 1355
17/04/17 15:02:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1355] at createStream at KafkaConsumer.java:64 of time 1492421540000 ms
17/04/17 15:02:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421520000 ms)
17/04/17 15:02:20 INFO InputInfoTracker: remove old batch metadata: 1492421520000 ms
17/04/17 15:02:30 INFO JobScheduler: Added jobs for time 1492421550000 ms
-------------------------------------------
Time: 1492421550000 ms
-------------------------------------------

17/04/17 15:02:30 INFO JobScheduler: Starting job streaming job 1492421550000 ms.0 from job set of time 1492421550000 ms
17/04/17 15:02:30 INFO JobScheduler: Finished job streaming job 1492421550000 ms.0 from job set of time 1492421550000 ms
17/04/17 15:02:30 INFO MapPartitionsRDD: Removing RDD 1360 from persistence list
17/04/17 15:02:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421550000 ms (execution: 0.000 s)
17/04/17 15:02:30 INFO BlockManager: Removing RDD 1360
17/04/17 15:02:30 INFO MapPartitionsRDD: Removing RDD 1359 from persistence list
17/04/17 15:02:30 INFO BlockManager: Removing RDD 1359
17/04/17 15:02:30 INFO BlockRDD: Removing RDD 1358 from persistence list
17/04/17 15:02:30 INFO BlockManager: Removing RDD 1358
17/04/17 15:02:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1358] at createStream at KafkaConsumer.java:64 of time 1492421550000 ms
17/04/17 15:02:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421530000 ms)
17/04/17 15:02:30 INFO InputInfoTracker: remove old batch metadata: 1492421530000 ms
17/04/17 15:02:40 INFO JobScheduler: Added jobs for time 1492421560000 ms
17/04/17 15:02:40 INFO JobScheduler: Starting job streaming job 1492421560000 ms.0 from job set of time 1492421560000 ms
-------------------------------------------
Time: 1492421560000 ms
-------------------------------------------

17/04/17 15:02:40 INFO JobScheduler: Finished job streaming job 1492421560000 ms.0 from job set of time 1492421560000 ms
17/04/17 15:02:40 INFO MapPartitionsRDD: Removing RDD 1363 from persistence list
17/04/17 15:02:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421560000 ms (execution: 0.000 s)
17/04/17 15:02:40 INFO BlockManager: Removing RDD 1363
17/04/17 15:02:40 INFO MapPartitionsRDD: Removing RDD 1362 from persistence list
17/04/17 15:02:40 INFO BlockManager: Removing RDD 1362
17/04/17 15:02:40 INFO BlockRDD: Removing RDD 1361 from persistence list
17/04/17 15:02:40 INFO BlockManager: Removing RDD 1361
17/04/17 15:02:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1361] at createStream at KafkaConsumer.java:64 of time 1492421560000 ms
17/04/17 15:02:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421540000 ms)
17/04/17 15:02:40 INFO InputInfoTracker: remove old batch metadata: 1492421540000 ms
17/04/17 15:02:50 INFO JobScheduler: Added jobs for time 1492421570000 ms
17/04/17 15:02:50 INFO JobScheduler: Starting job streaming job 1492421570000 ms.0 from job set of time 1492421570000 ms
-------------------------------------------
Time: 1492421570000 ms
-------------------------------------------

17/04/17 15:02:50 INFO JobScheduler: Finished job streaming job 1492421570000 ms.0 from job set of time 1492421570000 ms
17/04/17 15:02:50 INFO JobScheduler: Total delay: 0.005 s for time 1492421570000 ms (execution: 0.001 s)
17/04/17 15:02:50 INFO MapPartitionsRDD: Removing RDD 1366 from persistence list
17/04/17 15:02:50 INFO MapPartitionsRDD: Removing RDD 1365 from persistence list
17/04/17 15:02:50 INFO BlockManager: Removing RDD 1366
17/04/17 15:02:50 INFO BlockManager: Removing RDD 1365
17/04/17 15:02:50 INFO BlockRDD: Removing RDD 1364 from persistence list
17/04/17 15:02:50 INFO BlockManager: Removing RDD 1364
17/04/17 15:02:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1364] at createStream at KafkaConsumer.java:64 of time 1492421570000 ms
17/04/17 15:02:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421550000 ms)
17/04/17 15:02:50 INFO InputInfoTracker: remove old batch metadata: 1492421550000 ms
17/04/17 15:03:00 INFO JobScheduler: Added jobs for time 1492421580000 ms
17/04/17 15:03:00 INFO JobScheduler: Starting job streaming job 1492421580000 ms.0 from job set of time 1492421580000 ms
-------------------------------------------
Time: 1492421580000 ms
-------------------------------------------

17/04/17 15:03:00 INFO JobScheduler: Finished job streaming job 1492421580000 ms.0 from job set of time 1492421580000 ms
17/04/17 15:03:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421580000 ms (execution: 0.000 s)
17/04/17 15:03:00 INFO MapPartitionsRDD: Removing RDD 1369 from persistence list
17/04/17 15:03:00 INFO BlockManager: Removing RDD 1369
17/04/17 15:03:00 INFO MapPartitionsRDD: Removing RDD 1368 from persistence list
17/04/17 15:03:00 INFO BlockManager: Removing RDD 1368
17/04/17 15:03:00 INFO BlockRDD: Removing RDD 1367 from persistence list
17/04/17 15:03:00 INFO BlockManager: Removing RDD 1367
17/04/17 15:03:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1367] at createStream at KafkaConsumer.java:64 of time 1492421580000 ms
17/04/17 15:03:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421560000 ms)
17/04/17 15:03:00 INFO InputInfoTracker: remove old batch metadata: 1492421560000 ms
17/04/17 15:03:10 INFO JobScheduler: Added jobs for time 1492421590000 ms
17/04/17 15:03:10 INFO JobScheduler: Starting job streaming job 1492421590000 ms.0 from job set of time 1492421590000 ms
-------------------------------------------
Time: 1492421590000 ms
-------------------------------------------

17/04/17 15:03:10 INFO JobScheduler: Finished job streaming job 1492421590000 ms.0 from job set of time 1492421590000 ms
17/04/17 15:03:10 INFO MapPartitionsRDD: Removing RDD 1372 from persistence list
17/04/17 15:03:10 INFO JobScheduler: Total delay: 0.005 s for time 1492421590000 ms (execution: 0.001 s)
17/04/17 15:03:10 INFO BlockManager: Removing RDD 1372
17/04/17 15:03:10 INFO MapPartitionsRDD: Removing RDD 1371 from persistence list
17/04/17 15:03:10 INFO BlockManager: Removing RDD 1371
17/04/17 15:03:10 INFO BlockRDD: Removing RDD 1370 from persistence list
17/04/17 15:03:10 INFO BlockManager: Removing RDD 1370
17/04/17 15:03:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1370] at createStream at KafkaConsumer.java:64 of time 1492421590000 ms
17/04/17 15:03:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421570000 ms)
17/04/17 15:03:10 INFO InputInfoTracker: remove old batch metadata: 1492421570000 ms
17/04/17 15:03:20 INFO JobScheduler: Added jobs for time 1492421600000 ms
17/04/17 15:03:20 INFO JobScheduler: Starting job streaming job 1492421600000 ms.0 from job set of time 1492421600000 ms
-------------------------------------------
Time: 1492421600000 ms
-------------------------------------------

17/04/17 15:03:20 INFO JobScheduler: Finished job streaming job 1492421600000 ms.0 from job set of time 1492421600000 ms
17/04/17 15:03:20 INFO MapPartitionsRDD: Removing RDD 1375 from persistence list
17/04/17 15:03:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421600000 ms (execution: 0.000 s)
17/04/17 15:03:20 INFO BlockManager: Removing RDD 1375
17/04/17 15:03:20 INFO MapPartitionsRDD: Removing RDD 1374 from persistence list
17/04/17 15:03:20 INFO BlockManager: Removing RDD 1374
17/04/17 15:03:20 INFO BlockRDD: Removing RDD 1373 from persistence list
17/04/17 15:03:20 INFO BlockManager: Removing RDD 1373
17/04/17 15:03:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1373] at createStream at KafkaConsumer.java:64 of time 1492421600000 ms
17/04/17 15:03:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421580000 ms)
17/04/17 15:03:20 INFO InputInfoTracker: remove old batch metadata: 1492421580000 ms
17/04/17 15:03:30 INFO JobScheduler: Added jobs for time 1492421610000 ms
17/04/17 15:03:30 INFO JobScheduler: Starting job streaming job 1492421610000 ms.0 from job set of time 1492421610000 ms
-------------------------------------------
Time: 1492421610000 ms
-------------------------------------------

17/04/17 15:03:30 INFO JobScheduler: Finished job streaming job 1492421610000 ms.0 from job set of time 1492421610000 ms
17/04/17 15:03:30 INFO MapPartitionsRDD: Removing RDD 1378 from persistence list
17/04/17 15:03:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421610000 ms (execution: 0.000 s)
17/04/17 15:03:30 INFO BlockManager: Removing RDD 1378
17/04/17 15:03:30 INFO MapPartitionsRDD: Removing RDD 1377 from persistence list
17/04/17 15:03:30 INFO BlockManager: Removing RDD 1377
17/04/17 15:03:30 INFO BlockRDD: Removing RDD 1376 from persistence list
17/04/17 15:03:30 INFO BlockManager: Removing RDD 1376
17/04/17 15:03:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1376] at createStream at KafkaConsumer.java:64 of time 1492421610000 ms
17/04/17 15:03:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421590000 ms)
17/04/17 15:03:30 INFO InputInfoTracker: remove old batch metadata: 1492421590000 ms
17/04/17 15:03:40 INFO JobScheduler: Added jobs for time 1492421620000 ms
17/04/17 15:03:40 INFO JobScheduler: Starting job streaming job 1492421620000 ms.0 from job set of time 1492421620000 ms
-------------------------------------------
Time: 1492421620000 ms
-------------------------------------------

17/04/17 15:03:40 INFO JobScheduler: Finished job streaming job 1492421620000 ms.0 from job set of time 1492421620000 ms
17/04/17 15:03:40 INFO MapPartitionsRDD: Removing RDD 1381 from persistence list
17/04/17 15:03:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421620000 ms (execution: 0.001 s)
17/04/17 15:03:40 INFO BlockManager: Removing RDD 1381
17/04/17 15:03:40 INFO MapPartitionsRDD: Removing RDD 1380 from persistence list
17/04/17 15:03:40 INFO BlockManager: Removing RDD 1380
17/04/17 15:03:40 INFO BlockRDD: Removing RDD 1379 from persistence list
17/04/17 15:03:40 INFO BlockManager: Removing RDD 1379
17/04/17 15:03:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1379] at createStream at KafkaConsumer.java:64 of time 1492421620000 ms
17/04/17 15:03:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421600000 ms)
17/04/17 15:03:40 INFO InputInfoTracker: remove old batch metadata: 1492421600000 ms
17/04/17 15:03:50 INFO JobScheduler: Added jobs for time 1492421630000 ms
17/04/17 15:03:50 INFO JobScheduler: Starting job streaming job 1492421630000 ms.0 from job set of time 1492421630000 ms
-------------------------------------------
Time: 1492421630000 ms
-------------------------------------------

17/04/17 15:03:50 INFO JobScheduler: Finished job streaming job 1492421630000 ms.0 from job set of time 1492421630000 ms
17/04/17 15:03:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421630000 ms (execution: 0.000 s)
17/04/17 15:03:50 INFO MapPartitionsRDD: Removing RDD 1384 from persistence list
17/04/17 15:03:50 INFO BlockManager: Removing RDD 1384
17/04/17 15:03:50 INFO MapPartitionsRDD: Removing RDD 1383 from persistence list
17/04/17 15:03:50 INFO BlockManager: Removing RDD 1383
17/04/17 15:03:50 INFO BlockRDD: Removing RDD 1382 from persistence list
17/04/17 15:03:50 INFO BlockManager: Removing RDD 1382
17/04/17 15:03:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1382] at createStream at KafkaConsumer.java:64 of time 1492421630000 ms
17/04/17 15:03:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421610000 ms)
17/04/17 15:03:50 INFO InputInfoTracker: remove old batch metadata: 1492421610000 ms
17/04/17 15:04:00 INFO JobScheduler: Added jobs for time 1492421640000 ms
17/04/17 15:04:00 INFO JobScheduler: Starting job streaming job 1492421640000 ms.0 from job set of time 1492421640000 ms
-------------------------------------------
Time: 1492421640000 ms
-------------------------------------------

17/04/17 15:04:00 INFO JobScheduler: Finished job streaming job 1492421640000 ms.0 from job set of time 1492421640000 ms
17/04/17 15:04:00 INFO MapPartitionsRDD: Removing RDD 1387 from persistence list
17/04/17 15:04:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421640000 ms (execution: 0.000 s)
17/04/17 15:04:00 INFO BlockManager: Removing RDD 1387
17/04/17 15:04:00 INFO MapPartitionsRDD: Removing RDD 1386 from persistence list
17/04/17 15:04:00 INFO BlockManager: Removing RDD 1386
17/04/17 15:04:00 INFO BlockRDD: Removing RDD 1385 from persistence list
17/04/17 15:04:00 INFO BlockManager: Removing RDD 1385
17/04/17 15:04:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1385] at createStream at KafkaConsumer.java:64 of time 1492421640000 ms
17/04/17 15:04:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421620000 ms)
17/04/17 15:04:00 INFO InputInfoTracker: remove old batch metadata: 1492421620000 ms
17/04/17 15:04:10 INFO JobScheduler: Added jobs for time 1492421650000 ms
17/04/17 15:04:10 INFO JobScheduler: Starting job streaming job 1492421650000 ms.0 from job set of time 1492421650000 ms
-------------------------------------------
Time: 1492421650000 ms
-------------------------------------------

17/04/17 15:04:10 INFO JobScheduler: Finished job streaming job 1492421650000 ms.0 from job set of time 1492421650000 ms
17/04/17 15:04:10 INFO MapPartitionsRDD: Removing RDD 1390 from persistence list
17/04/17 15:04:10 INFO JobScheduler: Total delay: 0.005 s for time 1492421650000 ms (execution: 0.001 s)
17/04/17 15:04:10 INFO BlockManager: Removing RDD 1390
17/04/17 15:04:10 INFO MapPartitionsRDD: Removing RDD 1389 from persistence list
17/04/17 15:04:10 INFO BlockManager: Removing RDD 1389
17/04/17 15:04:10 INFO BlockRDD: Removing RDD 1388 from persistence list
17/04/17 15:04:10 INFO BlockManager: Removing RDD 1388
17/04/17 15:04:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1388] at createStream at KafkaConsumer.java:64 of time 1492421650000 ms
17/04/17 15:04:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421630000 ms)
17/04/17 15:04:10 INFO InputInfoTracker: remove old batch metadata: 1492421630000 ms
17/04/17 15:04:20 INFO JobScheduler: Added jobs for time 1492421660000 ms
17/04/17 15:04:20 INFO JobScheduler: Starting job streaming job 1492421660000 ms.0 from job set of time 1492421660000 ms
-------------------------------------------
Time: 1492421660000 ms
-------------------------------------------

17/04/17 15:04:20 INFO JobScheduler: Finished job streaming job 1492421660000 ms.0 from job set of time 1492421660000 ms
17/04/17 15:04:20 INFO MapPartitionsRDD: Removing RDD 1393 from persistence list
17/04/17 15:04:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421660000 ms (execution: 0.000 s)
17/04/17 15:04:20 INFO BlockManager: Removing RDD 1393
17/04/17 15:04:20 INFO MapPartitionsRDD: Removing RDD 1392 from persistence list
17/04/17 15:04:20 INFO BlockManager: Removing RDD 1392
17/04/17 15:04:20 INFO BlockRDD: Removing RDD 1391 from persistence list
17/04/17 15:04:20 INFO BlockManager: Removing RDD 1391
17/04/17 15:04:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1391] at createStream at KafkaConsumer.java:64 of time 1492421660000 ms
17/04/17 15:04:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421640000 ms)
17/04/17 15:04:20 INFO InputInfoTracker: remove old batch metadata: 1492421640000 ms
17/04/17 15:04:30 INFO JobScheduler: Added jobs for time 1492421670000 ms
17/04/17 15:04:30 INFO JobScheduler: Starting job streaming job 1492421670000 ms.0 from job set of time 1492421670000 ms
-------------------------------------------
Time: 1492421670000 ms
-------------------------------------------

17/04/17 15:04:30 INFO JobScheduler: Finished job streaming job 1492421670000 ms.0 from job set of time 1492421670000 ms
17/04/17 15:04:30 INFO MapPartitionsRDD: Removing RDD 1396 from persistence list
17/04/17 15:04:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421670000 ms (execution: 0.000 s)
17/04/17 15:04:30 INFO BlockManager: Removing RDD 1396
17/04/17 15:04:30 INFO MapPartitionsRDD: Removing RDD 1395 from persistence list
17/04/17 15:04:30 INFO BlockManager: Removing RDD 1395
17/04/17 15:04:30 INFO BlockRDD: Removing RDD 1394 from persistence list
17/04/17 15:04:30 INFO BlockManager: Removing RDD 1394
17/04/17 15:04:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1394] at createStream at KafkaConsumer.java:64 of time 1492421670000 ms
17/04/17 15:04:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421650000 ms)
17/04/17 15:04:30 INFO InputInfoTracker: remove old batch metadata: 1492421650000 ms
17/04/17 15:04:40 INFO JobScheduler: Added jobs for time 1492421680000 ms
17/04/17 15:04:40 INFO JobScheduler: Starting job streaming job 1492421680000 ms.0 from job set of time 1492421680000 ms
-------------------------------------------
Time: 1492421680000 ms
-------------------------------------------

17/04/17 15:04:40 INFO JobScheduler: Finished job streaming job 1492421680000 ms.0 from job set of time 1492421680000 ms
17/04/17 15:04:40 INFO MapPartitionsRDD: Removing RDD 1399 from persistence list
17/04/17 15:04:40 INFO JobScheduler: Total delay: 0.004 s for time 1492421680000 ms (execution: 0.000 s)
17/04/17 15:04:40 INFO BlockManager: Removing RDD 1399
17/04/17 15:04:40 INFO MapPartitionsRDD: Removing RDD 1398 from persistence list
17/04/17 15:04:40 INFO BlockManager: Removing RDD 1398
17/04/17 15:04:40 INFO BlockRDD: Removing RDD 1397 from persistence list
17/04/17 15:04:40 INFO BlockManager: Removing RDD 1397
17/04/17 15:04:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1397] at createStream at KafkaConsumer.java:64 of time 1492421680000 ms
17/04/17 15:04:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421660000 ms)
17/04/17 15:04:40 INFO InputInfoTracker: remove old batch metadata: 1492421660000 ms
17/04/17 15:04:50 INFO JobScheduler: Added jobs for time 1492421690000 ms
-------------------------------------------
Time: 1492421690000 ms
-------------------------------------------

17/04/17 15:04:50 INFO JobScheduler: Starting job streaming job 1492421690000 ms.0 from job set of time 1492421690000 ms
17/04/17 15:04:50 INFO JobScheduler: Finished job streaming job 1492421690000 ms.0 from job set of time 1492421690000 ms
17/04/17 15:04:50 INFO MapPartitionsRDD: Removing RDD 1402 from persistence list
17/04/17 15:04:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421690000 ms (execution: 0.000 s)
17/04/17 15:04:50 INFO BlockManager: Removing RDD 1402
17/04/17 15:04:50 INFO MapPartitionsRDD: Removing RDD 1401 from persistence list
17/04/17 15:04:50 INFO BlockManager: Removing RDD 1401
17/04/17 15:04:50 INFO BlockRDD: Removing RDD 1400 from persistence list
17/04/17 15:04:50 INFO BlockManager: Removing RDD 1400
17/04/17 15:04:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1400] at createStream at KafkaConsumer.java:64 of time 1492421690000 ms
17/04/17 15:04:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421670000 ms)
17/04/17 15:04:50 INFO InputInfoTracker: remove old batch metadata: 1492421670000 ms
17/04/17 15:05:00 INFO JobScheduler: Added jobs for time 1492421700000 ms
17/04/17 15:05:00 INFO JobScheduler: Starting job streaming job 1492421700000 ms.0 from job set of time 1492421700000 ms
-------------------------------------------
Time: 1492421700000 ms
-------------------------------------------

17/04/17 15:05:00 INFO JobScheduler: Finished job streaming job 1492421700000 ms.0 from job set of time 1492421700000 ms
17/04/17 15:05:00 INFO MapPartitionsRDD: Removing RDD 1405 from persistence list
17/04/17 15:05:00 INFO JobScheduler: Total delay: 0.005 s for time 1492421700000 ms (execution: 0.001 s)
17/04/17 15:05:00 INFO BlockManager: Removing RDD 1405
17/04/17 15:05:00 INFO MapPartitionsRDD: Removing RDD 1404 from persistence list
17/04/17 15:05:00 INFO BlockManager: Removing RDD 1404
17/04/17 15:05:00 INFO BlockRDD: Removing RDD 1403 from persistence list
17/04/17 15:05:00 INFO BlockManager: Removing RDD 1403
17/04/17 15:05:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1403] at createStream at KafkaConsumer.java:64 of time 1492421700000 ms
17/04/17 15:05:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421680000 ms)
17/04/17 15:05:00 INFO InputInfoTracker: remove old batch metadata: 1492421680000 ms
17/04/17 15:05:10 INFO JobScheduler: Added jobs for time 1492421710000 ms
17/04/17 15:05:10 INFO JobScheduler: Starting job streaming job 1492421710000 ms.0 from job set of time 1492421710000 ms
-------------------------------------------
Time: 1492421710000 ms
-------------------------------------------

17/04/17 15:05:10 INFO JobScheduler: Finished job streaming job 1492421710000 ms.0 from job set of time 1492421710000 ms
17/04/17 15:05:10 INFO MapPartitionsRDD: Removing RDD 1408 from persistence list
17/04/17 15:05:10 INFO JobScheduler: Total delay: 0.005 s for time 1492421710000 ms (execution: 0.001 s)
17/04/17 15:05:10 INFO BlockManager: Removing RDD 1408
17/04/17 15:05:10 INFO MapPartitionsRDD: Removing RDD 1407 from persistence list
17/04/17 15:05:10 INFO BlockManager: Removing RDD 1407
17/04/17 15:05:10 INFO BlockRDD: Removing RDD 1406 from persistence list
17/04/17 15:05:10 INFO BlockManager: Removing RDD 1406
17/04/17 15:05:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1406] at createStream at KafkaConsumer.java:64 of time 1492421710000 ms
17/04/17 15:05:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421690000 ms)
17/04/17 15:05:10 INFO InputInfoTracker: remove old batch metadata: 1492421690000 ms
17/04/17 15:05:20 INFO JobScheduler: Added jobs for time 1492421720000 ms
17/04/17 15:05:20 INFO JobScheduler: Starting job streaming job 1492421720000 ms.0 from job set of time 1492421720000 ms
-------------------------------------------
Time: 1492421720000 ms
-------------------------------------------

17/04/17 15:05:20 INFO JobScheduler: Finished job streaming job 1492421720000 ms.0 from job set of time 1492421720000 ms
17/04/17 15:05:20 INFO MapPartitionsRDD: Removing RDD 1411 from persistence list
17/04/17 15:05:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421720000 ms (execution: 0.001 s)
17/04/17 15:05:20 INFO BlockManager: Removing RDD 1411
17/04/17 15:05:20 INFO MapPartitionsRDD: Removing RDD 1410 from persistence list
17/04/17 15:05:20 INFO BlockManager: Removing RDD 1410
17/04/17 15:05:20 INFO BlockRDD: Removing RDD 1409 from persistence list
17/04/17 15:05:20 INFO BlockManager: Removing RDD 1409
17/04/17 15:05:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1409] at createStream at KafkaConsumer.java:64 of time 1492421720000 ms
17/04/17 15:05:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421700000 ms)
17/04/17 15:05:20 INFO InputInfoTracker: remove old batch metadata: 1492421700000 ms
17/04/17 15:05:30 INFO JobScheduler: Added jobs for time 1492421730000 ms
17/04/17 15:05:30 INFO JobScheduler: Starting job streaming job 1492421730000 ms.0 from job set of time 1492421730000 ms
-------------------------------------------
Time: 1492421730000 ms
-------------------------------------------

17/04/17 15:05:30 INFO JobScheduler: Finished job streaming job 1492421730000 ms.0 from job set of time 1492421730000 ms
17/04/17 15:05:30 INFO MapPartitionsRDD: Removing RDD 1414 from persistence list
17/04/17 15:05:30 INFO JobScheduler: Total delay: 0.004 s for time 1492421730000 ms (execution: 0.001 s)
17/04/17 15:05:30 INFO BlockManager: Removing RDD 1414
17/04/17 15:05:30 INFO MapPartitionsRDD: Removing RDD 1413 from persistence list
17/04/17 15:05:30 INFO BlockManager: Removing RDD 1413
17/04/17 15:05:30 INFO BlockRDD: Removing RDD 1412 from persistence list
17/04/17 15:05:30 INFO BlockManager: Removing RDD 1412
17/04/17 15:05:30 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1412] at createStream at KafkaConsumer.java:64 of time 1492421730000 ms
17/04/17 15:05:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421710000 ms)
17/04/17 15:05:30 INFO InputInfoTracker: remove old batch metadata: 1492421710000 ms
17/04/17 15:05:40 INFO JobScheduler: Added jobs for time 1492421740000 ms
17/04/17 15:05:40 INFO JobScheduler: Starting job streaming job 1492421740000 ms.0 from job set of time 1492421740000 ms
-------------------------------------------
Time: 1492421740000 ms
-------------------------------------------

17/04/17 15:05:40 INFO JobScheduler: Finished job streaming job 1492421740000 ms.0 from job set of time 1492421740000 ms
17/04/17 15:05:40 INFO MapPartitionsRDD: Removing RDD 1417 from persistence list
17/04/17 15:05:40 INFO JobScheduler: Total delay: 0.003 s for time 1492421740000 ms (execution: 0.000 s)
17/04/17 15:05:40 INFO BlockManager: Removing RDD 1417
17/04/17 15:05:40 INFO MapPartitionsRDD: Removing RDD 1416 from persistence list
17/04/17 15:05:40 INFO BlockManager: Removing RDD 1416
17/04/17 15:05:40 INFO BlockRDD: Removing RDD 1415 from persistence list
17/04/17 15:05:40 INFO BlockManager: Removing RDD 1415
17/04/17 15:05:40 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1415] at createStream at KafkaConsumer.java:64 of time 1492421740000 ms
17/04/17 15:05:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421720000 ms)
17/04/17 15:05:40 INFO InputInfoTracker: remove old batch metadata: 1492421720000 ms
17/04/17 15:05:50 INFO JobScheduler: Added jobs for time 1492421750000 ms
-------------------------------------------
Time: 1492421750000 ms
-------------------------------------------

17/04/17 15:05:50 INFO JobScheduler: Starting job streaming job 1492421750000 ms.0 from job set of time 1492421750000 ms
17/04/17 15:05:50 INFO JobScheduler: Finished job streaming job 1492421750000 ms.0 from job set of time 1492421750000 ms
17/04/17 15:05:50 INFO MapPartitionsRDD: Removing RDD 1420 from persistence list
17/04/17 15:05:50 INFO JobScheduler: Total delay: 0.004 s for time 1492421750000 ms (execution: 0.000 s)
17/04/17 15:05:50 INFO BlockManager: Removing RDD 1420
17/04/17 15:05:50 INFO MapPartitionsRDD: Removing RDD 1419 from persistence list
17/04/17 15:05:50 INFO BlockManager: Removing RDD 1419
17/04/17 15:05:50 INFO BlockRDD: Removing RDD 1418 from persistence list
17/04/17 15:05:50 INFO BlockManager: Removing RDD 1418
17/04/17 15:05:50 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1418] at createStream at KafkaConsumer.java:64 of time 1492421750000 ms
17/04/17 15:05:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421730000 ms)
17/04/17 15:05:50 INFO InputInfoTracker: remove old batch metadata: 1492421730000 ms
17/04/17 15:06:00 INFO JobScheduler: Added jobs for time 1492421760000 ms
17/04/17 15:06:00 INFO JobScheduler: Starting job streaming job 1492421760000 ms.0 from job set of time 1492421760000 ms
-------------------------------------------
Time: 1492421760000 ms
-------------------------------------------

17/04/17 15:06:00 INFO JobScheduler: Finished job streaming job 1492421760000 ms.0 from job set of time 1492421760000 ms
17/04/17 15:06:00 INFO MapPartitionsRDD: Removing RDD 1423 from persistence list
17/04/17 15:06:00 INFO JobScheduler: Total delay: 0.004 s for time 1492421760000 ms (execution: 0.000 s)
17/04/17 15:06:00 INFO BlockManager: Removing RDD 1423
17/04/17 15:06:00 INFO MapPartitionsRDD: Removing RDD 1422 from persistence list
17/04/17 15:06:00 INFO BlockManager: Removing RDD 1422
17/04/17 15:06:00 INFO BlockRDD: Removing RDD 1421 from persistence list
17/04/17 15:06:00 INFO BlockManager: Removing RDD 1421
17/04/17 15:06:00 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1421] at createStream at KafkaConsumer.java:64 of time 1492421760000 ms
17/04/17 15:06:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421740000 ms)
17/04/17 15:06:00 INFO InputInfoTracker: remove old batch metadata: 1492421740000 ms
17/04/17 15:06:10 INFO JobScheduler: Added jobs for time 1492421770000 ms
-------------------------------------------
Time: 1492421770000 ms
-------------------------------------------

17/04/17 15:06:10 INFO JobScheduler: Starting job streaming job 1492421770000 ms.0 from job set of time 1492421770000 ms
17/04/17 15:06:10 INFO JobScheduler: Finished job streaming job 1492421770000 ms.0 from job set of time 1492421770000 ms
17/04/17 15:06:10 INFO MapPartitionsRDD: Removing RDD 1426 from persistence list
17/04/17 15:06:10 INFO JobScheduler: Total delay: 0.004 s for time 1492421770000 ms (execution: 0.000 s)
17/04/17 15:06:10 INFO BlockManager: Removing RDD 1426
17/04/17 15:06:10 INFO MapPartitionsRDD: Removing RDD 1425 from persistence list
17/04/17 15:06:10 INFO BlockManager: Removing RDD 1425
17/04/17 15:06:10 INFO BlockRDD: Removing RDD 1424 from persistence list
17/04/17 15:06:10 INFO BlockManager: Removing RDD 1424
17/04/17 15:06:10 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1424] at createStream at KafkaConsumer.java:64 of time 1492421770000 ms
17/04/17 15:06:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421750000 ms)
17/04/17 15:06:10 INFO InputInfoTracker: remove old batch metadata: 1492421750000 ms
17/04/17 15:06:20 INFO JobScheduler: Added jobs for time 1492421780000 ms
17/04/17 15:06:20 INFO JobScheduler: Starting job streaming job 1492421780000 ms.0 from job set of time 1492421780000 ms
-------------------------------------------
Time: 1492421780000 ms
-------------------------------------------

17/04/17 15:06:20 INFO JobScheduler: Finished job streaming job 1492421780000 ms.0 from job set of time 1492421780000 ms
17/04/17 15:06:20 INFO MapPartitionsRDD: Removing RDD 1429 from persistence list
17/04/17 15:06:20 INFO JobScheduler: Total delay: 0.004 s for time 1492421780000 ms (execution: 0.001 s)
17/04/17 15:06:20 INFO BlockManager: Removing RDD 1429
17/04/17 15:06:20 INFO MapPartitionsRDD: Removing RDD 1428 from persistence list
17/04/17 15:06:20 INFO BlockManager: Removing RDD 1428
17/04/17 15:06:20 INFO BlockRDD: Removing RDD 1427 from persistence list
17/04/17 15:06:20 INFO BlockManager: Removing RDD 1427
17/04/17 15:06:20 INFO KafkaInputDStream: Removing blocks of RDD BlockRDD[1427] at createStream at KafkaConsumer.java:64 of time 1492421780000 ms
17/04/17 15:06:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1492421760000 ms)
17/04/17 15:06:20 INFO InputInfoTracker: remove old batch metadata: 1492421760000 ms
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Warning: Local jar /home/am372811/gitprojects/onlinetraining/datagenerator/target/chatbot-0.1.0-SNAPSHOT-fat.jar does not exist, skipping.
java.lang.ClassNotFoundException: com.wipro.chatbot.kafka.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:175)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:689)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
